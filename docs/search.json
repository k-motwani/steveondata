[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Steve On Data",
    "section": "",
    "text": "Some Examples of Cumulative Mean with {TidyDensity}\n\n\n\n\n\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting the CCI30 Index Current Makeup\n\n\n\n\n\n\n\ncrypto\n\n\ncci30\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUse of the apply family of functions\n\n\n\n\n\n\n\nthanks\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse of the apply family of functions\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\napply\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Solutions to speedup tidy_bernoulli() with {data.table}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\ndata.table\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting NYS Home Heating Oil Prices with {rvest}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nrvest\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidy_bernoulli() with {data.table}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ndata.table\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple examples of imap() from {purrr}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nMar 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple examples of pmap() from {purrr}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting Timeseries in a list with R\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText Processing Made Easy with {healthyR}’s sql_left(), sql_mid(), and sql_right() Functions in R\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nsql\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpen a File Folder in R\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nshell\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuickly Generate Nested Time Series Models\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nautoarima\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Preppers with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\npreprocessor\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalibrate and Plot a Time Series with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConverting a {tidyAML} tibble to a {workflowsets}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\nworkflowsets\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nFeb 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOfficially on CRAN {tidyAML}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\n\n\nFeb 16, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMoving Average Plots with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn example of using {box}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nbox\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOff to CRAN! {tidyAML}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGet the Current Hospital Data Set from CMS with {healthyR.data}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrdata\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating and Predicting Fast Regression Parsnip Models with {tidyAML}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating an R Project Directory\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubsetting Named Lists in R\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCumulative Measurement Functions with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Argument Matcher: A Function for Selecting the Right Arguments {tidyAML}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlist\n\n\ntidyaml\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiverging Lollipop Chart: A Visual Tool for Comparing Data with {healthyR}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyr\n\n\nplots\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttributes in R Functions: An Overview\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nmetadata\n\n\nattributes\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedian: A Simple Way to Detect Excess Events Over Time with {healthyR}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{healthyR.ts}: The New and Improved Library for Time Series Analysis\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nService Line Grouping with {healthyR}\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\naugment\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransforming Your Data: A Guide to Popular Methods and How to Implement Them with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntransforms\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimplifying List Filtering in R with purrr’s keep()\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlists\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nJan 25, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaking Non Stationary Data Stationary\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlists\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nADF and Phillips-Perron Tests for Stationarity using lists\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlists\n\n\ntimeseries\n\n\nlapply\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother Post on Lists\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlists\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBoilerplate XGBoost with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nxgboost\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeometric Brownian Motion with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAugmenting a Brownian Motion to a Time Series with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuto K-Means with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nkmeans\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe building of {tidyAML}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn Update on {tidyAML}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidyaml\n\n\nautoml\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReflecting on the Past Year: A LinkedIn Year in Review\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlinkedin\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal Break Points for Histograms with {healthyR}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyr\n\n\nhistograms\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew Release of {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrownian Motion\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore Randomwalks with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\nrandomwalk\n\n\n\n\n\n\n\n\n\n\n\nJan 5, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalendar Heatmap with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2023\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvent Analysis with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGartner Magic Chart and its usefulness in healthcare analytics with {healthyR}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\n\n\nDec 29, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimulating Time Series Model Forecasts with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\nhealthyrts\n\n\ntimeseries\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nDec 23, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nListing Functions and Parameters\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ndplyr\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution Statistics with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Walks with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nrandomwalk\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nViewing Different Versions of the Same Statistical Distribution with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ndistributions\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Scedacity Plots with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Moving Average Plots with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution Summaries with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMixture Distributions with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\nmixturemodels\n\n\n\n\n\n\n\n\n\n\n\nDec 13, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate QQ Plots for Time Series Models with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a Faceted Historgram Plot with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhistograms\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate Multiple {parsnip} Model Specs with {purrr}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nparsnip\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZ-Score Scaling Step Recipe with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNaming Items in a List with {purrr}, {dplyr}, or {healthyR}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuto KNN with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\nknn\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtract Boilerplate Workflow Metrics with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate Random Walk Data with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Lists\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlists\n\n\nlapply\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefault Metric Sets with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary Statistics with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\ntidydensity\n\n\ndatatable\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Preprocessing Scale/Normalize with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrap Modeling with Base R\n\n\n\n\n\n\n\ncode\n\n\nbootstrap\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUpdates to {healthyverse} packages\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\nhealthyverse\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrap Modeling with {purrr} and {modler}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\nmodelr\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCumulative Harmonic Mean with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuto Prep data for XGBoost with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nxgboost\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFind Skewed Features with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nskew\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nTime Series Lag Correlation Plots\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReading Multiple Files with {purrr}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMapping K-Means with healthyR.ai\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nkmeans\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHyperbolic Transform with healthyR.ai\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscrete Fourier Vec with healthyR.ai\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBootstrapping and Plots with TidyDensity\n\n\n\n\n\n\n\ncode\n\n\ntidydensity\n\n\nbootstrap\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCumulative Skewness\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nPCA with healthyR.ai\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nControl Charts in healthyR.ai\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCumulative Variance\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ncumulative\n\n\nsapply\n\n\nlapply\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nTime Series Clustering with healthyR.ts\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nhealthyR.ai Primer\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nTidyDensity Primer\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple lapply()\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To Steve On Data\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/healthyrai-20221013/index.html",
    "href": "posts/healthyrai-20221013/index.html",
    "title": "healthyR.ai Primer",
    "section": "",
    "text": "This is going to serve as a sort of primer for my r packge {healthyR.ai}. The goal of this package is to help with producing uniform machine learning/ai models either from scratch or by way of one of the boilerplate functions.\nThis particular article is going to focus on k-means clustering with umap projection and visualization.\nFirst things first, lets load in the library:\n\nlibrary(healthyR.ai)\n\n\n== Welcome to healthyR.ai ===========================================================================\nIf you find this package useful, please leave a star: \n   https://github.com/spsanderson/healthyR.ai'\n\nIf you encounter a bug or want to request an enhancement please file an issue at:\n   https://github.com/spsanderson/healthyR.ai/issues\n\nThank you for using healthyR.ai\n\n\n\nInformation\nK-Means is a partition algorithm initially designed for signal processing. The goal is to partition n observations into k clusters where each n is in k. The unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters.\nThe aim of this post is to showcase the use of the healthyR.ai wrapper for the kmeans function along with the wrapper and plot for the uwot::umap projection function. We will go through the entire workflow from getting the data to getting the final UMAP plot.\n\n\nGenerate some data\n\nsuppressPackageStartupMessages(library(healthyR.data))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(broom))\nsuppressPackageStartupMessages(library(ggplot2))\n\ndata_tbl <- healthyR_data %>%\n    filter(ip_op_flag == \"I\") %>%\n    filter(payer_grouping != \"Medicare B\") %>%\n    filter(payer_grouping != \"?\") %>%\n    select(service_line, payer_grouping) %>%\n    mutate(record = 1) %>%\n    as_tibble()\n\ndata_tbl %>%\n  glimpse()\n\nRows: 116,823\nColumns: 3\n$ service_line   <chr> \"Medical\", \"Schizophrenia\", \"Syncope\", \"Pneumonia\", \"Ch…\n$ payer_grouping <chr> \"Blue Cross\", \"Medicare A\", \"Medicare A\", \"Medicare A\",…\n$ record         <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\n\nNow that we have our data we need to generate what is called a user item table. To do this we use the function hai_kmeans_user_item_tbl which takes in just a few arguments. The purpose of the user item table is to aggregate and normalize the data between the users and the items.\nThe data that we have generated is going to look for clustering amongst the service_lines (the user) and the payer_grouping (item) columns.\nLets now create the user item table.\n\n\nUser Item Tibble\n\nuit_tbl <- hai_kmeans_user_item_tbl(\n  data_tbl, \n  service_line, \n  payer_grouping, \n  record\n)\n\nuit_tbl\n\n# A tibble: 23 × 12\n   service_line   Blue …¹ Comme…² Compe…³ Excha…⁴    HMO Medic…⁵ Medic…⁶ Medic…⁷\n   <chr>            <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>\n 1 Alcohol Abuse   0.0941 0.0321  5.25e-4 0.0116  0.0788 0.158    0.367   0.173 \n 2 Bariatric Sur…  0.317  0.0583  0       0.0518  0.168  0.00324  0.343   0.0485\n 3 Carotid Endar…  0.0845 0.0282  0       0       0.0141 0        0.0282  0.648 \n 4 Cellulitis      0.110  0.0339  1.18e-2 0.00847 0.0805 0.0869   0.192   0.355 \n 5 Chest Pain      0.144  0.0391  2.90e-3 0.00543 0.112  0.0522   0.159   0.324 \n 6 CHF             0.0295 0.00958 5.18e-4 0.00414 0.0205 0.0197   0.0596  0.657 \n 7 COPD            0.0493 0.0228  2.28e-4 0.00548 0.0342 0.0461   0.172   0.520 \n 8 CVA             0.0647 0.0246  1.07e-3 0.0107  0.0524 0.0289   0.0764  0.555 \n 9 GI Hemorrhage   0.0542 0.0175  1.25e-3 0.00834 0.0480 0.0350   0.0855  0.588 \n10 Joint Replace…  0.139  0.0179  3.36e-2 0.00673 0.0516 0        0.0874  0.5   \n# … with 13 more rows, 3 more variables: `Medicare HMO` <dbl>,\n#   `No Fault` <dbl>, `Self Pay` <dbl>, and abbreviated variable names\n#   ¹​`Blue Cross`, ²​Commercial, ³​Compensation, ⁴​`Exchange Plans`, ⁵​Medicaid,\n#   ⁶​`Medicaid HMO`, ⁷​`Medicare A`\n\n\nThe table is aggregated by item for the various users to which the algorithm will be applied.\nNow that we have this data we need to find what will be out optimal k (clusters). To do this we need to generate a table of data that will have a column of k and for that k apply the k-means function to the data with that k and return the total within sum of squares.\nTo do this there is a convienent function called hai_kmeans_mapped_tbl that takes as its sole argument the output from the hai_kmeans_user_item_tbl. There is an argument .centers where the default is set to 15.\n\n\nK-Means Mapped Tibble\n\nkmm_tbl <- hai_kmeans_mapped_tbl(uit_tbl)\nkmm_tbl\n\n# A tibble: 15 × 3\n   centers k_means  glance          \n     <int> <list>   <list>          \n 1       1 <kmeans> <tibble [1 × 4]>\n 2       2 <kmeans> <tibble [1 × 4]>\n 3       3 <kmeans> <tibble [1 × 4]>\n 4       4 <kmeans> <tibble [1 × 4]>\n 5       5 <kmeans> <tibble [1 × 4]>\n 6       6 <kmeans> <tibble [1 × 4]>\n 7       7 <kmeans> <tibble [1 × 4]>\n 8       8 <kmeans> <tibble [1 × 4]>\n 9       9 <kmeans> <tibble [1 × 4]>\n10      10 <kmeans> <tibble [1 × 4]>\n11      11 <kmeans> <tibble [1 × 4]>\n12      12 <kmeans> <tibble [1 × 4]>\n13      13 <kmeans> <tibble [1 × 4]>\n14      14 <kmeans> <tibble [1 × 4]>\n15      15 <kmeans> <tibble [1 × 4]>\n\n\nAs we see there are three columns, centers, k_means and glance. The k_means column is the k_means list object and glance is the tibble returned by the broom::glance function.\n\nkmm_tbl %>%\n  tidyr::unnest(glance)\n\n# A tibble: 15 × 6\n   centers k_means  totss tot.withinss betweenss  iter\n     <int> <list>   <dbl>        <dbl>     <dbl> <int>\n 1       1 <kmeans>  1.41       1.41    1.33e-15     1\n 2       2 <kmeans>  1.41       0.592   8.17e- 1     1\n 3       3 <kmeans>  1.41       0.372   1.04e+ 0     2\n 4       4 <kmeans>  1.41       0.276   1.13e+ 0     2\n 5       5 <kmeans>  1.41       0.202   1.21e+ 0     2\n 6       6 <kmeans>  1.41       0.159   1.25e+ 0     3\n 7       7 <kmeans>  1.41       0.124   1.28e+ 0     3\n 8       8 <kmeans>  1.41       0.0884  1.32e+ 0     2\n 9       9 <kmeans>  1.41       0.0745  1.33e+ 0     3\n10      10 <kmeans>  1.41       0.0576  1.35e+ 0     2\n11      11 <kmeans>  1.41       0.0460  1.36e+ 0     2\n12      12 <kmeans>  1.41       0.0363  1.37e+ 0     3\n13      13 <kmeans>  1.41       0.0293  1.38e+ 0     3\n14      14 <kmeans>  1.41       0.0202  1.39e+ 0     2\n15      15 <kmeans>  1.41       0.0161  1.39e+ 0     2\n\n\nAs stated we use the tot.withinss to decide what will become our k, an easy way to do this is to visualize the Scree Plot, also known as the elbow plot. This is done by ploting the x-axis as the centers and the y-axis as the tot.withinss.\n\n\nScree Plot and Data\n\nhai_kmeans_scree_plt(.data = kmm_tbl)\n\n\n\n\nIf we want to see the scree plot data that creates the plot then we can use another function hai_kmeans_scree_data_tbl.\n\nhai_kmeans_scree_data_tbl(kmm_tbl)\n\n# A tibble: 15 × 2\n   centers tot.withinss\n     <int>        <dbl>\n 1       1       1.41  \n 2       2       0.592 \n 3       3       0.372 \n 4       4       0.276 \n 5       5       0.202 \n 6       6       0.159 \n 7       7       0.124 \n 8       8       0.0884\n 9       9       0.0745\n10      10       0.0576\n11      11       0.0460\n12      12       0.0363\n13      13       0.0293\n14      14       0.0202\n15      15       0.0161\n\n\nWith the above pieces of information we can decide upon a value for k, in this instance we are going to use 3. Now that we have that we can go ahead with creating the umap list object where we can take a look at a great many things associated with the data.\n\n\nUMAP List Object\nNow lets go ahead and create our UMAP list object.\n\nump_lst <- hai_umap_list(.data = uit_tbl, kmm_tbl, 3)\n\nNow that it is created, lets take a look at each item in the list. The umap_list function returns a list of 5 items.\n\numap_obj\numap_results_tbl\nkmeans_obj\nkmeans_cluster_tbl\numap_kmeans_cluster_results_tbl\n\nSince we have the list object we can now inspect the kmeans_obj, first thing we will do is use the hai_kmeans_tidy_tbl function to inspect things.\n\nkm_obj <- ump_lst$kmeans_obj\nhai_kmeans_tidy_tbl(.kmeans_obj = km_obj, .data = uit_tbl, .tidy_type = \"glance\")\n\n# A tibble: 1 × 4\n  totss tot.withinss betweenss  iter\n  <dbl>        <dbl>     <dbl> <int>\n1  1.41        0.372      1.04     2\n\nhai_kmeans_tidy_tbl(km_obj, uit_tbl, \"augment\")\n\n# A tibble: 23 × 2\n   service_line                  cluster\n   <chr>                         <fct>  \n 1 Alcohol Abuse                 1      \n 2 Bariatric Surgery For Obesity 1      \n 3 Carotid Endarterectomy        2      \n 4 Cellulitis                    3      \n 5 Chest Pain                    3      \n 6 CHF                           2      \n 7 COPD                          2      \n 8 CVA                           2      \n 9 GI Hemorrhage                 2      \n10 Joint Replacement             2      \n# … with 13 more rows\n\nhai_kmeans_tidy_tbl(km_obj, uit_tbl, \"tidy\")\n\n# A tibble: 3 × 14\n  Blue …¹ Comme…² Compe…³ Excha…⁴    HMO Medic…⁵ Medic…⁶ Medic…⁷ Medic…⁸ No Fa…⁹\n    <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1  0.150   0.0368 3.07e-4 0.0207  0.163   0.131   0.314    0.132  0.0319 0.00136\n2  0.0784  0.0218 4.32e-3 0.00620 0.0449  0.0368  0.0800   0.563  0.152  0.00348\n3  0.117   0.0314 1.02e-2 0.0139  0.0982  0.0856  0.147    0.354  0.105  0.00707\n# … with 4 more variables: `Self Pay` <dbl>, size <int>, withinss <dbl>,\n#   cluster <fct>, and abbreviated variable names ¹​`Blue Cross`, ²​Commercial,\n#   ³​Compensation, ⁴​`Exchange Plans`, ⁵​Medicaid, ⁶​`Medicaid HMO`,\n#   ⁷​`Medicare A`, ⁸​`Medicare HMO`, ⁹​`No Fault`\n\n\n\n\nUMAP Plot\nNow that we have all of the above data we can visualize our clusters that are colored by their cluster number.\n\nhai_umap_plot(.data = ump_lst, .point_size = 3, TRUE)"
  },
  {
    "objectID": "posts/healthyrts-20221021/index.html",
    "href": "posts/healthyrts-20221021/index.html",
    "title": "Time Series Clustering with healthyR.ts",
    "section": "",
    "text": "There are two components to time-series clustering with {healthyR.ts}. There is the function that will create the clustering data along with a slew of other information and then there is a plotting function that will plot out the data in a time-series fashion colored by cluster.\nThe first function as mentioned is the function ts_feature_cluster(), and the next is ts_feature_cluster_plot()\nFunction Reference:\n\nts_feature_cluster()\nts_feature_cluster_plot()`\n\nWe are going to use the built-in AirPassengers data set for this example so let’s get right to it!"
  },
  {
    "objectID": "posts/healthyrts-20221021/index.html#ts_feature_cluster",
    "href": "posts/healthyrts-20221021/index.html#ts_feature_cluster",
    "title": "Time Series Clustering with healthyR.ts",
    "section": "ts_feature_cluster()",
    "text": "ts_feature_cluster()\nAs mentioned there are several outputs from the ts_feature_cluster(). Those are as follows:\nData Section\n\nts_feature_tbl\nuser_item_matrix_tbl\nmapped_tbl\nscree_data_tbl\ninput_data_tbl (the original data)\n\nPlots\n\nstatic_plot\nplotly_plot\n\nNow that we have our output, let’s take a look at each individual component of the output.\nts_feature_tbl\n\noutput$data$ts_feature_tbl |> glimpse()\n\nRows: 12\nColumns: 8\n$ group_id       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ ts_x_acf1      <dbl> 0.7405485, 0.7301127, 0.7664718, 0.7151592, 0.7296932, …\n$ ts_x_acf10     <dbl> 1.552193, 1.499645, 1.622249, 1.457441, 1.479361, 1.609…\n$ ts_diff1_acf1  <dbl> -0.09954220, -0.01546920, -0.47063892, -0.25269676, -0.…\n$ ts_diff1_acf10 <dbl> 0.4742052, 0.6538733, 0.5623693, 0.4570952, 0.4168840, …\n$ ts_diff2_acf1  <dbl> -0.18227012, -0.14657790, -0.62033635, -0.55453480, -0.…\n$ ts_seas_acf1   <dbl> 0.2829677, 0.2792932, 0.2947631, 0.2931678, 0.2888097, …\n$ ts_entropy     <dbl> 0.4945771, 0.4818227, 0.6426700, 0.6798957, 0.8772866, …\n\n\nuser_item_matrix_tbl\n\noutput$data$user_item_matrix_tbl |> glimpse()\n\nRows: 12\nColumns: 8\n$ group_id       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ ts_x_acf1      <dbl> 0.7405485, 0.7301127, 0.7664718, 0.7151592, 0.7296932, …\n$ ts_x_acf10     <dbl> 1.552193, 1.499645, 1.622249, 1.457441, 1.479361, 1.609…\n$ ts_diff1_acf1  <dbl> -0.09954220, -0.01546920, -0.47063892, -0.25269676, -0.…\n$ ts_diff1_acf10 <dbl> 0.4742052, 0.6538733, 0.5623693, 0.4570952, 0.4168840, …\n$ ts_diff2_acf1  <dbl> -0.18227012, -0.14657790, -0.62033635, -0.55453480, -0.…\n$ ts_seas_acf1   <dbl> 0.2829677, 0.2792932, 0.2947631, 0.2931678, 0.2888097, …\n$ ts_entropy     <dbl> 0.4945771, 0.4818227, 0.6426700, 0.6798957, 0.8772866, …\n\n\nmapped_tbl\n\noutput$data$mapped_tbl |> glimpse()\n\nRows: 3\nColumns: 3\n$ centers <int> 1, 2, 3\n$ k_means <list> [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.7416450, 1.5450677, -0.…\n$ glance  <list> [<tbl_df[1 x 4]>], [<tbl_df[1 x 4]>], [<tbl_df[1 x 4]>]\n\n\nscree_data_tbl\n\noutput$data$scree_data_tbl |> glimpse()\n\nRows: 3\nColumns: 2\n$ centers      <int> 1, 2, 3\n$ tot.withinss <dbl> 1.8324477, 0.7364934, 0.4571258\n\n\ninput_data_tbl\n\noutput$data$input_data_tbl |> glimpse()\n\nRows: 144\nColumns: 3\n$ date_col <date> 1949-01-01, 1949-02-01, 1949-03-01, 1949-04-01, 1949-05-01, …\n$ value    <dbl> 112, 118, 132, 129, 121, 135, 148, 148, 136, 119, 104, 118, 1…\n$ group_id <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8…\n\n\nNow the plots.\nstatic_plot\n\noutput$plots$static_plot\n\n\n\n\nplotly_plot\n\noutput$plots$plotly_plot\n\n\n\n\n\nNow that we have seen the output of the ts_feature_cluster() function, let’s take a look at the output of the ts_feature_cluster_plot() function."
  },
  {
    "objectID": "posts/healthyrts-20221021/index.html#ts_feature_cluster_plot",
    "href": "posts/healthyrts-20221021/index.html#ts_feature_cluster_plot",
    "title": "Time Series Clustering with healthyR.ts",
    "section": "ts_feature_cluster_plot()",
    "text": "ts_feature_cluster_plot()\nThis function itself returns a list object of a multitude of data. First before we get into that lets look at the function call itself:\n\nts_feature_cluster_plot(\n  .data,\n  .date_col,\n  .value_col,\n  ...,\n  .center = 3,\n  .facet_ncol = 3,\n  .smooth = FALSE\n)\n\nThe data that comes back from this function is:\nData Section\n\noriginal_data\nkmm_data_tbl\nuser_item_tbl\ncluster_tbl\n\nPlots\n\nstatic_plot\nplotly_plot\n\nK-Means Object\n\nk-means object\n\nWe will go through the same exercise and show the output of all the sections. First we have to create the output. The static plot will automatically print out.\n\nplot_out <- ts_feature_cluster_plot(\n  .data = output,\n  .date_col = date_col,\n  .value_col = value,\n  .center = 2,\n  group_id\n)\n\nJoining, by = \"group_id\"\n\n\n\n\n\n\nThe Data Section:\noriginal_data\n\nplot_out$data$original_data |> glimpse()\n\nRows: 144\nColumns: 3\n$ date_col <date> 1949-01-01, 1949-02-01, 1949-03-01, 1949-04-01, 1949-05-01, …\n$ value    <dbl> 112, 118, 132, 129, 121, 135, 148, 148, 136, 119, 104, 118, 1…\n$ group_id <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8…\n\n\nkmm_data_tbl\n\nplot_out$data$kmm_data_tbl |> glimpse()\n\nRows: 3\nColumns: 3\n$ centers <int> 1, 2, 3\n$ k_means <list> [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.7416450, 1.5450677, -0.…\n$ glance  <list> [<tbl_df[1 x 4]>], [<tbl_df[1 x 4]>], [<tbl_df[1 x 4]>]\n\n\nuser_item_data\n\nplot_out$data$user_item_data |> glimpse()\n\n NULL\n\n\ncluster_tbl\n\nplot_out$data$cluster_tbl |> glimpse()\n\nRows: 12\nColumns: 9\n$ cluster        <int> 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2\n$ group_id       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ ts_x_acf1      <dbl> 0.7405485, 0.7301127, 0.7664718, 0.7151592, 0.7296932, …\n$ ts_x_acf10     <dbl> 1.552193, 1.499645, 1.622249, 1.457441, 1.479361, 1.609…\n$ ts_diff1_acf1  <dbl> -0.09954220, -0.01546920, -0.47063892, -0.25269676, -0.…\n$ ts_diff1_acf10 <dbl> 0.4742052, 0.6538733, 0.5623693, 0.4570952, 0.4168840, …\n$ ts_diff2_acf1  <dbl> -0.18227012, -0.14657790, -0.62033635, -0.55453480, -0.…\n$ ts_seas_acf1   <dbl> 0.2829677, 0.2792932, 0.2947631, 0.2931678, 0.2888097, …\n$ ts_entropy     <dbl> 0.4945771, 0.4818227, 0.6426700, 0.6798957, 0.8772866, …\n\n\n\n\nThe plot data.\nstatic_plot\n\nplot_out$plot$static_plot\n\n\n\n\nplotly_plot\n\nplot_out$plot$plotly_plot\n\n\n\n\n\n\n\nThe K-Means Object\nkmeans_object\n\nplot_out$kmeans_object\n\n[[1]]\nK-means clustering with 2 clusters of sizes 5, 7\n\nCluster means:\n  ts_x_acf1 ts_x_acf10 ts_diff1_acf1 ts_diff1_acf10 ts_diff2_acf1 ts_seas_acf1\n1 0.7456468   1.568532     0.1172685      0.4858013    -0.1799728    0.2876449\n2 0.7387865   1.528308    -0.2909349      0.3638392    -0.5916245    0.2930543\n  ts_entropy\n1  0.4918321\n2  0.6438176\n\nClustering vector:\n [1] 1 1 2 2 2 1 1 1 2 2 2 2\n\nWithin cluster sum of squares by cluster:\n[1] 0.3704304 0.3660630\n (between_SS / total_SS =  59.8 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nThanks for reading!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Simple lapply()",
    "section": "",
    "text": "This is a simple lapply example to start things off.\n\n# Let l be some list of lists, where all elements of lists are numbers\nl <- list(\n  a = 1:10,\n  b = 11:20,\n  c = 21:30\n)\n\nNow let’s take a look at our list l and see it’s structure.\n\nl\n\n$a\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$b\n [1] 11 12 13 14 15 16 17 18 19 20\n\n$c\n [1] 21 22 23 24 25 26 27 28 29 30\n\n\nNow that we see the structure, we can use the lapply function to get the sum of each list element, the mean, etc.\n\nlapply(l, sum)\n\n$a\n[1] 55\n\n$b\n[1] 155\n\n$c\n[1] 255\n\nlapply(l, mean)\n\n$a\n[1] 5.5\n\n$b\n[1] 15.5\n\n$c\n[1] 25.5\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2021-01-11/index.html",
    "href": "posts/rtip-2021-01-11/index.html",
    "title": "Reflecting on the Past Year: A LinkedIn Year in Review",
    "section": "",
    "text": "Introduction\nAs we close the door on another year, it’s always a good time to look back and reflect on the past 12 months. And what better way to do that than with a LinkedIn Year in Review?\nWhether you’re a job seeker, a business owner, or just someone who enjoys staying connected with their professional network, LinkedIn is an invaluable tool for staying up to date with the latest trends and opportunities in your field. And as we wrap up 2022, now is the perfect time to take a look at how you’ve been using LinkedIn and see where you can improve.\nSo what did your LinkedIn Year in Review have in store for you? Here are just a few of the things you might have seen:\n\nA summary of your activity on LinkedIn, including how many people you’ve connected with, how many posts you’ve shared, and how many likes, comments, and shares your content has received.\nA breakdown of the industries and job titles of the people you’ve connected with, which can give you a good idea of the types of people you’ve been spending the most time interacting with.\nA summary of your skills and endorsements, as well as which skills are most in demand in your industry.\n\nAll of these insights can be incredibly valuable as you start planning for the year ahead. With this information in hand, you can start looking for new opportunities, strengthening your existing relationships, and building new ones. And with the start of the new year, it’s a perfect time to set new goals and improve your LinkedIn profile.\nIf you haven’t yet checked out your LinkedIn Year in Review, don’t wait any longer! Log in to your account and take a look. And while you’re there, why not make a resolution to be more active on LinkedIn in the coming year? The more you put into your professional network, the more you’ll get out of it.\nSo let’s make the most of this new year, take advantage of the insights from your LinkedIn Year in Review, and make the most of your professional network. Here’s to a successful, connected, and productive 2023!\nI have done a lot of work on this already, it is not comprehensive but it is enough to understand what is happening, and I used a lot of functionality from the {healthyverse}\nDon’t forget you can also see my package and GitHub development wrap up on my year in review 2022\n\n\nAnalysis\nI will do this in chunks, as it will be easier to digest. First of course you have to get your data. I am not going to go over this process as there are many a great link just a search away.\nLet’s load in those libraries and read the files in.\n\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(lubridate)\nlibrary(timetk)\nlibrary(tidyr)\nlibrary(janitor)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(healthyR.ts)\n\nfp <- \"linkedin_content.xlsx\"\n\nengagement_tbl <- read_excel(fp, sheet = \"ENGAGEMENT\") %>%\n  clean_names() %>%\n  mutate(date = mdy(date)) %>% \n  mutate(`Engagement Rate` = (engagements / impressions) * 100) %>%\n  filter_by_time(\n    .date_var = date,\n    .end_date = \"2022-12-31\"\n  )\n\ntop_posts_tbl <- read_excel(fp, sheet = \"TOP POSTS\", skip = 1) %>%\n  clean_names()\n\nfollowers_tbl <- read_excel(fp, sheet = \"FOLLOWERS\", skip = 2) %>%\n  clean_names() %>%\n  mutate(date = mdy(date)) %>%\n  filter_by_time(\n    .date_var = date,\n    .end_date = \"2022-12-31\"\n  )\n\ndemographics_tbl <- read_excel(fp, sheet = \"DEMOGRAPHICS\") %>%\n  clean_names()\n\nNow let’s take a look at that data.\n\nglimpse(engagement_tbl)\n\nRows: 362\nColumns: 4\n$ date              <date> 2022-01-04, 2022-01-05, 2022-01-06, 2022-01-07, 202…\n$ impressions       <dbl> 3088, 3911, 3303, 3134, 1118, 799, 3068, 1954, 2663,…\n$ engagements       <dbl> 31, 56, 51, 42, 8, 4, 43, 20, 33, 43, 14, 41, 5, 17,…\n$ `Engagement Rate` <dbl> 1.0038860, 1.4318589, 1.5440509, 1.3401404, 0.715563…\n\nglimpse(top_posts_tbl)\n\nRows: 50\nColumns: 5\n$ post_url_1  <chr> \"https://www.linkedin.com/feed/update/urn:li:activity:6999…\n$ engagements <dbl> 241, 136, 123, 117, 117, 115, 107, 106, 104, 104, 95, 81, …\n$ x3          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ post_url_4  <chr> \"https://www.linkedin.com/feed/update/urn:li:activity:6999…\n$ impressions <dbl> 52300, 33903, 30752, 29887, 25953, 24139, 23769, 18522, 18…\n\nglimpse(followers_tbl)\n\nRows: 362\nColumns: 2\n$ date          <date> 2022-01-04, 2022-01-05, 2022-01-06, 2022-01-07, 2022-01…\n$ new_followers <dbl> 10, 10, 12, 5, 12, 13, 9, 8, 11, 4, 9, 6, 7, 9, 10, 11, …\n\nglimpse(demographics_tbl)\n\nRows: 30\nColumns: 3\n$ top_demographics <chr> \"Job titles\", \"Job titles\", \"Job titles\", \"Job titles…\n$ value            <chr> \"Data Scientist\", \"Data Analyst\", \"Software Engineer\"…\n$ percentage       <chr> \"0.054587073624134064\", \"0.035217467695474625\", \"0.02…\n\n\nWe are really only going to focus on the engagement_tbl and the followers_tbl as this is more of a time series analysis.\nOk, so let’s see how my Impressions, Engagements, and Engagement Rate have been.\n\nengagement_tbl %>%\n  pivot_longer(cols = -date) %>%\n  mutate(name = str_to_title(name)) %>%\n  plot_time_series(\n    .facet_vars = name,\n    .value = value,\n    .date_var = date,\n    .interactive = FALSE,\n    .smooth = FALSE,\n    .title = \"LinkedIn Stats Time Series Plot\"\n  ) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal()\n\n\n\nfollowers_tbl %>%\n  plot_time_series(\n    .value = new_followers,\n    .date_var = date,\n    .interactive = FALSE,\n    .smooth = FALSE,\n    .title = \"LinkedIn Stats Time Series Plot - New Followers\"\n  ) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal()\n\n\n\n\nYou will notice that I placed a blue line where I started my telegram channel @steveondata and a red line where I started this blog. So far, not bad, it looks like the telegram channel helped a little bit but writing on the blog seems to maybe been helping the most.\nLet’s look at a cumulative view of things.\n\nengagement_tbl %>%\n  summarise_by_time(\n    .date_var = date,\n    .by = \"month\",\n    `Cumulative Impressions` = sum(impressions),\n    `Cumulative Engagements` = sum(engagements)\n  ) %>%\n  mutate(\n    `Cumulative Impressions` = cumsum(`Cumulative Impressions`),\n    `Cumulative Engagements` = cumsum(`Cumulative Engagements`)\n  ) %>%\n  slice(1:12) %>%\n  pivot_longer(cols = -date) %>%\n  mutate(name = str_to_title(name)) %>%\n  ggplot(aes(x = date, y = value)) +\n  geom_col() +\n  facet_wrap(~ name, scales = \"free\") +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    x = \"Date\",\n    y = \"Value\",\n    title = \"LinkedIn Stats Time Series Plot\",\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal() \n\n\n\nfollowers_tbl %>%\n    summarise_by_time(\n    .date_var = date,\n    .by = \"month\",\n    `Cumulative Followers` = sum(new_followers)\n  ) %>%\n  mutate(\n    `Cumulative Followers` = cumsum(`Cumulative Followers`)\n  ) %>%\n  slice(1:12) %>%\n  ggplot(aes(x = date, y = `Cumulative Followers`)) +\n  geom_col() +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(\n    x = \"Date\",\n    y = \"Value\",\n    title = \"LinkedIn Stats Time Series Plot - New Followers\",\n    subtitle = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\"\n  ) +\n  theme_minimal() \n\n\n\n\nIt seems again that writing blog posts and sharing them daily has a nice effect. Now we are going to look at some Value, Velocity, and Acceleration plots with the ts_vva_plot() function from the {healthyR.ts} package.\n\nts_vva_plot(engagement_tbl, date, engagements)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_vva_plot(engagement_tbl, date, impressions)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_vva_plot(engagement_tbl, date, `Engagement Rate`)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_vva_plot(followers_tbl, date, new_followers)$plots$static_plot +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\n\nNow some simple moving average plots using the function ts_sma_plot() again from the {healthyR.ts} library.\n\nts_sma_plot(engagement_tbl, date, impressions, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_sma_plot(engagement_tbl, date, engagements, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_sma_plot(engagement_tbl, date, `Engagement Rate`, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\nts_sma_plot(followers_tbl, date, new_followers, .sma_order = c(7, 14, 21, 30))$plots$static_plot +\n  theme(axis.text.x = element_blank()) +\n  geom_vline(xintercept = as.Date(\"2022-07-07\"),\n             color = \"blue\", linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2022-10-05\"), \n             color = \"red\", linetype = \"dashed\") +\n  labs(caption = \"Blue Line - Start Telegram Channel, Red Line - Start Blog\")\n\n\n\n\nNow some calendar heatmaps with ts_calendar_heatmap_plot()\n\nts_calendar_heatmap_plot(engagement_tbl, date, impressions, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Impressions\")\n\n\n\nts_calendar_heatmap_plot(engagement_tbl, date, engagements, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Engagemets\")\n\n\n\nts_calendar_heatmap_plot(engagement_tbl, date, `Engagement Rate`, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - Engagement Rate\")\n\n\n\nts_calendar_heatmap_plot(followers_tbl, date, new_followers, .interactive = FALSE) +\n  labs(title = \"Calendar Heatmap - New Followers\")\n\n\n\n\nSome seasonal diagnostics using {timetk}\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, engagements, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Engagements\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, impressions, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Impressions\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(engagement_tbl\n                          , date, `Engagement Rate`, .interactive = FALSE,\n                          .feature_set = c(\"wday.lbl\", \"month.lbl\"),\n                          .title = \"Seasonal Diagnostics - Engagement Rate\") +\n  theme_minimal()\n\n\n\nplot_seasonal_diagnostics(\n  followers_tbl, date, new_followers, .interactive = FALSE, \n  .feature_set = c(\"wday.lbl\",\"month.lbl\"), \n  .title = \"Seasonal Diagnostics - New Followers\") +\n  theme_minimal()\n\n\n\n\nFinally some lag correlation plots with ts_lag_correlation().\n\nts_lag_correlation(engagement_tbl, date, engagements, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Engagements\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(engagement_tbl, date, impressions, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Impressions\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(engagement_tbl, date, `Engagement Rate`, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - Engagement Rate\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\nts_lag_correlation(followers_tbl, date, new_followers, .lags = c(7, 14, 21, 28))$plots$lag_plot +\n  labs(title = \"Lag Correlation Plot - New Followers\") +\n  geom_smooth(se = FALSE, method = \"lm\", color = \"black\", linetype = \"dashed\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-10-24/index.html",
    "href": "posts/rtip-2022-10-24/index.html",
    "title": "Cumulative Variance",
    "section": "",
    "text": "Introducton\nThis is going to be a simple example on how we can make a function in #base #r that will crate a cumulative variance function. From base R we are going to use seq_along(), stats::var(), and sapply() inside of the function we will call cvar for cumulative variance.\n\n\nGenerate Data\nThe first thing we need to do in order to showcase this function is to generate some data. Lets do that below:\n\nl <- list(\n  a = rnorm(50),\n  b = rnorm(50, 1),\n  c = rnorm(50, 2)\n)\n\nl\n\n$a\n [1] -0.96548479  0.49276394  0.14030455  1.11786377 -1.47239834 -0.06906506\n [7] -1.51133985  1.48910665  0.09444727 -0.01216806  0.35365683 -1.13562871\n[13] -1.27899694  0.10963391 -0.00708945 -1.26718573  0.92143855  0.09716551\n[19] -0.28025814 -0.18046616 -1.75919633  0.01686201 -0.10204673  0.91791398\n[25] -1.70503761  1.50856724 -1.29433294  0.42665133 -0.78176459  0.16141529\n[31]  1.42536506 -0.42168041 -0.30222269  0.05129043 -0.73717680 -1.60823604\n[37] -0.11921815  0.08357566  0.23250949  0.50846618 -0.02674088  0.12101223\n[43]  0.10390867 -1.11476987 -1.42201791 -1.35493159  0.35703193 -1.08176152\n[49] -0.08189606  0.46341303\n\n$b\n [1]  1.67636555  1.22588224  0.44445597  2.06992723  1.82473269 -0.03321279\n [7]  1.29568923 -0.29542080  1.46614555  0.51617492  2.03383464  0.13835453\n[13]  3.18982479 -0.38493278  0.67450796  1.69715532  1.19963387  1.17294403\n[19]  0.83585415  1.49308994  0.53831112  1.76345465  1.80154859  0.47358491\n[25]  1.40422472  2.50254552 -0.07376997  0.38077031  1.13606122 -0.26052567\n[31]  0.88624336  1.89232197  1.37488657  2.53211686  1.77919794  3.42367520\n[37] -0.59175356 -0.04816522  2.08963807  1.40124074 -0.73135934  0.65282741\n[43]  0.87359580  0.14540086  1.52502012  0.52190806  2.29922084  0.62462975\n[49]  2.94462210  1.06173482\n\n$c\n [1]  1.01194711  1.36267530  1.37423091  1.14980487  1.39304340  3.26911528\n [7]  1.71184232  1.88096194  2.90461886  1.39510407  1.86157191  1.14906542\n[13]  1.90072693  1.78998258  1.61307934  0.76604158  2.92366827  2.32424523\n[19]  2.94645235  2.73102591  0.87949048  3.31239943  1.05720691  1.42571354\n[25]  1.79266828  1.84627335  0.81364549  0.25976918  1.48698512  1.10254109\n[31]  1.60219278  1.84545465  1.93508206  2.13570750  2.32733075  2.53404107\n[37]  1.25864169  3.28238628  1.98998276  1.44299079  2.26296491  3.86667748\n[43]  1.84651988  3.24765507  0.18464631 -0.01404234  2.78432762 -0.05193538\n[49]  0.35160392  2.58212054\n\n\n\n\nMake Function\nNow that we have our data, lets make the function:\n\ncvar <- function(.x){\n  sapply(seq_along(.x), function(k, z) stats::var(z[1:k]), z = .x)\n}\n\nOk, now that we have our function, lets take a look at it in use.\n\n\nUse Function\n\nsapply(l, cvar)\n\n              a         b          c\n [1,]        NA        NA         NA\n [2,] 1.0632447 0.1014676 0.06150513\n [3,] 0.5789145 0.3885272 0.04239889\n [4,] 0.7633500 0.4867186 0.03075658\n [5,] 1.1294646 0.4093271 0.02873772\n [6,] 0.9043498 0.6932616 0.69685950\n [7,] 1.0277904 0.5789892 0.58271798\n [8,] 1.2918409 0.7813852 0.50862439\n [9,] 1.1344452 0.7052323 0.62156290\n[10,] 1.0088029 0.6580963 0.56764373\n[11,] 0.9242084 0.6858993 0.51210764\n[12,] 0.9418512 0.7024341 0.49623990\n[13,] 0.9661294 1.0026509 0.45782344\n[14,] 0.8992042 1.1041314 0.42295247\n[15,] 0.8371837 1.0364119 0.39358167\n[16,] 0.8556585 0.9929979 0.42396425\n[17,] 0.8822275 0.9315646 0.49164277\n[18,] 0.8344918 0.8770439 0.48215682\n[19,] 0.7888762 0.8321667 0.52875406\n[20,] 0.7473648 0.7964123 0.54171586\n[21,] 0.8305355 0.7722667 0.56162921\n[22,] 0.7940780 0.7564316 0.63535849\n[23,] 0.7587189 0.7425071 0.63686713\n[24,] 0.7802954 0.7290301 0.61692337\n[25,] 0.8409644 0.7019443 0.59130379\n[26,] 0.9248957 0.7464413 0.56765490\n[27,] 0.9359290 0.7761117 0.58464117\n[28,] 0.9159283 0.7676951 0.64765859\n[29,] 0.8952420 0.7403040 0.62681485\n[30,] 0.8690093 0.7773175 0.61856072\n[31,] 0.9251735 0.7524213 0.59834912\n[32,] 0.8976913 0.7499102 0.57961326\n[33,] 0.8702922 0.7290409 0.56296663\n[34,] 0.8452302 0.7678835 0.55094641\n[35,] 0.8301013 0.7571526 0.54480209\n[36,] 0.8638223 0.8786798 0.54627250\n[37,] 0.8400510 0.9426489 0.53823917\n[38,] 0.8195803 0.9560736 0.58478212\n[39,] 0.8028106 0.9542482 0.57032971\n[40,] 0.7943867 0.9312335 0.55895977\n[41,] 0.7750385 0.9957722 0.55033290\n[42,] 0.7581242 0.9766789 0.63799874\n[43,] 0.7417073 0.9547108 0.62281006\n[44,] 0.7453949 0.9533619 0.65240410\n[45,] 0.7629119 0.9360654 0.70195202\n[46,] 0.7747320 0.9223139 0.76179573\n[47,] 0.7652088 0.9339432 0.76550157\n[48,] 0.7645076 0.9188787 0.82293002\n[49,] 0.7490587 0.9695572 0.84800554\n[50,] 0.7434405 0.9498710 0.84419808\n\nlapply(l, cvar)\n\n$a\n [1]        NA 1.0632447 0.5789145 0.7633500 1.1294646 0.9043498 1.0277904\n [8] 1.2918409 1.1344452 1.0088029 0.9242084 0.9418512 0.9661294 0.8992042\n[15] 0.8371837 0.8556585 0.8822275 0.8344918 0.7888762 0.7473648 0.8305355\n[22] 0.7940780 0.7587189 0.7802954 0.8409644 0.9248957 0.9359290 0.9159283\n[29] 0.8952420 0.8690093 0.9251735 0.8976913 0.8702922 0.8452302 0.8301013\n[36] 0.8638223 0.8400510 0.8195803 0.8028106 0.7943867 0.7750385 0.7581242\n[43] 0.7417073 0.7453949 0.7629119 0.7747320 0.7652088 0.7645076 0.7490587\n[50] 0.7434405\n\n$b\n [1]        NA 0.1014676 0.3885272 0.4867186 0.4093271 0.6932616 0.5789892\n [8] 0.7813852 0.7052323 0.6580963 0.6858993 0.7024341 1.0026509 1.1041314\n[15] 1.0364119 0.9929979 0.9315646 0.8770439 0.8321667 0.7964123 0.7722667\n[22] 0.7564316 0.7425071 0.7290301 0.7019443 0.7464413 0.7761117 0.7676951\n[29] 0.7403040 0.7773175 0.7524213 0.7499102 0.7290409 0.7678835 0.7571526\n[36] 0.8786798 0.9426489 0.9560736 0.9542482 0.9312335 0.9957722 0.9766789\n[43] 0.9547108 0.9533619 0.9360654 0.9223139 0.9339432 0.9188787 0.9695572\n[50] 0.9498710\n\n$c\n [1]         NA 0.06150513 0.04239889 0.03075658 0.02873772 0.69685950\n [7] 0.58271798 0.50862439 0.62156290 0.56764373 0.51210764 0.49623990\n[13] 0.45782344 0.42295247 0.39358167 0.42396425 0.49164277 0.48215682\n[19] 0.52875406 0.54171586 0.56162921 0.63535849 0.63686713 0.61692337\n[25] 0.59130379 0.56765490 0.58464117 0.64765859 0.62681485 0.61856072\n[31] 0.59834912 0.57961326 0.56296663 0.55094641 0.54480209 0.54627250\n[37] 0.53823917 0.58478212 0.57032971 0.55895977 0.55033290 0.63799874\n[43] 0.62281006 0.65240410 0.70195202 0.76179573 0.76550157 0.82293002\n[49] 0.84800554 0.84419808\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-10-26/index.html",
    "href": "posts/rtip-2022-10-26/index.html",
    "title": "Control Charts in healthyR.ai",
    "section": "",
    "text": "Sometimes you may be working with a time series or some process data and you will want to make a control chart. This is simple to do with the {healthyR.ai} package.\nIf you do not already have it, then you can follow the simple code below to get the latest version.\n\n\nYou can install the released version of healthyR.ai from CRAN with:\n\ninstall.packages(\"healthyR.ai\")\n\nAnd the development version from GitHub with:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"spsanderson/healthyR.ai\")\n\nNow that we have the latest version installed, lets get some data and then use the function."
  },
  {
    "objectID": "posts/rtip-2022-10-31/index.html",
    "href": "posts/rtip-2022-10-31/index.html",
    "title": "Cumulative Skewness",
    "section": "",
    "text": "Function\nIn this post we will make a function cum_skewness() that will generate a vector output of the cumulative skewness of some given vector. The full function call is simply:\n\ncum_skewness(.x)\n\nIt only takes in a numeric vector, we are not going to write type checks in the function as it won’t be necessary for this post.\n\ncum_skewness <- function(.x){\n  skewness <- function(.x){\n    sqrt(length(.x)) * sum((.x - mean(.x))^3 / (sum((.x))^2)^(3/2))\n  }\n  sapply(seq_along(.x), function(k, z) skewness(z[1:k]), z = .x)\n}\n\n\n\nData\nWe are going to use the mtcars data set and use the mpg column for this example. Let’s set x equal to mtcars$mpg\n\nx <- mtcars$mpg\n\n\n\nExample\nNow let’s see the function in use.\n\ncum_skewness(x)\n\n [1]  0.000000e+00  0.000000e+00  8.249747e-06  5.049149e-06 -1.113787e-05\n [6] -8.569220e-06 -1.134377e-04 -8.440629e-05 -8.280585e-05 -5.457236e-05\n[11] -3.209937e-05 -1.758922e-05 -5.567456e-06  1.436318e-07 -6.299325e-05\n[16] -8.605705e-05 -5.869380e-05  1.594511e-04  1.675837e-04  2.221143e-04\n[21]  1.855217e-04  1.936299e-04  1.998527e-04  2.082240e-04  1.897575e-04\n[26]  1.505425e-04  1.180971e-04  9.974055e-05  1.048461e-04  9.801797e-05\n[31]  1.024713e-04  9.107160e-05\n\n\nLet’s plot it out.\n\nplot(cum_skewness(x), type = \"l\")"
  },
  {
    "objectID": "posts/rtip-2022-11-07/index.html",
    "href": "posts/rtip-2022-11-07/index.html",
    "title": "Discrete Fourier Vec with healthyR.ai",
    "section": "",
    "text": "Introduction\nSometimes in modeling you may want to get a discrete 1/0 vector of a fourier transform of some input vector. With {healthyR.ai} we can do this easily.\n\n\nFunction\nHere is the full function call:\n\nhai_fourier_discrete_vec(\n  .x,\n  .period,\n  .order,\n  .scale_type = c(\"sin\", \"cos\", \"sincos\")\n)\n\nHere are the parameters to the function and what they expect:\n\n.x - A numeric vector\n.period - The number of observations that complete a cycle\n.order - The fourier term order\n.scale_type - A character of one of the following: sin,cos,sincos\n\nThe internal caluclation is straightforward:\n\nsin = sin(2 * pi * h * x), where h = .order/.period\ncos = cos(2 * pi * h * x), where h = .order/.period\nsincos = sin(2 * pi * h * x) * cos(2 * pi * h * x) where h = .order/.period\n\n\n\nExample\nLet’s work throught a quick and simple example.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(healthyR.ai)\nlibrary(tidyr)\n\nlen_out <- 24\nby_unit <- \"month\"\nstart_date <- as.Date(\"2021-01-01\")\n\ndata_tbl <- tibble(\n  date_col = seq.Date(\n    from = start_date, \n    length.out = len_out, \n    by = by_unit\n  ),\n  a = rnorm(len_out, sd = 2),\n  fv_sin = hai_fourier_discrete_vec(a, 12, 1, \"sin\"),\n  fv_cos = hai_fourier_discrete_vec(a, 12, 1, \"cos\"),\n  fv_sc  = hai_fourier_discrete_vec(a, 12, 1, \"sincos\")\n)\n\ndata_tbl\n\n# A tibble: 24 × 5\n   date_col         a fv_sin fv_cos fv_sc\n   <date>       <dbl>  <dbl>  <dbl> <dbl>\n 1 2021-01-01 -0.486       0      1     0\n 2 2021-02-01 -0.708       0      1     0\n 3 2021-03-01 -0.119       0      1     0\n 4 2021-04-01  0.0405      1      1     1\n 5 2021-05-01  1.19        1      1     1\n 6 2021-06-01  1.88        1      1     1\n 7 2021-07-01 -1.32        0      1     0\n 8 2021-08-01 -0.0214      0      1     0\n 9 2021-09-01  2.80        1      1     1\n10 2021-10-01  1.67        1      1     1\n# … with 14 more rows\n\n\n\n\nVisual\nLet’s visualize.\n\ndata_tbl %>% \n  pivot_longer(cols = -date_col) %>% \n  ggplot(aes(x = date_col, y = value, color = name)) + \n  geom_line() + \n  facet_wrap(~ name, scales = \"free\") +\n  theme_minimal() +\n  labs(color = \"\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-08/index.html",
    "href": "posts/rtip-2022-11-08/index.html",
    "title": "Hyperbolic Transform with healthyR.ai",
    "section": "",
    "text": "Introduction\nIn data modeling there can be instanes where you will want some sort of hyperbolic transformation of your data. In {healthyR.ai} this is easy with the use of the function hai_hyperbolic_vec() along with it’s corresponding augment and step functions.\n\n\nFunction\nThe function takes in a numeric vector as it’s argument and will transform the data with one of the following:\n\nsin\ncos\ntan\nsincos This will do: value = sin(x) * cos(x)\n\nThe full function call is:\n\nhai_hyperbolic_vec(.x, .scale_type = c(\"sin\", \"cos\", \"tan\", \"sincos\"))\n\n\n\nExample\n\nlibrary(dplyr)\nlibrary(healthyR.ai)\nlibrary(tidyr)\nlibrary(ggplot2)\n\nlen_out <- 25\nby_unit <- \"month\"\nstart_date <- as.Date(\"2021-01-01\")\n\ndata_tbl <- tibble(\n  date_col = seq.Date(\n    from = start_date, \n    length.out = len_out, \n    by = by_unit\n    ),\n  b = runif(len_out),\n  fv_sin = hai_hyperbolic_vec(b, .scale_type = \"sin\"),\n  fv_cos = hai_hyperbolic_vec(b, .scale_type = \"cos\"),\n  fv_sc  = hai_hyperbolic_vec(b, .scale_type = \"sincos\")\n)\n\ndata_tbl\n\n# A tibble: 25 × 5\n   date_col        b fv_sin fv_cos  fv_sc\n   <date>      <dbl>  <dbl>  <dbl>  <dbl>\n 1 2021-01-01 0.961  0.820   0.573 0.470 \n 2 2021-02-01 0.418  0.406   0.914 0.371 \n 3 2021-03-01 0.0729 0.0728  0.997 0.0726\n 4 2021-04-01 0.426  0.413   0.911 0.376 \n 5 2021-05-01 0.851  0.752   0.659 0.496 \n 6 2021-06-01 0.824  0.734   0.679 0.499 \n 7 2021-07-01 0.659  0.612   0.791 0.484 \n 8 2021-08-01 0.683  0.631   0.776 0.490 \n 9 2021-09-01 0.173  0.172   0.985 0.169 \n10 2021-10-01 0.345  0.338   0.941 0.318 \n# … with 15 more rows\n\n\n\n\nVisual\n\ndata_tbl %>% \n  pivot_longer(cols = -date_col) %>% \n  ggplot(aes(x = date_col, y = value, color = name)) + \n  geom_line() + \n  facet_wrap(~ name, scales = \"free\") +\n  theme_minimal() +\n  labs(color = \"\")"
  },
  {
    "objectID": "posts/rtip-2022-11-09/index.html",
    "href": "posts/rtip-2022-11-09/index.html",
    "title": "Mapping K-Means with healthyR.ai",
    "section": "",
    "text": "K-Means is a clustering algorithm that can be used to find potential clusters in your data.\nThe algorithm does require that you look at different values of K in order to assess which is the optimal value.\nIn the R package {healthyR.ai} there is a utility to do this."
  },
  {
    "objectID": "posts/rtip-2022-11-09/index.html#parameters",
    "href": "posts/rtip-2022-11-09/index.html#parameters",
    "title": "Mapping K-Means with healthyR.ai",
    "section": "Parameters",
    "text": "Parameters\nThe parameters take the following arguments:\n\n.data - This is the data that should be an output of the hai_user_item_tbl() or it’s synonym, or should at least be in the user item matrix format.\n.centers - The maximum amount of centers you want to map to the k-means function. The default is 15."
  },
  {
    "objectID": "posts/rtip-2022-11-10/index.html",
    "href": "posts/rtip-2022-11-10/index.html",
    "title": "Reading Multiple Files with {purrr}",
    "section": "",
    "text": "Introduction\nThere may be times when you have multiple structured files in the same folder, maybe they are .csv files. For this short tip, we will say that they are.\nI will show the short script and then discuss it.\n\n# Library Load ----\nlibrary(dplyr)\nlibrary(purrr)\n\n# Set file path ----\nfolder    <- \"FileFolder\"\npath      <- \"C:/Some/Root/Path/\"\nfull_path <- paste0(path,folder,\"/\")\n\n# File List ----\nfile_list <- dir(full_path\n                 , pattern = \"\\\\.csv$\"\n                 , full.names = T)\n\n# Read Files ----\nfiles <- file_list %>%\n  map(read.csv) %>%\n  map(as_tibble)\n\n# Clean File Names ----\nfile_names <- file_list %>%\n  str_remove(full_path) %>%\n  str_replace(\n    pattern = \"_OldStuff.csv\", \n    replacement = \"_NewStuff.csv\"\n  )\n\nnames(files) <- file_names\n\nWe load in {dplyr} for the pipe and the as_tibble function. After this we set out to create the file path. I have chosen to do this in two separate pieces as I have had experience with needing to go through different folders in the same root directory. While this could further be scripted I leave it as is.\nfolder is the folder that has the files of interest, in this case the .csv files. We then get the root path to that folder but not including it, this is defined as path in the above. After we have both folder and path we can create the full_path by using paste0\nNow after this we use the base R function of dir to list out all of the files that fit the specific format of .csv with a regex pattern. I always want the name of the file as it allows me to go back to the file later and lets me name the files in the upcoming list later on.\nSince these are .csv files I use purrr::map and then read.csv to read in all of the .csv files in the list that was created, we then used map again and this time used as_tibble to make sure that each file is a tibble and not something else like data.frame\nSince I provided the argument of T to dir, full.names I can then get a character vector of the names of the files which then is applied to the file list.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-14/index.html",
    "href": "posts/rtip-2022-11-14/index.html",
    "title": "Find Skewed Features with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nSometimes we may want to quickly find skewed features in a data set. This is easily achiveable using the {healthyR.ai} library. There is a simple function called hai_skewed_features(). We are going to go over this function today.\n\n\nFunction\nLet’s first take a look at the function call.\n\nhai_skewed_features(\n  .data, \n  .threshold = 0.6, \n  .drop_keys = NULL\n  )\n\nNow let’s take a look at the arguments that go to the parameters of the function.\n\n.data - The data.frame/tibble you are passing in.\n.threshold - A level of skewness that indicates where you feel a column should be considered skewed.\n.drop_keys - A c() character vector of columns you do not want passed to the function.\n\n\n\nExample\nHere are a couple of examples.\n\nlibrary(healthyR.ai)\n\nhai_skewed_features(mtcars)\n\n[1] \"mpg\"  \"hp\"   \"carb\"\n\nhai_skewed_features(mtcars, .drop_keys = \"hp\")\n\n[1] \"mpg\"  \"carb\""
  },
  {
    "objectID": "posts/rtip-2022-11-15/index.html",
    "href": "posts/rtip-2022-11-15/index.html",
    "title": "Auto Prep data for XGBoost with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nSometimes we may want to quickly format some data in order to just pass it through some algorithm just to see what happens, how crazy are things, just to get an idea of what may lie ahead…a lot of prep.\nWith my r package {healthyR.ai} there is a set of prepper functions that will automatically do a ‘best effort’ to format you data to be used in the algorithm you choose (should it be supported).\nToday we will talk about [hai_xgboost_data_prepper()\n\n\nFunction\nLet’s take a look at the function call.\n\nhai_xgboost_data_prepper(.data, .recipe_formula)\n\nNow let’s go over the arguments that are passed to the function.\n\n.data - The data that you are passing to the function. Can be any type of data that is accepted by the data parameter of the recipes::reciep() function.\n.recipe_formula - The formula that is going to be passed. For example if you are using the diamonds data then the formula would most likely be something like price ~ .\n\n\n\nExample\nLet’s go over some examples.\n\nlibrary(ggplot2)\nlibrary(healthyR.ai)\n\n# Regression\nhai_xgboost_data_prepper(.data = diamonds, .recipe_formula = price ~ .)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          9\n\nOperations:\n\nFactor variables from tidyselect::vars_select_helpers$where(is.charac...\nNovel factor level assignment for recipes::all_nominal_predictors()\nDummy variables from recipes::all_nominal_predictors()\nZero variance filter on recipes::all_predictors()\n\nreg_obj <- hai_xgboost_data_prepper(diamonds, price ~ .)\nget_juiced_data(reg_obj)\n\n# A tibble: 53,940 × 27\n   carat depth table     x     y     z price  cut_1  cut_2  cut_3  cut_4   cut_5\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <int>  <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n 1  0.23  61.5    55  3.95  3.98  2.43   326  0.359 -0.109 -0.522 -0.567 -0.315 \n 2  0.21  59.8    61  3.89  3.84  2.31   326  0.120 -0.436 -0.298  0.378  0.630 \n 3  0.23  56.9    65  4.05  4.07  2.31   327 -0.359 -0.109  0.522 -0.567  0.315 \n 4  0.29  62.4    58  4.2   4.23  2.63   334  0.120 -0.436 -0.298  0.378  0.630 \n 5  0.31  63.3    58  4.34  4.35  2.75   335 -0.359 -0.109  0.522 -0.567  0.315 \n 6  0.24  62.8    57  3.94  3.96  2.48   336 -0.120 -0.436  0.298  0.378 -0.630 \n 7  0.24  62.3    57  3.95  3.98  2.47   336 -0.120 -0.436  0.298  0.378 -0.630 \n 8  0.26  61.9    55  4.07  4.11  2.53   337 -0.120 -0.436  0.298  0.378 -0.630 \n 9  0.22  65.1    61  3.87  3.78  2.49   337 -0.598  0.546 -0.373  0.189 -0.0630\n10  0.23  59.4    61  4     4.05  2.39   338 -0.120 -0.436  0.298  0.378 -0.630 \n# … with 53,930 more rows, and 15 more variables: color_1 <dbl>, color_2 <dbl>,\n#   color_3 <dbl>, color_4 <dbl>, color_5 <dbl>, color_6 <dbl>, color_7 <dbl>,\n#   clarity_1 <dbl>, clarity_2 <dbl>, clarity_3 <dbl>, clarity_4 <dbl>,\n#   clarity_5 <dbl>, clarity_6 <dbl>, clarity_7 <dbl>, clarity_8 <dbl>\n\n# Classification\nhai_xgboost_data_prepper(Titanic, Survived ~ .)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nOperations:\n\nFactor variables from tidyselect::vars_select_helpers$where(is.charac...\nNovel factor level assignment for recipes::all_nominal_predictors()\nDummy variables from recipes::all_nominal_predictors()\nZero variance filter on recipes::all_predictors()\n\ncla_obj <- hai_xgboost_data_prepper(Titanic, Survived ~ .)\nget_juiced_data(cla_obj)\n\n# A tibble: 32 × 7\n       n Survived Class_X2nd Class_X3rd Class_Crew Sex_Male Age_Child\n   <dbl> <fct>         <dbl>      <dbl>      <dbl>    <dbl>     <dbl>\n 1     0 No                0          0          0        1         1\n 2     0 No                1          0          0        1         1\n 3    35 No                0          1          0        1         1\n 4     0 No                0          0          1        1         1\n 5     0 No                0          0          0        0         1\n 6     0 No                1          0          0        0         1\n 7    17 No                0          1          0        0         1\n 8     0 No                0          0          1        0         1\n 9   118 No                0          0          0        1         0\n10   154 No                1          0          0        1         0\n# … with 22 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-16/index.html",
    "href": "posts/rtip-2022-11-16/index.html",
    "title": "Cumulative Harmonic Mean with {TidyDensity}",
    "section": "",
    "text": "Introduction\nThere can be times in which you may want to see a cumulative statistic, maybe in this particular case it is the harmonic mean. Well with the {TidyDensity} it is possible with a function called chmean()\nLet’s take a look at the function.\n\n\nFunction\nHere is the function call, it is very simple as it is a vectorized function.\n\nchmean(.x)\n\nThe only argument you provide to this function is a numeric vector. Let’s take a quick look at the construction of the function.\n\nchmean <- function(.x) {\n  1 / (cumsum(1 / .x))\n}\n\n\n\nExamples\nLet’s take a look at an example.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\nx <- mtcars$mpg\n\nchmean(x)\n\n [1] 21.0000000 10.5000000  7.1891892  5.3813575  4.1788087  3.3949947\n [7]  2.7436247  2.4663044  2.2255626  1.9943841  1.7934398  1.6166494\n[13]  1.4784877  1.3474251  1.1928760  1.0701322  0.9975150  0.9677213\n[19]  0.9378663  0.9126181  0.8754572  0.8286539  0.7858140  0.7419753\n[25]  0.7143688  0.6961523  0.6779989  0.6632076  0.6364908  0.6165699\n[31]  0.5922267  0.5762786\n\nmtcars %>%\n  select(mpg) %>%\n  mutate(cum_har_mean = chmean(mpg)) %>%\n  head(10)\n\n                   mpg cum_har_mean\nMazda RX4         21.0    21.000000\nMazda RX4 Wag     21.0    10.500000\nDatsun 710        22.8     7.189189\nHornet 4 Drive    21.4     5.381358\nHornet Sportabout 18.7     4.178809\nValiant           18.1     3.394995\nDuster 360        14.3     2.743625\nMerc 240D         24.4     2.466304\nMerc 230          22.8     2.225563\nMerc 280          19.2     1.994384\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-17/index.html",
    "href": "posts/rtip-2022-11-17/index.html",
    "title": "Bootstrap Modeling with {purrr} and {modler}",
    "section": "",
    "text": "Introduction\nMany times in modeling we want to get the uncertainty in the model, well, bootstrapping to the rescue!\nI am going to go over a very simple example on how to use purrr and modelr for this situation. We will use the mtcars dataset.\n\n\nFunctions\nThe main functions that we are going to showcase are purrr::map() and modelr::bootstrap()\n\n\nExamples\nLet’s get right into it.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\ndf <- mtcars\n\nfit_boots <- df %>% \n  modelr::bootstrap(n = 200, id = 'boot_num') %>%\n  group_by(boot_num) %>%\n  mutate(fit = map(strap, ~lm(mpg ~ ., data = data.frame(.))))\n\nfit_boots\n\n# A tibble: 200 × 3\n# Groups:   boot_num [200]\n   strap                boot_num fit   \n   <list>               <chr>    <list>\n 1 <resample [32 x 11]> 001      <lm>  \n 2 <resample [32 x 11]> 002      <lm>  \n 3 <resample [32 x 11]> 003      <lm>  \n 4 <resample [32 x 11]> 004      <lm>  \n 5 <resample [32 x 11]> 005      <lm>  \n 6 <resample [32 x 11]> 006      <lm>  \n 7 <resample [32 x 11]> 007      <lm>  \n 8 <resample [32 x 11]> 008      <lm>  \n 9 <resample [32 x 11]> 009      <lm>  \n10 <resample [32 x 11]> 010      <lm>  \n# … with 190 more rows\n\n\nNow lets get our parameter estimates.\n\n# get parameters ####\nparams_boot <- fit_boots %>%\n  mutate(tidy_fit = map(fit, tidy)) %>%\n  unnest(cols = tidy_fit) %>%\n  ungroup()\n\n# get predictions\npreds_boot <- fit_boots %>%\n  mutate(augment_fit = map(fit, augment)) %>%\n  unnest(cols = augment_fit) %>%\n  ungroup()\n\nTime to visualize.\n\nlibrary(patchwork)\n\n# plot distribution of estimated parameters\np1 <- ggplot(params_boot, aes(estimate)) +\n  geom_histogram(col = 'black', fill = 'white') +\n  facet_wrap(~ term, scales = 'free') +\n  theme_minimal()\n\n# plot points with predictions\np2 <- ggplot() +\n  geom_line(aes(mpg, .fitted, group = boot_num), preds_boot, alpha = .03) +\n  geom_point(aes(mpg, .fitted), preds_boot, col = 'steelblue', alpha = 0.05) +\n  theme_minimal()\n  \n# plot both\np1 + p2\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-21/index.html",
    "href": "posts/rtip-2022-11-21/index.html",
    "title": "Bootstrap Modeling with Base R",
    "section": "",
    "text": "Introduction\nI have previously written about bootstrap modeling with {purrr} and {modelr} here. What if you would like to do some simple bootstrap modeling without importing a library? This itself is easy too!\n\n\nExample\nWe will be using a very simple for loop to accomplish this. You will find an excellent post on this on Stats StackExchange from Francisco Jos Goerlich Gisbert\n\nn    <- 2000\ndf   <- mtcars\npred <- numeric(0)\n\nlibrary(tictoc) # for timing\n\ntic()\nset.seed(123)\nfor (i in 1:n){\n  boot    <- sample(nrow(df), n, replace = TRUE)\n  fit     <- lm(mpg ~ wt, data = df[boot,])\n  pred[i] <- predict(fit, newdata = df[boot,]) +\n    sample(resid(fit), size = 1)\n}\ntoc()\n\n6.8 sec elapsed\n\n\nSo we can see that the process ran pretty quickly and the loop itself is not a very difficult one. Let’s explain a little.\nSo the boot object is a sampling of df which in this case is the mtcars data set. We took a sample with replacement from this data set. We took 2000 samples and did this 2000 times.\nNext we made the fit object by fitting a simple linear model to the data where mpg is a function of wt. Once this is done, we made out predictions.\nThat’s it!"
  },
  {
    "objectID": "posts/rtip-2022-11-22/index.html",
    "href": "posts/rtip-2022-11-22/index.html",
    "title": "Data Preprocessing Scale/Normalize with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nA large portion of data modeling occurrs not only in the data cleaning phase but also in the data preprocessing phase. This can include things like scaling or normalizing data before proceeding to the modeling phase. I will discuss one such function from my r package {healthyR.ai}. In this post I will go over hai_data_scale()\nThis is a {recipes} style step function and is tidymodels compliant.\n\n\nFunction\nLet’s take a look at the function call.\n\nhai_data_scale(\n  .recipe_object = NULL,\n  ...,\n  .type_of_scale = \"center\",\n  .range_min = 0,\n  .range_max = 1,\n  .scale_factor = 1\n)\n\nNow let’s go over the arguments that get supplied to the parameters of this function.\n\n.recipe_object - The data that you want to process\n... - One or more selector functions to choose variables to be imputed. When used with imp_vars, these dots indicate which variables are used to predict the missing data in each variable. See selections() for more details\n.type_of_scale - This is a quoted argument and can be one of the following:\n\n“center”\n“normalize”\n“range”\n“scale”\n\nrange_min - A single numeric value for the smallest value in the range. This defaults to 0.\n.range_max - A single numeric value for the largeest value in the range. This defaults to 1.\n.scale_factor - A numeric value of either 1 or 2 that scales the numeric inputs by one or two standard deviations. By dividing by two standard deviations, the coefficients attached to continuous predictors can be interpreted the same way as with binary inputs. Defaults to 1.\n\n\n\nExample\nNow let’s see it in action!\n\nlibrary(healthyR.ai)\nlibrary(dplyr)\nlibrary(recipes)\n\ndate_seq <- seq.Date(\n  from = as.Date(\"2013-01-01\"), \n  length.out = 100, \n  by = \"month\"\n)\n\nval_seq <- rep(rnorm(10, mean = 6, sd = 2), times = 10)\n\ndf_tbl <- tibble(\n  date_col = date_seq,\n  value    = val_seq\n)\n\ndf_tbl\n\n# A tibble: 100 × 2\n   date_col   value\n   <date>     <dbl>\n 1 2013-01-01  6.66\n 2 2013-02-01  6.66\n 3 2013-03-01  5.09\n 4 2013-04-01  6.94\n 5 2013-05-01  5.96\n 6 2013-06-01  6.18\n 7 2013-07-01  3.62\n 8 2013-08-01  7.31\n 9 2013-09-01  4.58\n10 2013-10-01  7.29\n# … with 90 more rows\n\nrec_obj <- recipe(value ~ ., df_tbl)\n\nnew_rec_obj <- hai_data_scale(\n  .recipe_object = rec_obj,\n  value,\n  .type_of_scale = \"center\"\n)$scale_rec_obj\n\nnew_rec_obj %>% \n  get_juiced_data()\n\n# A tibble: 100 × 2\n   date_col     value\n   <date>       <dbl>\n 1 2013-01-01  0.633 \n 2 2013-02-01  0.630 \n 3 2013-03-01 -0.935 \n 4 2013-04-01  0.909 \n 5 2013-05-01 -0.0676\n 6 2013-06-01  0.149 \n 7 2013-07-01 -2.41  \n 8 2013-08-01  1.28  \n 9 2013-09-01 -1.45  \n10 2013-10-01  1.26  \n# … with 90 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-28/index.html",
    "href": "posts/rtip-2022-11-28/index.html",
    "title": "Default Metric Sets with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nWhen modeling it is always good to understand your model performance against some metric The {tidymodels} package {yardstick} is a great resource for this.\nIn my R package {healthyR.ai} there are two functions that allow you to either minimize or maximize some cost function against your modeling problem.\nThese functions are: * hai_default_regression_metric_set() * hai_default_classification_metric_set()\n\n\nFunction\nThe functions themselves are {yardstick} metric set functions. Let’s take a look at them.\n\nlibrary(healthyR.ai)\n\nhai_default_classification_metric_set()\n\n# A tibble: 11 × 3\n   metric       class        direction\n   <chr>        <chr>        <chr>    \n 1 sensitivity  class_metric maximize \n 2 specificity  class_metric maximize \n 3 recall       class_metric maximize \n 4 precision    class_metric maximize \n 5 mcc          class_metric maximize \n 6 accuracy     class_metric maximize \n 7 f_meas       class_metric maximize \n 8 kap          class_metric maximize \n 9 ppv          class_metric maximize \n10 npv          class_metric maximize \n11 bal_accuracy class_metric maximize \n\nhai_default_regression_metric_set()\n\n# A tibble: 6 × 3\n  metric class          direction\n  <chr>  <chr>          <chr>    \n1 mae    numeric_metric minimize \n2 mape   numeric_metric minimize \n3 mase   numeric_metric minimize \n4 smape  numeric_metric minimize \n5 rmse   numeric_metric minimize \n6 rsq    numeric_metric maximize \n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-29/index.html",
    "href": "posts/rtip-2022-11-29/index.html",
    "title": "Working with Lists",
    "section": "",
    "text": "Introduction\nIn R there are many times where we will work with lists. I won’t go into why lists are great or really the structure of a list but rather simply working with them.\n\n\nExample\nFirst let’s make a list.\n\nl <- list(\n  letters,\n  1:26,\n  rnorm(26)\n)\n\nl\n\n[[1]]\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n[[2]]\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26\n\n[[3]]\n [1] -1.5647537840 -1.3080486753  1.3331315389 -0.5490502644 -0.4467608750\n [6] -1.5876952894  0.2292049732 -0.2885449316  1.4614499298 -0.0864987690\n[11]  0.5686850031 -0.3897819578  0.1776603862 -1.1326372302 -1.8651290164\n[16]  1.2676006036  0.2405115523 -1.0506728047  1.4069277686 -1.0125778892\n[21] -0.7687818102 -0.1325350681  0.3639485041  0.0005700058 -1.0698214370\n[26]  1.1972767040\n\n\nNow let’s look at somethings we can do with lists. First, let’s see if we can get the class of each item in the list. We are going to use lapply() for this.\n\nlapply(l, class)\n\n[[1]]\n[1] \"character\"\n\n[[2]]\n[1] \"integer\"\n\n[[3]]\n[1] \"numeric\"\n\n\nNow, let’s perform some simple operations on each item of the list.\n\nlapply(l, length)\n\n[[1]]\n[1] 26\n\n[[2]]\n[1] 26\n\n[[3]]\n[1] 26\n\ntry(lapply(l, sum))\n\nError in FUN(X[[i]], ...) : invalid 'type' (character) of argument\n\n\nOk so we see taking the sum of the first element of the list in lapply() did not work because of a class type mismatch. Let’s see how we can get around this an only apply the sum function to a numeric type. To do this we can rely on {purrr} by using a function map_if()\n\nlibrary(purrr)\n\nmap_if(l, is.numeric, sum)\n\n[[1]]\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n[[2]]\n[1] 351\n\n[[3]]\n[1] -5.006323\n\n\n\nmap_if(l, is.numeric, mean)\n\n[[1]]\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n[[2]]\n[1] 13.5\n\n[[3]]\n[1] -0.1925509\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-30/index.html",
    "href": "posts/rtip-2022-11-30/index.html",
    "title": "Generate Random Walk Data with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nGenerating random walk data for timesieries analysis does not have to be difficult, and in fact is not. It can be generated for multiple simulations and have a tidy output. How? ts_random_walk() from the {healthyR.ts} package. Let’s take a look at the function.\n\n\nFunction\nHere is the full function call.\n\nts_random_walk(\n  .mean = 0,\n  .sd = 0.1,\n  .num_walks = 100,\n  .periods = 100,\n  .initial_value = 1000\n)\n\nNow let’s look at the arguments to the parameters.\n\n.mean - The desired mean of the random walks\n.sd - The standard deviation of the random walks\n.num_walks - The number of random walks you want generated\n.periods - The length of the random walk(s) you want generated\n.initial_value - The initial value where the random walks should start\n\nThe underlying data of this function is generated by rnorm()\n\n\nExample\nLet’s take a look at an example and see some visuals.\n\nlibrary(healthyR.ts)\nlibrary(ggplot2)\n\ndf <- ts_random_walk(.num_walks = 100)\n\ndf\n\n# A tibble: 10,000 × 4\n     run     x        y cum_y\n   <dbl> <dbl>    <dbl> <dbl>\n 1     1     1 -0.144    856.\n 2     1     2  0.00648  862.\n 3     1     3  0.0726   924.\n 4     1     4 -0.152    784.\n 5     1     5  0.0228   802.\n 6     1     6 -0.0455   765.\n 7     1     7  0.0972   840.\n 8     1     8 -0.234    643.\n 9     1     9 -0.0501   611.\n10     1    10 -0.0358   589.\n# … with 9,990 more rows\n\n\nThere are attributes attached to the output of this function, let’s see what they are.\n\natb <- attributes(df)\n\nnames_to_print <- names(atb)[which(names(atb) != \"row.names\")]\n\natb[names_to_print]\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$names\n[1] \"run\"   \"x\"     \"y\"     \"cum_y\"\n\n$.mean\n[1] 0\n\n$.sd\n[1] 0.1\n\n$.num_walks\n[1] 100\n\n$.periods\n[1] 100\n\n$.initial_value\n[1] 1000\n\n\nNow lets visualize.\n\ndf %>%\n   ggplot(\n       mapping = aes(\n           x = x\n           , y = cum_y\n           , color = factor(run)\n           , group = factor(run)\n        )\n    ) +\n    geom_line(alpha = 0.8) +\n    ts_random_walk_ggplot_layers(df)\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-01/index.html",
    "href": "posts/rtip-2022-12-01/index.html",
    "title": "Extract Boilerplate Workflow Metrics with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nWhen working with the {tidymodels} framework there are ways to pull model metrics from a workflow, since {healthyR.ai} is built on and around the {tidyverse} and {tidymodels} we can do the same. This post will focus on the function hai_auto_wflw_metrics()\n\n\nFunction\nLet’s take a look at the function call.\n\nhai_auto_wflw_metrics(.data)\n\nThe only parameter is .data and this is strictly the output object of one of the hai_auto_ boiler plate functions\n\n\nExample\nSince this function requires the input from an hai_auto function, we will walk through an example with the iris data set. We are going to use the hai_auto_knn() to classify the Species.\n\nlibrary(healthyR.ai)\n\ndata <- iris\n\nrec_obj <- hai_knn_data_prepper(data, Species ~ .)\n\nauto_knn <- hai_auto_knn(\n  .data = data,\n  .rec_obj = rec_obj,\n  .best_metric = \"f_meas\",\n  .model_type = \"classification\",\n  .grid_size = 2,\n  .num_cores = 4\n)\n\nhai_auto_wflw_metrics(auto_knn)\n\n# A tibble: 22 × 9\n   neighbors weight_func dist_power .metric  .esti…¹  mean     n std_err .config\n       <int> <chr>            <dbl> <chr>    <chr>   <dbl> <int>   <dbl> <chr>  \n 1         8 rank             0.888 accuracy multic… 0.95     25 0.00652 Prepro…\n 2         8 rank             0.888 bal_acc… macro   0.962    25 0.00471 Prepro…\n 3         8 rank             0.888 f_meas   macro   0.947    25 0.00649 Prepro…\n 4         8 rank             0.888 kap      multic… 0.922    25 0.0102  Prepro…\n 5         8 rank             0.888 mcc      multic… 0.925    25 0.00964 Prepro…\n 6         8 rank             0.888 npv      macro   0.975    25 0.00351 Prepro…\n 7         8 rank             0.888 ppv      macro   0.949    25 0.00663 Prepro…\n 8         8 rank             0.888 precisi… macro   0.949    25 0.00663 Prepro…\n 9         8 rank             0.888 recall   macro   0.949    25 0.00633 Prepro…\n10         8 rank             0.888 sensiti… macro   0.949    25 0.00633 Prepro…\n# … with 12 more rows, and abbreviated variable name ¹​.estimator\n\n\nAs we see this pulls out the full metric table from the workflow.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-05/index.html",
    "href": "posts/rtip-2022-12-05/index.html",
    "title": "Naming Items in a List with {purrr}, {dplyr}, or {healthyR}",
    "section": "",
    "text": "Introduction\nMany times when we are working with a data set we will want to break it up into groups and place them into a list and work with them in that fashion. With this it can be useful to the elements of the list named by the column that the data was split upon. Let’s use the iris set as an example where we split on Species.\nThere are two main functions that we will use in this scenario, namely purrr:map() and dplyr::group_split(), you could also use the split function from base r for this.\nWe will also go over how simple this is using the {healthyR} package. Let’s look at the function from {healthyR}\n\n\nFunction\nFull function call.\n\nnamed_item_list(.data, .group_col)\n\nThere are only two arguments to supply.\n\n.data - The data.frame/tibble.\n.group_col - The column that contains the groupings.\n\nThat’s it.\n\n\nExamples\nLet’s jump into it.\n\nlibrary(purrr)\nlibrary(dplyr)\n\ndata_tbl <- iris\n\ndata_tbl_list <- data_tbl %>%\n  group_split(Species)\n\ndata_tbl_list\n\n<list_of<\n  tbl_df<\n    Sepal.Length: double\n    Sepal.Width : double\n    Petal.Length: double\n    Petal.Width : double\n    Species     : factor<fb977>\n  >\n>[3]>\n[[1]]\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 40 more rows\n\n[[2]]\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n 1          7           3.2          4.7         1.4 versicolor\n 2          6.4         3.2          4.5         1.5 versicolor\n 3          6.9         3.1          4.9         1.5 versicolor\n 4          5.5         2.3          4           1.3 versicolor\n 5          6.5         2.8          4.6         1.5 versicolor\n 6          5.7         2.8          4.5         1.3 versicolor\n 7          6.3         3.3          4.7         1.6 versicolor\n 8          4.9         2.4          3.3         1   versicolor\n 9          6.6         2.9          4.6         1.3 versicolor\n10          5.2         2.7          3.9         1.4 versicolor\n# … with 40 more rows\n\n[[3]]\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>    \n 1          6.3         3.3          6           2.5 virginica\n 2          5.8         2.7          5.1         1.9 virginica\n 3          7.1         3            5.9         2.1 virginica\n 4          6.3         2.9          5.6         1.8 virginica\n 5          6.5         3            5.8         2.2 virginica\n 6          7.6         3            6.6         2.1 virginica\n 7          4.9         2.5          4.5         1.7 virginica\n 8          7.3         2.9          6.3         1.8 virginica\n 9          6.7         2.5          5.8         1.8 virginica\n10          7.2         3.6          6.1         2.5 virginica\n# … with 40 more rows\n\ndata_tbl_list %>%\n   map( ~ pull(., Species)) %>%\n   map( ~ as.character(.)) %>%\n   map( ~ unique(.))\n\n[[1]]\n[1] \"setosa\"\n\n[[2]]\n[1] \"versicolor\"\n\n[[3]]\n[1] \"virginica\"\n\n\nNow lets go ahead and apply the names.\n\nnames(data_tbl_list) <- data_tbl_list %>%\n   map( ~ pull(., Species)) %>%\n   map( ~ as.character(.)) %>%\n   map( ~ unique(.))\n\ndata_tbl_list\n\n<list_of<\n  tbl_df<\n    Sepal.Length: double\n    Sepal.Width : double\n    Petal.Length: double\n    Petal.Width : double\n    Species     : factor<fb977>\n  >\n>[3]>\n$setosa\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 40 more rows\n\n$versicolor\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n 1          7           3.2          4.7         1.4 versicolor\n 2          6.4         3.2          4.5         1.5 versicolor\n 3          6.9         3.1          4.9         1.5 versicolor\n 4          5.5         2.3          4           1.3 versicolor\n 5          6.5         2.8          4.6         1.5 versicolor\n 6          5.7         2.8          4.5         1.3 versicolor\n 7          6.3         3.3          4.7         1.6 versicolor\n 8          4.9         2.4          3.3         1   versicolor\n 9          6.6         2.9          4.6         1.3 versicolor\n10          5.2         2.7          3.9         1.4 versicolor\n# … with 40 more rows\n\n$virginica\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>    \n 1          6.3         3.3          6           2.5 virginica\n 2          5.8         2.7          5.1         1.9 virginica\n 3          7.1         3            5.9         2.1 virginica\n 4          6.3         2.9          5.6         1.8 virginica\n 5          6.5         3            5.8         2.2 virginica\n 6          7.6         3            6.6         2.1 virginica\n 7          4.9         2.5          4.5         1.7 virginica\n 8          7.3         2.9          6.3         1.8 virginica\n 9          6.7         2.5          5.8         1.8 virginica\n10          7.2         3.6          6.1         2.5 virginica\n# … with 40 more rows\n\n\nLet’s now see how we do this in {healthyR}\n\nlibrary(healthyR)\n\nnamed_item_list(iris, Species)\n\n<list_of<\n  tbl_df<\n    Sepal.Length: double\n    Sepal.Width : double\n    Petal.Length: double\n    Petal.Width : double\n    Species     : factor<fb977>\n  >\n>[3]>\n$setosa\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 40 more rows\n\n$versicolor\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n 1          7           3.2          4.7         1.4 versicolor\n 2          6.4         3.2          4.5         1.5 versicolor\n 3          6.9         3.1          4.9         1.5 versicolor\n 4          5.5         2.3          4           1.3 versicolor\n 5          6.5         2.8          4.6         1.5 versicolor\n 6          5.7         2.8          4.5         1.3 versicolor\n 7          6.3         3.3          4.7         1.6 versicolor\n 8          4.9         2.4          3.3         1   versicolor\n 9          6.6         2.9          4.6         1.3 versicolor\n10          5.2         2.7          3.9         1.4 versicolor\n# … with 40 more rows\n\n$virginica\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>    \n 1          6.3         3.3          6           2.5 virginica\n 2          5.8         2.7          5.1         1.9 virginica\n 3          7.1         3            5.9         2.1 virginica\n 4          6.3         2.9          5.6         1.8 virginica\n 5          6.5         3            5.8         2.2 virginica\n 6          7.6         3            6.6         2.1 virginica\n 7          4.9         2.5          4.5         1.7 virginica\n 8          7.3         2.9          6.3         1.8 virginica\n 9          6.7         2.5          5.8         1.8 virginica\n10          7.2         3.6          6.1         2.5 virginica\n# … with 40 more rows\n\n\nIf you use this in conjunction with the healthyR function save_to_excel() then it will write an excel file with a tab for each named item in the list.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-06/index.html",
    "href": "posts/rtip-2022-12-06/index.html",
    "title": "Z-Score Scaling Step Recipe with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nSometimes one may find it useful or necessary to scale their data during a modeling or analysis phase. One of these such transformations is the z-score scaling.\nThis is done simply by performing the below transform where x is simply some numeric vector:\n\\[ z_x = (x - mu(x))/sd(x) \\]\nLet’s take a look at the recipe function called step_hai_scale_zscore\n\n\nFunction\nHere is the full function call:\n\nstep_hai_scale_zscore(\n  recipe,\n  ...,\n  role = \"predictor\",\n  trained = FALSE,\n  columns = NULL,\n  skip = FALSE,\n  id = rand_id(\"hai_scale_zscore\")\n)\n\nHere are the arguments to the function.\n\nrecipe - A recipe object. The step will be added to the sequence of operations for this recipe.\n... - One or more selector functions to choose which variables that will be used to create the new variables. The selected variables should have class numeric\nrole - For model terms created by this step, what analysis role should they be assigned?. By default, the function assumes that the new variable columns created by the original variables will be used as predictors in a model.\ntrained - A logical to indicate if the quantities for preprocessing have been estimated.\ncolumns - A character string of variables that will be used as inputs. This field is a placeholder and will be populated once recipes::prep() is used.\nskip - A logical. Should the step be skipped when the recipe is baked by bake.recipe()? While all operations are baked when prep.recipe() is run, some operations may not be able to be conducted on new data (e.g. processing the outcome variable(s)). Care should be taken when using skip = TRUE as it may affect the computations for subsequent operations.\nid - A character string that is unique to this step to identify it.\n\n\n\nExample\nHere is a simple example.\n\nlibrary(healthyR.ai)\nlibrary(dplyr)\nlibrary(recipes)\n\ndf <- iris |>\n  as_tibble() |>\n  select(Species, Sepal.Length)\n\nrec_obj <- recipe(Sepal.Length ~ ., data = df) %>%\n  step_hai_scale_zscore(Sepal.Length)\n\nrec_obj\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          1\n\nOperations:\n\nZero-One Scale Transformation on Sepal.Length\n\nsummary(rec_obj)\n\n# A tibble: 2 × 4\n  variable     type      role      source  \n  <chr>        <list>    <chr>     <chr>   \n1 Species      <chr [3]> predictor original\n2 Sepal.Length <chr [2]> outcome   original\n\n\nNow let’s take a look at the differences.\n\nlibrary(ggplot2)\nlibrary(plotly)\n\ndf_tbl <- get_juiced_data(rec_obj)\n\ndf_tbl |>\n  purrr::set_names(\"Species\",\"Sepal_Length\",\"Scaled_Sepal_Length\") |>\n  ggplot(aes(x = Sepal_Length)) +\n  geom_histogram(color = \"black\", fill = \"lightgreen\") +\n  geom_histogram(aes(x = Scaled_Sepal_Length), \n                 color = \"black\", fill = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    y = \"Count\",\n    x = \"Sepal Length\",\n    title = \"Speal.Length: Original vs. Z-Score Scaled\",\n    subtitle = \"Original (Light Green) Scaled (Steelblue)\"\n  )\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-07/index.html",
    "href": "posts/rtip-2022-12-07/index.html",
    "title": "Create Multiple {parsnip} Model Specs with {purrr}",
    "section": "",
    "text": "Introduction\nIf you want to generate multiple parsnip model specifications at the same time then it’s really not to hard. This sort of thing is being addressed in an upcoming package of mine called {tidyaml}\nThis post is going to be quick and simple, I will showcase how you can generate many different model specifications in one go. I will also discuss the function create_model_spec() that will allow you to do this with a simple function call once the package is actually released.\n\n\nFunction\nHere is the function call for the create_model_spec() for once it is release.\n\ncreate_model_spec(\n  .parsnip_eng = list(\"lm\"),\n  .mode = list(\"regression\"),\n  .parsnip_fns = list(\"linear_reg\"),\n  .return_tibble = TRUE\n)\n\nHere are the arguments to the function. * .parsnip_eng - The input must be a list. The default for this is set to all. This means that all of the parsnip linear regression engines will be used, for example lm, or glm. You can also choose to pass a c() vector like c(‘lm’, ‘glm’) * .mode- The input must be a list. The default is ‘regression’ * .parsnip_fns - The input must be a list. The default for this is set to all. This means that all of the parsnip linear regression functions will be used, for example linear_reg(), or cubist_rules. You can also choose to pass a c() vector like c(“linear_reg”,“cubist_rules”) * .return_tibble - The default is TRUE. FALSE will return a list object.\n\n\nExample\nHere is the function at work.\n\nlibrary(tidyaml)\n\ncreate_model_spec(\n .parsnip_eng = list(\"lm\",\"glm\",\"glmnet\",\"cubist\"),\n .parsnip_fns = list(\n      rep(\n        \"linear_reg\", 3),\n        \"cubist_rules\"\n     )\n )\n\n# A tibble: 4 × 4\n  .parsnip_engine .parsnip_mode .parsnip_fns .model_spec\n  <chr>           <chr>         <chr>        <list>     \n1 lm              regression    linear_reg   <spec[+]>  \n2 glm             regression    linear_reg   <spec[+]>  \n3 glmnet          regression    linear_reg   <spec[+]>  \n4 cubist          regression    cubist_rules <spec[+]>  \n\ncreate_model_spec(\n .parsnip_eng = list(\"lm\",\"glm\",\"glmnet\",\"cubist\"),\n .parsnip_fns = list(\n      rep(\n        \"linear_reg\", 3),\n        \"cubist_rules\"\n     ),\n .return_tibble = FALSE\n )\n\n$.parsnip_engine\n$.parsnip_engine[[1]]\n[1] \"lm\"\n\n$.parsnip_engine[[2]]\n[1] \"glm\"\n\n$.parsnip_engine[[3]]\n[1] \"glmnet\"\n\n$.parsnip_engine[[4]]\n[1] \"cubist\"\n\n\n$.parsnip_mode\n$.parsnip_mode[[1]]\n[1] \"regression\"\n\n\n$.parsnip_fns\n$.parsnip_fns[[1]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[2]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[3]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[4]]\n[1] \"cubist_rules\"\n\n\n$.model_spec\n$.model_spec[[1]]\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n$.model_spec[[2]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\n$.model_spec[[3]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glmnet \n\n\n$.model_spec[[4]]\nCubist Model Specification (regression)\n\nComputational engine: cubist \n\n\nNow that we have seen what is to come in the future, let’s take a look at a pseudo solution that is easy to replicate now.\n\n# Load the purrr package\nlibrary(purrr)\nlibrary(parsnip)\n\n# Create a list of parsnip engines\nengines <- list(\n  engine1 = \"lm\",\n  engine2 = \"glm\",\n  engine3 = \"randomForest\"\n)\n\n# Create a list of parsnip call names\nparsnip_calls <- list(\n  call1 = \"linear_reg\",\n  call2 = \"linear_reg\",\n  call3 = \"rand_forest\"\n)\n\n# Use pmap() to create a list of parsnip model specs from the list of engines\n# and parsnip call names\n# Set the mode argument to \"regression\"\nmodel_specs <- pmap(list(engines, parsnip_calls), function(engine, call) {\n  match.fun(call)(engine = engine, mode = \"regression\")\n})\n\n# Print the list of model specs to the console\nmodel_specs\n\n$engine1\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n$engine2\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\n$engine3\nRandom Forest Model Specification (regression)\n\nComputational engine: randomForest \n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-08/index.html",
    "href": "posts/rtip-2022-12-08/index.html",
    "title": "Create a Faceted Historgram Plot with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nOne of the most important steps in data analysis is visualizing the distribution of your data. This can help you identify patterns, outliers, and trends in your data, and can also provide valuable insights into the relationships between different variables.\nOne way to visualize data distributions is by using histograms. A histogram is a graphical representation of the distribution of a numeric variable. It shows the number of observations (or the frequency) within each bin or range of values.\nIn this blog post, we will showcase the hai_histogram_facet_plot() function from the {healthyR.ai} package, which makes it easy to create faceted histograms of numeric and factor data in R.\n\n\nFunction\nLet’s take a look at the full function call.\n\nhai_histogram_facet_plot(\n  .data,\n  .bins = 10,\n  .scale_data = FALSE,\n  .ncol = 5,\n  .fct_reorder = FALSE,\n  .fct_rev = FALSE,\n  .fill = \"steelblue\",\n  .color = \"white\",\n  .scale = \"free\",\n  .interactive = FALSE\n)\n\nHere are the parameters and the arguments that get passed to them.\n\n.data - The data you want to pass to the function.\n.bins - The number of bins for the histograms.\n.scale_data - This is a boolean set to FALSE. TRUE will use hai_scale_zero_one_vec() to [0, 1] scale the data.\n.ncol - The number of columns for the facet_warp argument.\n.fct_reorder - Should the factor column be reordered? TRUE/FALSE, default of FALSE\n.fct_rev - Should the factor column be reversed? TRUE/FALSE, default of FALSE\n.fill - Default is steelblue\n.color - Default is ‘white’\n.scale - Default is ‘free’\n.interactive - Default is FALSE, TRUE will produce a {plotly} plot.\n\n\n\nExamples\nLet’s take a look at some example.\n\nlibrary(healthyR.ai, quietly = TRUE)\n\nhai_histogram_facet_plot(mtcars)\n\n\n\n\nNow lets scale the data and review.\n\nhai_histogram_facet_plot(mtcars, .scale_data = TRUE)\n\n\n\n\nLet’s take a look the iris data set now.\n\noutput <- hai_histogram_facet_plot(iris, .interactive = TRUE)\noutput$plot\n\n\n\n\n\nIn this blog post, we showcased the hai_histogram_facet_plot() function from the {healthyR.ai} package, which makes it easy to create histogram plots and faceted histograms in R. The hai_histogram_facet_plot() function allows you to quickly and easily visualize the distribution of your data, and can provide valuable insights into the relationships between different variables\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-13/index.html",
    "href": "posts/rtip-2022-12-13/index.html",
    "title": "Mixture Distributions with {TidyDensity}",
    "section": "",
    "text": "Introduction\nA mixture distribution is a type of probability distribution that is created by combining two or more simpler distributions. This allows us to model complex data that may have multiple underlying patterns. For example, a mixture distribution could be used to model a dataset that includes both continuous and discrete variables.\nTo create a mixture distribution, we first need to specify the individual distributions that will be combined, as well as the weights that determine how much each distribution contributes to the overall mixture. Once we have these components, we can use them to calculate the probability of any given value occurring in the mixture distribution.\nMixture distributions can be useful in a variety of applications, such as data analysis and machine learning. In data analysis, they can be used to model data that is not well-described by a single distribution, and in machine learning, they can be used to improve the performance of predictive models. Overall, mixture distributions are a powerful tool for understanding and working with complex data.\n\n\nFunction\nLet’s take a look a function in {TidyDensity} that allows us to do this. At this moment, weights are not a parameter to the function.\n\ntidy_mixture_density(...)\n\nNow let’s take a look at the arguments that get supplied to the ... parameter.\n\n... - The random data you want to pass. Example rnorm(50,0,1) or something like tidy_normal(.mean = 5, .sd = 1)\n\n\n\nExample\nLet’s take a look at an example.\n\nlibrary(TidyDensity)\n\noutput <- tidy_mixture_density(\n  rnorm(100, 0, 1), \n  tidy_normal(.mean = 5, .sd = 1)\n)\n\nAs you can see, you can enter a function that outputs a numeric vector or you can use a {TidyDensity} distribution function.\nLet’s take a look at the outputs.\n\noutput$data\n\n$dist_tbl\n# A tibble: 150 × 2\n       x       y\n   <int>   <dbl>\n 1     1  0.442 \n 2     2  1.80  \n 3     3  0.571 \n 4     4 -0.0365\n 5     5  0.854 \n 6     6 -0.634 \n 7     7 -0.189 \n 8     8 -0.415 \n 9     9  1.36  \n10    10  0.107 \n# … with 140 more rows\n\n$dens_tbl\n# A tibble: 150 × 2\n       x         y\n   <dbl>     <dbl>\n 1 -4.17 0.0000995\n 2 -4.08 0.000145 \n 3 -3.98 0.000207 \n 4 -3.89 0.000294 \n 5 -3.79 0.000413 \n 6 -3.70 0.000574 \n 7 -3.60 0.000788 \n 8 -3.51 0.00107  \n 9 -3.41 0.00145  \n10 -3.32 0.00193  \n# … with 140 more rows\n\n$input_data\n$input_data$`rnorm(100, 0, 1)`\n  [1]  0.44169781  1.80418306  0.57133927 -0.03649729  0.85387119 -0.63383074\n  [7] -0.18854658 -0.41451222  1.36023418  0.10726858  0.08526992 -0.64879496\n [13]  0.69255412 -0.75735669  0.19705920 -0.17721516 -0.63079170 -1.39983310\n [19]  1.01755199 -0.83631414  0.72912414 -0.14737137  1.27082258 -1.04753889\n [25] -0.16141490  0.22198899  2.83598596 -0.22484669 -0.58487594 -0.62746477\n [31] -0.81873031  1.74559087  1.36529721  1.45023471 -0.06258668  2.14467649\n [37]  0.10043517 -0.67990809  2.85050168 -1.45216256  0.01049808  0.22827703\n [43] -0.51146361  0.43143915 -0.59915348  1.61324991 -0.58580448 -0.46120961\n [49]  0.98191810 -0.31593955  0.86164296  1.18808250  1.09066101  0.39150090\n [55]  0.50730674  1.88640675  1.55522681 -0.65149477 -0.27561149 -0.31867192\n [61]  0.08555271 -1.00047014  1.12127311 -1.23597493  0.96384070  0.99097697\n [67] -0.25932523  0.25407058 -0.35294377 -0.72055148 -0.40429088 -0.08843004\n [73]  0.95498089 -0.68453125  1.67531797 -0.20665261  0.57318766 -0.12758793\n [79] -0.38044927  1.81833828  1.05959931  0.08519174  0.16865694 -0.15828443\n [85]  0.08736815  0.70222886  1.27180668  0.76483122 -0.43573173  0.02909088\n [91] -1.31286933 -0.09244617  0.22188836 -0.88909052  1.22243358  0.48397190\n [97]  0.82291445  0.46595188  0.68619052 -1.65739185\n\n$input_data$`tidy_normal(.mean = 5, .sd = 1)`\n# A tibble: 50 × 7\n   sim_number     x     y    dx       dy      p     q\n   <fct>      <int> <dbl> <dbl>    <dbl>  <dbl> <dbl>\n 1 1              1  6.20  1.24 0.000230 0.886   6.20\n 2 1              2  3.95  1.40 0.000639 0.146   3.95\n 3 1              3  4.59  1.55 0.00158  0.342   4.59\n 4 1              4  3.16  1.70 0.00349  0.0326  3.16\n 5 1              5  5.03  1.86 0.00689  0.513   5.03\n 6 1              6  4.89  2.01 0.0122   0.455   4.89\n 7 1              7  5.49  2.16 0.0195   0.687   5.49\n 8 1              8  6.78  2.32 0.0283   0.962   6.78\n 9 1              9  5.17  2.47 0.0376   0.566   5.17\n10 1             10  6.36  2.62 0.0464   0.913   6.36\n# … with 40 more rows\n\n\nAnd now the visuals that come with it.\n\noutput$plots\n\n$line_plot\n\n\n\n\n\n\n$dens_plot\n\n\n\n\n\nThe function also lists the input functions as well.\n\noutput$input_fns\n\n[[1]]\nrnorm(100, 0, 1)\n\n[[2]]\ntidy_normal(.mean = 5, .sd = 1)\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-14/index.html",
    "href": "posts/rtip-2022-12-14/index.html",
    "title": "Distribution Summaries with {TidyDensity}",
    "section": "",
    "text": "Introduction\n{TidyDensity} is an R package that provides tools for working with probability distributions in a tidy data format. One of the key functions in the package is tidy_distribution_summary_tbl(), which allows users to quickly and easily get summary information about a probability distribution.\nThe tidy_distribution_summary_tbl() function takes a vector of data as input and returns a table with basic statistics about the distribution of the data. This includes the mean, standard deviation, kurtosis, and skewness of the data, as well as other useful information.\nUsing tidy_distribution_summary_tbl(), users can easily get a high-level overview of their data, which can be useful for exploratory data analysis, data visualization, and other tasks. The function is designed to work seamlessly with the other tools in the {TidyDensity} package, making it easy to combine with other operations and build complex data analysis pipelines.\nOverall, TidyDensity and its tidy_distribution_summary_tbl() function are valuable tools for anyone working with probability distributions in R. Whether you are a seasoned data scientist or a beginner, TidyDensity can help you quickly and easily explore and understand your data.\n\n\nFunction\nLet’s take a look at the full function call.\n\ntidy_distribution_summary_tbl(.data, ...)\n\nHere are the arguments that go to the parameters.\n\n.data - The data that is going to be passed from a a tidy_ distribution function.\n... - This is the grouping variable that gets passed to dplyr::group_by() and dplyr::select().\n\n\n\nExample\nNow let’s go over a simple example.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\ntn <- tidy_normal(.num_sims = 5)\ntb <- tidy_beta(.num_sims = 5)\n\ntidy_distribution_summary_tbl(tn) |>\n  glimpse()\n\nRows: 1\nColumns: 12\n$ mean_val   <dbl> -0.044964\n$ median_val <dbl> -0.0266966\n$ std_val    <dbl> 1.020322\n$ min_val    <dbl> -2.834123\n$ max_val    <dbl> 3.336879\n$ skewness   <dbl> 0.03115634\n$ kurtosis   <dbl> 2.772527\n$ range      <dbl> 6.171002\n$ iqr        <dbl> 1.447849\n$ variance   <dbl> 1.041057\n$ ci_low     <dbl> -1.873091\n$ ci_high    <dbl> 1.868382\n\ntidy_distribution_summary_tbl(tn, sim_number) |>\n  glimpse()\n\nRows: 5\nColumns: 13\n$ sim_number <fct> 1, 2, 3, 4, 5\n$ mean_val   <dbl> -0.09684833, -0.13886169, 0.23257556, -0.32487778, 0.103192…\n$ median_val <dbl> -0.1358051, -0.2550682, 0.3069263, -0.1334922, 0.2898412\n$ std_val    <dbl> 1.1231699, 1.0954659, 0.8902380, 0.9270631, 0.9919932\n$ min_val    <dbl> -2.834123, -2.340575, -1.963215, -2.396105, -1.827744\n$ max_val    <dbl> 3.336879, 1.987640, 2.066451, 1.526231, 2.093211\n$ skewness   <dbl> 0.352771389, 0.132723834, -0.282840344, -0.191853538, 0.006…\n$ kurtosis   <dbl> 3.652828, 2.169309, 2.749967, 2.332081, 2.409223\n$ range      <dbl> 6.171002, 4.328215, 4.029666, 3.922336, 3.920956\n$ iqr        <dbl> 1.5256470, 1.6335396, 0.9368546, 1.3968485, 1.3469671\n$ variance   <dbl> 1.2615106, 1.2000455, 0.7925236, 0.8594460, 0.9840505\n$ ci_low     <dbl> -1.834548, -1.844197, -1.428713, -2.193065, -1.626225\n$ ci_high    <dbl> 1.860755, 1.858576, 1.644153, 1.090125, 1.976371\n\ndata_tbl <- tidy_combine_distributions(tn, tb)\n\ntidy_distribution_summary_tbl(data_tbl) |>\n  glimpse()\n\nRows: 1\nColumns: 12\n$ mean_val   <dbl> 0.2413251\n$ median_val <dbl> 0.3687409\n$ std_val    <dbl> 0.8030476\n$ min_val    <dbl> -2.834123\n$ max_val    <dbl> 3.336879\n$ skewness   <dbl> -0.7608556\n$ kurtosis   <dbl> 4.248452\n$ range      <dbl> 6.171002\n$ iqr        <dbl> 0.7835065\n$ variance   <dbl> 0.6448855\n$ ci_low     <dbl> -1.695096\n$ ci_high    <dbl> 1.585147\n\ntidy_distribution_summary_tbl(data_tbl, dist_type) |>\n  glimpse()\n\nRows: 2\nColumns: 13\n$ dist_type  <fct> \"Gaussian c(0, 1)\", \"Beta c(1, 1, 0)\"\n$ mean_val   <dbl> -0.0449640, 0.5276142\n$ median_val <dbl> -0.0266966, 0.5301650\n$ std_val    <dbl> 1.0203220, 0.2944871\n$ min_val    <dbl> -2.834123047, 0.001236575\n$ max_val    <dbl> 3.3368786, 0.9992146\n$ skewness   <dbl> 0.03115634, -0.08744219\n$ kurtosis   <dbl> 2.772527, 1.751248\n$ range      <dbl> 6.171002, 0.997978\n$ iqr        <dbl> 1.447849, 0.511105\n$ variance   <dbl> 1.04105699, 0.08672268\n$ ci_low     <dbl> -1.87309115, 0.04220623\n$ ci_high    <dbl> 1.8683817, 0.9771898"
  },
  {
    "objectID": "posts/rtip-2022-12-15/index.html",
    "href": "posts/rtip-2022-12-15/index.html",
    "title": "Simple Moving Average Plots with {healthyR.ts}",
    "section": "",
    "text": "The R package {healthyR.ts}, is an R package that allows users to easily plot and analyze their time series data. The package includes a variety of functions, but one of the standout features is the ts_sma_plot() function, which allows users to quickly visualize their time series data and any number of simple moving averages (SMAs) of their choosing.\nSMAs are a common tool used by analysts and investors to smooth out short-term fluctuations in data and identify longer-term trends. By overlaying SMAs of different time periods on top of the original time series data, the ts_sma_plot() function makes it easy to compare and contrast different time periods and identify potential trends and patterns in the data.\nWith {healthyR.ts} and the ts_sma_plot() function, users can quickly and easily gain valuable insights into their time series data and make more informed decisions based on the trends and patterns they uncover.\nOk enough of that, let’s see the function."
  },
  {
    "objectID": "posts/rtip-2022-12-15/index.html#data",
    "href": "posts/rtip-2022-12-15/index.html#data",
    "title": "Simple Moving Average Plots with {healthyR.ts}",
    "section": "Data",
    "text": "Data\n\nout$data\n\n# A tibble: 288 × 5\n   index     date_col   value sma_order sma_value\n   <yearmon> <date>     <dbl> <fct>         <dbl>\n 1 Jan 1949  1949-01-01   112 3               NA \n 2 Feb 1949  1949-02-01   118 3              121.\n 3 Mar 1949  1949-03-01   132 3              126.\n 4 Apr 1949  1949-04-01   129 3              127.\n 5 May 1949  1949-05-01   121 3              128.\n 6 Jun 1949  1949-06-01   135 3              135.\n 7 Jul 1949  1949-07-01   148 3              144.\n 8 Aug 1949  1949-08-01   148 3              144 \n 9 Sep 1949  1949-09-01   136 3              134.\n10 Oct 1949  1949-10-01   119 3              120.\n# … with 278 more rows"
  },
  {
    "objectID": "posts/rtip-2022-12-15/index.html#plots",
    "href": "posts/rtip-2022-12-15/index.html#plots",
    "title": "Simple Moving Average Plots with {healthyR.ts}",
    "section": "Plots",
    "text": "Plots\n\nout$plots$static_plot\n\nWarning: Removed 7 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\nout$plots$interactive_plot\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-19/index.html",
    "href": "posts/rtip-2022-12-19/index.html",
    "title": "Viewing Different Versions of the Same Statistical Distribution with {TidyDensity}",
    "section": "",
    "text": "Introduction\nIn statistics, it is often useful to view different versions of the same statistical distribution. For example, when working with the normal distribution, it may be helpful to see how the distribution changes as the mean and standard deviation are varied.\nOne way to do this is by using the R library {TidyDensity}, which has a function called tidy_multi_single_dist(). This function allows a user to easily generate multiple versions of the same statistical distribution which can be plotted on the same graph, with each version representing a different combination of mean and standard deviation.\nTo use this function, the user simply needs to specify the distribution they want to plot (e.g. “normal”), the range of values for the mean and standard deviation, and the number of versions they want to plot. The function will then generate a plot showing the different versions of the distribution, with each version represented by a different color.\nThere are several reasons why it might be a good idea to view different versions of the same statistical distribution. For one, it can help the user understand how the shape of the distribution changes as the mean and standard deviation are varied. This can be particularly useful for distributions that have a wide range of possible values for the mean and standard deviation, such as the normal distribution.\nIn addition, viewing different versions of the same distribution can also help the user identify patterns and trends in the data. For example, the user may notice that the distribution becomes more spread out as the standard deviation increases, or that the distribution shifts to the left or right as the mean changes.\nOverall, the TidyDensity function tidy_multi_single_dist() is a useful tool for anyone interested in visualizing different versions of the same statistical distribution. Whether you are a student learning about statistics for the first time, or an experienced data scientist looking to better understand your data, this function can help you gain a deeper understanding of the underlying distribution and identify patterns and trends in your data.\n\n\nFunction\nLet’s take a look at the full function call.\n\ntidy_multi_single_dist(\n  .tidy_dist = NULL, \n  .param_list = list()\n  )\n\nNow let’s look at the arguments that go to the parameters.\n\n.tidy_dist - The type of tidy_ distribution that you want to run. You can only choose one.\n.param_list - This must be a list() object of the parameters that you want to pass through to the {TidyDensity} ‘tidy_’ distribution function.\n\n\n\nExample\nLet’s run through an example.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\ntn <-tidy_multi_single_dist(\n  .tidy_dist = \"tidy_normal\",\n  .param_list = list(\n    .n = 200,\n    .mean = c(-1, 0, 1),\n    .sd = 1,\n    .num_sims = 3\n  )\n)\n\nNow that we have generated the data, let’s take a look and see if these different distributions have indeed been created.\n\ntidy_distribution_summary_tbl(tn, dist_name) |>\n  select(dist_name, mean_val, std_val)\n\n# A tibble: 3 × 3\n  dist_name         mean_val std_val\n  <fct>                <dbl>   <dbl>\n1 Gaussian c(-1, 1)  -1.03     0.988\n2 Gaussian c(0, 1)    0.0136   1.01 \n3 Gaussian c(1, 1)    0.990    1.02 \n\n\nLook’s good there, now let’s visualize.\n\ntn %>%\n  tidy_multi_dist_autoplot()\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-20/index.html",
    "href": "posts/rtip-2022-12-20/index.html",
    "title": "Random Walks with {TidyDensity}",
    "section": "",
    "text": "Introduction\nRandom walks are a type of stochastic process that can be used to model the movement of a particle or system over time. At each time step, the position of the particle is updated based on a random step drawn from a given probability distribution. This process can be represented as a sequence of independent and identically distributed (i.i.d.) random variables, and the resulting path traced by the particle is known as a random walk.\nRandom walks have a wide range of applications, including modeling the movement of stock prices, animal migration, and the spread of infectious diseases. They are also a fundamental concept in probability and statistics, and have been studied extensively in the literature.\nThe {TidyDensity} package provides a convenient way to generate and visualize random walks using the tidy_random_walk() function. This function takes a probability distribution as an argument, and generates a random walk by sampling from this distribution at each time step. For example, to generate a random walk with normally distributed steps, we can use the tidy_normal() function as follows:\n\nlibrary(TidyDensity)\n\n# Generate a random walk with normally distributed steps\nrw <- tidy_random_walk(tidy_normal())\n\nThe resulting object rw is a tibble with the typical tidy_ distribution columns and one augmented column called random_walk_value. The columns that are output are:\n\nsim_number The current simulation number from the tidy_ distribution\nx (You can think of this as time t)\ny The randomly generated value.\ndx & dy The density estimates of y at x\np & q The probability and quantile values of y\nrandom_walk_value The random walk value generated from tidy_random_walk() (You can think of this as the position of the particle at time t or the x)\n\nTo visualize the random walk, we can use the tidy_random_walk_autoplot() function, which creates a ggplot object showing the position of the particle at each time step. For example:\n\n# Visualize the random walk\ntidy_random_walk_autoplot(rw)\n\nThis will produce a plot showing the trajectory of the particle over time. You can customize the appearance of the plot by passing additional arguments to the tidy_random_walk_autoplot() function, such as the geom argument to specify the type of plot to use (e.g. geom = “line” for a line plot, or geom = “point” for a scatter plot).\nIn summary, the {TidyDensity} package provides a convenient and user-friendly interface for generating and visualizing random walks. With the tidy_random_walk() and tidy_random_walk_autoplot() functions, you can easily explore the behavior of random walks and their applications in a wide range of contexts.\nLet’s take a look at these functions.\n\n\nFunction\nFirstly we will look at the tidy_random_walk() function.\n\ntidy_random_walk(\n  .data,\n  .initial_value = 0,\n  .sample = FALSE,\n  .replace = FALSE,\n  .value_type = \"cum_prod\"\n)\n\nHere are the arguments that get provided to the parameters of this function.\n\n.data - The data that is being passed from a tidy_ distribution function.\n.initial_value - The default is 0, this can be set to whatever you want.\n.sample - This is a boolean value TRUE/FALSE. The default is FALSE. If set to TRUE then the y value from the tidy_ distribution function is sampled.\n.replace - This is a boolean value TRUE/FALSE. The default is FALSE. If set to TRUE AND .sample is set to TRUE then the replace parameter of the sample function will be set to TRUE.\n.value_type - This can take one of three different values for now. These are the following:\n\n“cum_prod” - This will take the cumprod of y\n“cum_sum” - This will take the cumsum of y\n\n\nNow let’s do the same with the tidy_random_walk_autoplot() function.\n\ntidy_random_walk_autoplot(\n  .data,\n  .line_size = 1,\n  .geom_rug = FALSE,\n  .geom_smooth = FALSE,\n  .interactive = FALSE\n)\n\nHere are the arguments that get provided to the parameters.\n\n.data - The data passed in from a tidy_distribution function like tidy_normal()\n.line_size - The size param ggplot\n.geom_rug - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return the use of ggplot2::geom_rug()\n.geom_smooth - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return the use of ggplot2::geom_smooth() The aes parameter of group is set to FALSE. This ensures a single smoothing band returned with SE also set to FALSE. Color is set to ‘black’ and linetype is ‘dashed’.\n.interactive - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return an interactive plotly plot.\n\n\n\nExamples\nLet’s go over some examples.\n\nlibrary(TidyDensity)\n\ndist_data <- tidy_normal(.sd = .1, .num_sims = 5)\n\ntidy_random_walk(.data = dist_data, .value_type = \"cum_sum\") %>%\n  tidy_random_walk_autoplot()\n\n\n\n\nAnd another.\n\ntidy_normal(.sd = .1, .num_sims = 20) %>%\n  tidy_random_walk(\n    .value_type = \"cum_sum\", \n    .sample = TRUE, \n    .replace = TRUE\n    ) %>%\n  tidy_random_walk_autoplot()\n\n\n\n\nNow let’s get an interactive one.\n\ntidy_normal(.sd = .1, .num_sims = 20) %>%\n  tidy_random_walk(\n    .value_type = \"cum_sum\", \n    .sample = TRUE, \n    .replace = TRUE\n    ) %>%\n  tidy_random_walk_autoplot(.interactive = TRUE)\n\n\n\n\n\nOne last example, let’s use a different distribution. Let’s use a cauchy distribution.\n\ntidy_cauchy(.num_sims = 9, .location = .5) %>%\n  tidy_random_walk(.value_type = \"cum_sum\", .sample = TRUE) %>%\n  tidy_random_walk_autoplot()\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-21/index.html",
    "href": "posts/rtip-2022-12-21/index.html",
    "title": "Distribution Statistics with {TidyDensity}",
    "section": "",
    "text": "Introduction\nIf you’re working with statistical distributions in R, you may be interested in the {TidyDensity} package. This package provides a set of functions for creating, manipulating, and visualizing probability distributions in a tidy format. One of these functions is tidy_chisquare(), which allows you to create a chi-square distribution with a specified number of degrees of freedom and a non-centrality parameter.\nOnce you’ve created a chi-square distribution using tidy_chisquare(), you may want to get some summary statistics about the distribution. This is where the util_chisquare_stats_tbl() function comes in handy. This function takes a chi-square distribution (created with tidy_chisquare()) as input and returns a tibble with several statistics about the distribution.\nSome of the statistics included in the table are:\n\nMean: The mean of the chi-square distribution, also known as the expected value.\nVariance: The variance of the chi-square distribution, which is a measure of how spread out the data is.\nSkewness: The skewness of the chi-square distribution, which is a measure of the symmetry of the data.\nKurtosis: The kurtosis of the chi-square distribution, which is a measure of the peakedness of the data.\n\nTo use the util_chisquare_stats_tbl() function, you’ll need to install and load the {TidyDensity} package first. Then, you can create a chi-square distribution using tidy_chisquare() and pass it to util_chisquare_stats_tbl() like this:\n\n# install and load TidyDensity\ninstall.packages(\"TidyDensity\")\nlibrary(TidyDensity)\nlibrary(dplyr)\n\n# create a chi-square distribution with 5 degrees of freedom\ndistribution <- tidy_chisquare(.df = 5)\n\n# get statistics about the distribution\nutil_chisquare_stats_tbl(distribution) |>\n  glimpse()\n\nThe output will be a table with the mean, variance, skewness, and kurtosis of the chi-square distribution. These statistics can be useful for understanding the characteristics of the distribution and making statistical inferences.\nOverall, the {TidyDensity} package is a useful tool for working with statistical distributions in R. The util_chisquare_stats_tbl() function is just one of many functions available in the package that can help you analyze and understand your data. Give it a try and see how it can help with your statistical analysis!\n\n\nFunction\nLet’s take a look at the full function call.\n\nutil_chisquare_stats_tbl(.data)\n\nLet’s take a look at the arguments that get supplied to the function parameters.\n\n.data - The data being passed from a tidy_ distribution function.\n\n\n\nExample\nNow for a full example with output.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\ntidy_chisquare() %>%\n  util_chisquare_stats_tbl() %>%\n  glimpse()\n\nRows: 1\nColumns: 17\n$ tidy_function     <chr> \"tidy_chisquare\"\n$ function_call     <chr> \"Chisquare c(1, 1)\"\n$ distribution      <chr> \"Chisquare\"\n$ distribution_type <chr> \"continuous\"\n$ points            <dbl> 50\n$ simulations       <dbl> 1\n$ mean              <dbl> 1\n$ median            <dbl> 0.3333333\n$ mode              <chr> \"undefined\"\n$ std_dv            <dbl> 1.414214\n$ coeff_var         <dbl> 1.414214\n$ skewness          <dbl> 2.828427\n$ kurtosis          <dbl> 15\n$ computed_std_skew <dbl> 1.132669\n$ computed_std_kurt <dbl> 3.894553\n$ ci_lo             <dbl> 0.002189912\n$ ci_hi             <dbl> 6.521727\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-22/index.html",
    "href": "posts/rtip-2022-12-22/index.html",
    "title": "Listing Functions and Parameters",
    "section": "",
    "text": "Introduction\nI got a little bored one day and decided I wanted to list out all of the functions inside of a package along with their parameters in a tibble. Not sure if this serves any particular purpose or not, I was just bored.\nThis does not work for packages that have data as an export like {healthyR} or {healthyR.data} but it will work for packages like {TidyDensity}.\nLet’s run through it\n\n\nExamples\nHere we go.\n\nlibrary(TidyDensity)\n\ndplyr::tibble(fns = ls(paste0(\"package:TidyDensity\"))) |>\n  dplyr::group_by(fns) |>\n  dplyr::mutate(params = purrr::map(fns, formalArgs) |>\n           toString()) |>\n  dplyr::mutate(func_with_params = stringr::str_remove(params, \"c\")) |>\n  dplyr::mutate(func_with_params = ifelse(\n    stringr::str_detect(\n      func_with_params, \"\\\\(\"), \n      paste0(fns, func_with_params), \n      paste0(fns, \"(\", func_with_params, \")\")\n    )) |>\n  dplyr::ungroup() |>\n  knitr::kable()\n\n\n\n\n\n\n\n\n\nfns\nparams\nfunc_with_params\n\n\n\n\n%>%\nc(“lhs”, “rhs”)\n%>%(“lhs”, “rhs”)\n\n\n:=\nc(“x”, “y”)\n:=(“x”, “y”)\n\n\nas_label\nx\nas_label(x)\n\n\nas_name\nx\nas_name(x)\n\n\nbootstrap_density_augment\n.data\nbootstrap_density_augment(.data)\n\n\nbootstrap_p_augment\nc(“.data”, “.value”, “.names”)\nbootstrap_p_augment(“.data”, “.value”, “.names”)\n\n\nbootstrap_p_vec\n.x\nbootstrap_p_vec(.x)\n\n\nbootstrap_q_augment\nc(“.data”, “.value”, “.names”)\nbootstrap_q_augment(“.data”, “.value”, “.names”)\n\n\nbootstrap_q_vec\n.x\nbootstrap_q_vec(.x)\n\n\nbootstrap_stat_plot\nc(“.data”, “.value”, “.stat”, “.show_groups”, “.show_ci_labels”, “.interactive”)\nbootstrap_stat_plot(“.data”, “.value”, “.stat”, “.show_groups”, “.show_ci_labels”, “.interactive”)\n\n\nbootstrap_unnest_tbl\n.data\nbootstrap_unnest_tbl(.data)\n\n\ncgmean\n.x\ncgmean(.x)\n\n\nchmean\n.x\nchmean(.x)\n\n\nci_hi\nc(“.x”, “.na_rm”)\nci_hi(“.x”, “.na_rm”)\n\n\nci_lo\nc(“.x”, “.na_rm”)\nci_lo(“.x”, “.na_rm”)\n\n\nckurtosis\n.x\nckurtosis(.x)\n\n\ncmean\n.x\ncmean(.x)\n\n\ncmedian\n.x\ncmedian(.x)\n\n\ncolor_blind\nNULL\ncolor_blind(NULL)\n\n\ncsd\n.x\ncsd(.x)\n\n\ncskewness\n.x\ncskewness(.x)\n\n\ncvar\n.x\ncvar(.x)\n\n\ndist_type_extractor\n.x\ndist_type_extractor(.x)\n\n\nenquo\narg\nenquo(arg)\n\n\nenquos\nc(“…”, “.named”, “.ignore_empty”, “.unquote_names”, “.homonyms”, “.check_assign”)\nenquos(“…”, “.named”, “.ignore_empty”, “.unquote_names”, “.homonyms”, “.check_assign”)\n\n\ntd_scale_color_colorblind\nc(“…”, “theme”)\ntd_scale_color_colorblind(“…”, “theme”)\n\n\ntd_scale_fill_colorblind\nc(“…”, “theme”)\ntd_scale_fill_colorblind(“…”, “theme”)\n\n\ntidy_autoplot\nc(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\ntidy_autoplot(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\n\n\ntidy_bernoulli\nc(“.n”, “.prob”, “.num_sims”)\ntidy_bernoulli(“.n”, “.prob”, “.num_sims”)\n\n\ntidy_beta\nc(“.n”, “.shape1”, “.shape2”, “.ncp”, “.num_sims”)\ntidy_beta(“.n”, “.shape1”, “.shape2”, “.ncp”, “.num_sims”)\n\n\ntidy_binomial\nc(“.n”, “.size”, “.prob”, “.num_sims”)\ntidy_binomial(“.n”, “.size”, “.prob”, “.num_sims”)\n\n\ntidy_bootstrap\nc(“.x”, “.num_sims”, “.proportion”, “.distribution_type”)\ntidy_bootstrap(“.x”, “.num_sims”, “.proportion”, “.distribution_type”)\n\n\ntidy_burr\nc(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\ntidy_burr(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_cauchy\nc(“.n”, “.location”, “.scale”, “.num_sims”)\ntidy_cauchy(“.n”, “.location”, “.scale”, “.num_sims”)\n\n\ntidy_chisquare\nc(“.n”, “.df”, “.ncp”, “.num_sims”)\ntidy_chisquare(“.n”, “.df”, “.ncp”, “.num_sims”)\n\n\ntidy_combine_distributions\n…\ntidy_combine_distributions(…)\n\n\ntidy_combined_autoplot\nc(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\ntidy_combined_autoplot(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\n\n\ntidy_distribution_comparison\nc(“.x”, “.distribution_type”)\ntidy_distribution_comparison(“.x”, “.distribution_type”)\n\n\ntidy_distribution_summary_tbl\nc(“.data”, “…”)\ntidy_distribution_summary_tbl(“.data”, “…”)\n\n\ntidy_empirical\nc(“.x”, “.num_sims”, “.distribution_type”)\ntidy_empirical(“.x”, “.num_sims”, “.distribution_type”)\n\n\ntidy_exponential\nc(“.n”, “.rate”, “.num_sims”)\ntidy_exponential(“.n”, “.rate”, “.num_sims”)\n\n\ntidy_f\nc(“.n”, “.df1”, “.df2”, “.ncp”, “.num_sims”)\ntidy_f(“.n”, “.df1”, “.df2”, “.ncp”, “.num_sims”)\n\n\ntidy_four_autoplot\nc(“.data”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\ntidy_four_autoplot(“.data”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\n\n\ntidy_gamma\nc(“.n”, “.shape”, “.scale”, “.num_sims”)\ntidy_gamma(“.n”, “.shape”, “.scale”, “.num_sims”)\n\n\ntidy_generalized_beta\nc(“.n”, “.shape1”, “.shape2”, “.shape3”, “.rate”, “.scale”, “.num_sims”)\ntidy_generalized_beta(“.n”, “.shape1”, “.shape2”, “.shape3”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_generalized_pareto\nc(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\ntidy_generalized_pareto(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_geometric\nc(“.n”, “.prob”, “.num_sims”)\ntidy_geometric(“.n”, “.prob”, “.num_sims”)\n\n\ntidy_hypergeometric\nc(“.n”, “.m”, “.nn”, “.k”, “.num_sims”)\ntidy_hypergeometric(“.n”, “.m”, “.nn”, “.k”, “.num_sims”)\n\n\ntidy_inverse_burr\nc(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\ntidy_inverse_burr(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_inverse_exponential\nc(“.n”, “.rate”, “.scale”, “.num_sims”)\ntidy_inverse_exponential(“.n”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_inverse_gamma\nc(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\ntidy_inverse_gamma(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_inverse_normal\nc(“.n”, “.mean”, “.shape”, “.dispersion”, “.num_sims”)\ntidy_inverse_normal(“.n”, “.mean”, “.shape”, “.dispersion”, “.num_sims”)\n\n\ntidy_inverse_pareto\nc(“.n”, “.shape”, “.scale”, “.num_sims”)\ntidy_inverse_pareto(“.n”, “.shape”, “.scale”, “.num_sims”)\n\n\ntidy_inverse_weibull\nc(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\ntidy_inverse_weibull(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_kurtosis_vec\n.x\ntidy_kurtosis_vec(.x)\n\n\ntidy_logistic\nc(“.n”, “.location”, “.scale”, “.num_sims”)\ntidy_logistic(“.n”, “.location”, “.scale”, “.num_sims”)\n\n\ntidy_lognormal\nc(“.n”, “.meanlog”, “.sdlog”, “.num_sims”)\ntidy_lognormal(“.n”, “.meanlog”, “.sdlog”, “.num_sims”)\n\n\ntidy_mixture_density\n…\ntidy_mixture_density(…)\n\n\ntidy_multi_dist_autoplot\nc(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\ntidy_multi_dist_autoplot(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\n\n\ntidy_multi_single_dist\nc(“.tidy_dist”, “.param_list”)\ntidy_multi_single_dist(“.tidy_dist”, “.param_list”)\n\n\ntidy_negative_binomial\nc(“.n”, “.size”, “.prob”, “.num_sims”)\ntidy_negative_binomial(“.n”, “.size”, “.prob”, “.num_sims”)\n\n\ntidy_normal\nc(“.n”, “.mean”, “.sd”, “.num_sims”)\ntidy_normal(“.n”, “.mean”, “.sd”, “.num_sims”)\n\n\ntidy_paralogistic\nc(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\ntidy_paralogistic(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_pareto\nc(“.n”, “.shape”, “.scale”, “.num_sims”)\ntidy_pareto(“.n”, “.shape”, “.scale”, “.num_sims”)\n\n\ntidy_pareto1\nc(“.n”, “.shape”, “.min”, “.num_sims”)\ntidy_pareto1(“.n”, “.shape”, “.min”, “.num_sims”)\n\n\ntidy_poisson\nc(“.n”, “.lambda”, “.num_sims”)\ntidy_poisson(“.n”, “.lambda”, “.num_sims”)\n\n\ntidy_random_walk\nc(“.data”, “.initial_value”, “.sample”, “.replace”, “.value_type”)\ntidy_random_walk(“.data”, “.initial_value”, “.sample”, “.replace”, “.value_type”)\n\n\ntidy_random_walk_autoplot\nc(“.data”, “.line_size”, “.geom_rug”, “.geom_smooth”, “.interactive”)\ntidy_random_walk_autoplot(“.data”, “.line_size”, “.geom_rug”, “.geom_smooth”, “.interactive”)\n\n\ntidy_range_statistic\n.x\ntidy_range_statistic(.x)\n\n\ntidy_scale_zero_one_vec\n.x\ntidy_scale_zero_one_vec(.x)\n\n\ntidy_skewness_vec\n.x\ntidy_skewness_vec(.x)\n\n\ntidy_stat_tbl\nc(“.data”, “.x”, “.fns”, “.return_type”, “.use_data_table”, “…”)\ntidy_stat_tbl(“.data”, “.x”, “.fns”, “.return_type”, “.use_data_table”, “…”)\n\n\ntidy_t\nc(“.n”, “.df”, “.ncp”, “.num_sims”)\ntidy_t(“.n”, “.df”, “.ncp”, “.num_sims”)\n\n\ntidy_uniform\nc(“.n”, “.min”, “.max”, “.num_sims”)\ntidy_uniform(“.n”, “.min”, “.max”, “.num_sims”)\n\n\ntidy_weibull\nc(“.n”, “.shape”, “.scale”, “.num_sims”)\ntidy_weibull(“.n”, “.shape”, “.scale”, “.num_sims”)\n\n\ntidy_zero_truncated_binomial\nc(“.n”, “.size”, “.prob”, “.num_sims”)\ntidy_zero_truncated_binomial(“.n”, “.size”, “.prob”, “.num_sims”)\n\n\ntidy_zero_truncated_geometric\nc(“.n”, “.prob”, “.num_sims”)\ntidy_zero_truncated_geometric(“.n”, “.prob”, “.num_sims”)\n\n\ntidy_zero_truncated_negative_binomial\nc(“.n”, “.size”, “.prob”, “.num_sims”)\ntidy_zero_truncated_negative_binomial(“.n”, “.size”, “.prob”, “.num_sims”)\n\n\ntidy_zero_truncated_poisson\nc(“.n”, “.lambda”, “.num_sims”)\ntidy_zero_truncated_poisson(“.n”, “.lambda”, “.num_sims”)\n\n\nutil_bernoulli_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_bernoulli_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_bernoulli_stats_tbl\n.data\nutil_bernoulli_stats_tbl(.data)\n\n\nutil_beta_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_beta_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_beta_stats_tbl\n.data\nutil_beta_stats_tbl(.data)\n\n\nutil_binomial_param_estimate\nc(“.x”, “.size”, “.auto_gen_empirical”)\nutil_binomial_param_estimate(“.x”, “.size”, “.auto_gen_empirical”)\n\n\nutil_binomial_stats_tbl\n.data\nutil_binomial_stats_tbl(.data)\n\n\nutil_cauchy_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_cauchy_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_cauchy_stats_tbl\n.data\nutil_cauchy_stats_tbl(.data)\n\n\nutil_chisquare_stats_tbl\n.data\nutil_chisquare_stats_tbl(.data)\n\n\nutil_exponential_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_exponential_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_exponential_stats_tbl\n.data\nutil_exponential_stats_tbl(.data)\n\n\nutil_f_stats_tbl\n.data\nutil_f_stats_tbl(.data)\n\n\nutil_gamma_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_gamma_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_gamma_stats_tbl\n.data\nutil_gamma_stats_tbl(.data)\n\n\nutil_geometric_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_geometric_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_geometric_stats_tbl\n.data\nutil_geometric_stats_tbl(.data)\n\n\nutil_hypergeometric_param_estimate\nc(“.x”, “.m”, “.total”, “.k”, “.auto_gen_empirical”)\nutil_hypergeometric_param_estimate(“.x”, “.m”, “.total”, “.k”, “.auto_gen_empirical”)\n\n\nutil_hypergeometric_stats_tbl\n.data\nutil_hypergeometric_stats_tbl(.data)\n\n\nutil_logistic_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_logistic_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_logistic_stats_tbl\n.data\nutil_logistic_stats_tbl(.data)\n\n\nutil_lognormal_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_lognormal_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_lognormal_stats_tbl\n.data\nutil_lognormal_stats_tbl(.data)\n\n\nutil_negative_binomial_param_estimate\nc(“.x”, “.size”, “.auto_gen_empirical”)\nutil_negative_binomial_param_estimate(“.x”, “.size”, “.auto_gen_empirical”)\n\n\nutil_negative_binomial_stats_tbl\n.data\nutil_negative_binomial_stats_tbl(.data)\n\n\nutil_normal_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_normal_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_normal_stats_tbl\n.data\nutil_normal_stats_tbl(.data)\n\n\nutil_pareto_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_pareto_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_pareto_stats_tbl\n.data\nutil_pareto_stats_tbl(.data)\n\n\nutil_poisson_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_poisson_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_poisson_stats_tbl\n.data\nutil_poisson_stats_tbl(.data)\n\n\nutil_t_stats_tbl\n.data\nutil_t_stats_tbl(.data)\n\n\nutil_uniform_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_uniform_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_uniform_stats_tbl\n.data\nutil_uniform_stats_tbl(.data)\n\n\nutil_weibull_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_weibull_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_weibull_stats_tbl\n.data\nutil_weibull_stats_tbl(.data)\n\n\n\n\n\nAnother example.\n\nlibrary(healthyverse)\n\nlibrary(TidyDensity)\n\ndplyr::tibble(fns = ls(paste0(\"package:healthyverse\"))) |>\n  dplyr::group_by(fns) |>\n  dplyr::mutate(params = purrr::map(fns, formalArgs) |>\n           toString()) |>\n  dplyr::mutate(func_with_params = stringr::str_remove(params, \"c\")) |>\n  dplyr::mutate(func_with_params = ifelse(\n    stringr::str_detect(\n      func_with_params, \"\\\\(\"), \n      paste0(fns, func_with_params), \n      paste0(fns, \"(\", func_with_params, \")\")\n    )) |>\n  dplyr::ungroup() |>\n  knitr::kable()\n\n\n\n\n\n\n\n\n\nfns\nparams\nfunc_with_params\n\n\n\n\n%>%\nc(“lhs”, “rhs”)\n%>%(“lhs”, “rhs”)\n\n\n:=\nc(“x”, “y”)\n:=(“x”, “y”)\n\n\nas_label\nx\nas_label(x)\n\n\nas_name\nx\nas_name(x)\n\n\nenquo\narg\nenquo(arg)\n\n\nenquos\nc(“…”, “.named”, “.ignore_empty”, “.unquote_names”, “.homonyms”, “.check_assign”)\nenquos(“…”, “.named”, “.ignore_empty”, “.unquote_names”, “.homonyms”, “.check_assign”)\n\n\nexpr\nexpr\nexpr(expr)\n\n\nhealthyverse_conflicts\nNULL\nhealthyverse_conflicts(NULL)\n\n\nhealthyverse_deps\nc(“recursive”, “repos”)\nhealthyverse_deps(“recursive”, “repos”)\n\n\nhealthyverse_packages\ninclude_self\nhealthyverse_packages(inlude_self)\n\n\nhealthyverse_sitrep\nNULL\nhealthyverse_sitrep(NULL)\n\n\nhealthyverse_update\nc(“recursive”, “repos”)\nhealthyverse_update(“recursive”, “repos”)\n\n\nsym\nx\nsym(x)\n\n\nsyms\nx\nsyms(x)\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-29/index.html",
    "href": "posts/rtip-2022-12-29/index.html",
    "title": "Gartner Magic Chart and its usefulness in healthcare analytics with {healthyR}",
    "section": "",
    "text": "Introduction\nThe Gartner Magic Chart is a powerful tool for analyzing healthcare data and identifying trends and patterns that can inform decision making. It was developed by Dr. James Gartner in the early 2000s as a way to visualize the relationship between two key metrics, for example: Excess Length of Stay (ELOS) and Excess Readmit Rate.\nIn healthcare, length of stay (LOS) refers to the amount of time a patient spends in the hospital. Excess LOS is the difference between the actual LOS of a patient and the expected LOS for that patient, based on their diagnosis and other factors. Excess readmit rate is the percentage of patients who are readmitted to the hospital within a certain time period after being discharged, above and beyond what is expected based on their diagnosis and other factors.\nThe Gartner Magic Chart can plot excess LOS on the x-axis and excess readmit rate on the y-axis. The resulting chart is divided into four quadrants, with the top right quadrant representing high excess LOS and high excess readmit rate, the bottom left quadrant representing low excess LOS and low excess readmit rate, and the other two quadrants representing intermediate values of these metrics.\nOne of the key benefits of the Gartner Magic Chart is that it allows healthcare professionals to quickly and easily identify areas of concern and opportunities for improvement. For example, if a hospital has a high excess LOS and a high excess readmit rate, it may be an indication that the hospital is not effectively managing patient care and is instead relying on costly and unnecessary readmissions to address problems that could have been avoided in the first place.\nThe Gartner Magic Chart can also be used to identify trends over time, allowing healthcare professionals to track progress and see the impact of changes they have made to patient care processes.\nIf you are interested in creating a Gartner Magic Chart for your own healthcare data, the R package {healthyR} has a convenient function called gartner_magic_chart_plt() that allows you to easily create this chart from data supplied by the end user. Simply input your excess LOS and excess readmit rate data, and the function will generate the chart for you.\nIn summary, the Gartner Magic Chart is a valuable tool for analyzing healthcare data and identifying trends and patterns that can inform decision making. By using the gartner_magic_chart_plt() function from the {healthyR} package, you can easily create this chart for your own data and start using it to improve patient care and outcomes.\n\n\nFunction\nLet’s take a look at the full function call for gartner_magic_chart_plt().\n\ngartner_magic_chart_plt(\n  .data,\n  .x_col,\n  .y_col,\n  .point_size_col = NULL,\n  .y_lab,\n  .x_lab,\n  .plt_title,\n  .tl_lbl,\n  .tr_lbl,\n  .br_lbl,\n  .bl_lbl\n)\n\nNow let’s take a look at the arguments to the parameters.\n\n.data - The data set you want to plot\n.x_col - The x-axis for the plot\n.y_col - The y-axis for the plot\n.point_size_col - The default is NULL, if you want to size the dots by a column in the data.frame/tibble then enter the column name here.\n.y_lab - The y-axis label\n.x_lab - The x-axis label\n.plt_title - The title of the plot\n.tl_lbl - The top left label\n.tr_lbl - The top right label\n.br_lbl - The bottom right label\n.bl_lbl - The bottom left label\n\n\n\nExample\nLet’s see the function in action.\n\nlibrary(dplyr)\nlibrary(healthyR)\n\ndata_tbl <- tibble(\n    x = rnorm(100, 0, 1),\n    y = rnorm(100, 0, 1),\n    z = abs(x) + abs(y)\n )\n\ngartner_magic_chart_plt(\n  .data = data_tbl,\n  .x_col = x,\n  .y_col = y,\n  .point_size = z,\n  .x_lab = \"los\",\n  .y_lab = \"ra\",\n  .plt_title = \"Test Title\",\n  .tr_lbl = \"High RA-LOS\",\n  .tl_lbl = \"High RA\",\n  .bl_lbl = \"Leader\",\n  .br_lbl = \"High LOS\"\n)\n\n\n\n\nExample two.\n\ngartner_magic_chart_plt(\n  .data = data_tbl,\n  .x_col = x,\n  .y_col = y,\n  .point_size = NULL,\n  .x_lab = \"los\",\n  .y_lab = \"ra\",\n  .plt_title = \"Test Title\",\n  .tr_lbl = \"High RA-LOS\",\n  .tl_lbl = \"High RA\",\n  .bl_lbl = \"Leader\",\n  .br_lbl = \"High LOS\"\n)"
  },
  {
    "objectID": "posts/rtip-2023-01-03/index.html",
    "href": "posts/rtip-2023-01-03/index.html",
    "title": "Calendar Heatmap with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nCalendar heatmaps are a useful visualization tool for understanding patterns and trends in time series data. They are particularly useful for displaying daily data, as they allow for the visualization of multiple weeks or months at a time.\nThe ts_calendar_heatmap_plot() function from the R library {healthyR.ts} is a powerful tool for creating calendar heatmaps. This function takes in a time series object and creates a heatmap plot with daily data plotted on the calendar. The intensity of the color on each day corresponds to the value of the data for that day.\nOne application of calendar heatmaps is in understanding daily patterns in data such as website traffic or sales. For example, a business owner could use a calendar heatmap to identify trends in their daily sales data and make informed decisions about their operations. Similarly, a website owner could use a calendar heatmap to understand the daily traffic patterns on their site and optimize their content strategy accordingly.\nCalendar heatmaps are also useful for identifying anomalies or unusual patterns in time series data. For example, a calendar heatmap could be used to identify unexpected spikes or dips in daily sales data, or to identify unusual patterns in website traffic.\nIn addition to their practical applications, calendar heatmaps are also aesthetically pleasing and can be a fun way to visualize data. The ts_calendar_heatmap_plot() function allows for customization of the color palette and other visual options, making it easy to create visually appealing heatmaps.\nOverall, calendar heatmaps are a useful tool for understanding patterns and trends in daily time series data. The ts_calendar_heatmap_plot() function from the R library healthyR.ts is a powerful tool for creating calendar heatmaps and can be easily customized to suit the needs of the user.\n\n\nFunction\nLet’s take a look at the function.\n\nts_calendar_heatmap_plot(\n  .data,\n  .date_col,\n  .value_col,\n  .low = \"red\",\n  .high = \"green\",\n  .plt_title = \"\",\n  .interactive = TRUE\n)\n\nNow let’s see the arguments to the parameters.\n\n.data - The time-series data with a date column and value column.\n.date_col - The column that has the datetime values\n.value_col - The column that has the values\n.low - The color for the low value, must be quoted like “red”. The default is “red”\n.high - The color for the high value, must be quoted like “green”. The default is “green”\n.plt_title - The title of the plot\n.interactive - Default is TRUE to get an interactive plot using plotly::ggplotly(). It can be set to FALSE to get a ggplot plot.\n\n\n\nExample\nNow for an example.\n\nlibrary(healthyR.ts)\n\ndata_tbl <- data.frame(\n  date_col = seq.Date(\n    from = as.Date(\"2020-01-01\"),\n    to   = as.Date(\"2022-06-01\"),\n    length.out = 365*2 + 180\n    ),\n  value = rnorm(365*2+180, mean = 100)\n)\n\nts_calendar_heatmap_plot(\n  .data          = data_tbl\n  , .date_col    = date_col\n  , .value_col   = value\n  , .interactive = TRUE\n)\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-05/index.html",
    "href": "posts/rtip-2023-01-05/index.html",
    "title": "More Randomwalks with {TidyDensity}",
    "section": "",
    "text": "Introduction\nRandom walks are a mathematical concept that have found various applications in fields such as economics, biology, and computer science. At a high level, a random walk refers to a process in which a set of objects move in a random direction at each time step. The path that the objects take over time forms a random walk.\nOne of the main uses of random walks is in modeling the behavior of stock prices. In the stock market, prices can be thought of as performing a random walk because they are influenced by a variety of unpredictable factors such as market trends, news events, and investor sentiment. By modeling stock prices as a random walk, it is possible to make predictions about future price movements and to understand the underlying factors that drive these movements.\nAnother application of random walks is in studying the movement patterns of animals. For example, biologists have used random walk models to understand the foraging behavior of ants and the migration patterns of animals such as birds and whales.\nOne interesting aspect of random walks is that they can be generated with different statistical distributions. For example, a random walk could be generated with a standard normal distribution (mean = 0, standard deviation = 1) or with a distribution that has a different mean and standard deviation. By looking at random walks with different distributional parameters, it is possible to understand how the underlying distribution affects the overall shape and pattern of the random walk.\nTo generate random walks with different distributional parameters, you can use the R package {TidyDensity}. This package provides functions for generating random walks and visualizing them using density plots. With {TidyDensity}, you can easily compare random walks with different mean and standard deviation values to see how these parameters affect the shape of the random walk.\nIn summary, random walks are a useful tool for modeling the behavior of various systems over time. They are particularly useful for understanding the movement patterns of stock prices and animals, and can be generated with different statistical distributions using the R package {TidyDensity}.\n\n\nFunctions\nThere are a couple of functions that we are going to use, below you will find them with their full function call and parameter arguments.\ntidy_multi_single_dist()\n\ntidy_multi_single_dist(.tidy_dist = NULL, .param_list = list())\n\n\n.tidy_dist - The type of tidy_ distribution that you want to run. You can only choose one.\n.param_list - This must be a list() object of the parameters that you want to pass through to the TidyDensity tidy_ distribution function.\n\ntidy_random_walk()\n\ntidy_random_walk(\n  .data,\n  .initial_value = 0,\n  .sample = FALSE,\n  .replace = FALSE,\n  .value_type = \"cum_prod\"\n)\n\n\n.data - The data that is being passed from a tidy_ distribution function.\n.initial_value - The default is 0, this can be set to whatever you want.\n.sample - This is a boolean value TRUE/FALSE. The default is FALSE. If set to TRUE then the y value from the tidy_ distribution function is sampled.\n.replace - This is a boolean value TRUE/FALSE. The default is FALSE. If set to TRUE AND .sample is set to TRUE then the replace parameter of the sample function will be set to TRUE.\n.value_type - This can take one of three different values for now. These are the following:\n\n“cum_prod” - This will take the cumprod of y\n“cum_sum” - This will take the cumsum of y\n\n\ntidy_random_walk_autoplot()\n\ntidy_random_walk_autoplot(\n  .data,\n  .line_size = 1,\n  .geom_rug = FALSE,\n  .geom_smooth = FALSE,\n  .interactive = FALSE\n)\n\n\n.data - The data passed in from a tidy_distribution function like tidy_normal()\n.line_size - The size param ggplot\n.geom_rug - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return the use of ggplot2::geom_rug()\n.geom_smooth - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return the use of ggplot2::geom_smooth() The aes parameter of group is set to FALSE. This ensures a single smoothing band returned with SE also set to FALSE. Color is set to ‘black’ and linetype is ‘dashed’.\n.interactive - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return an interactive plotly plot.\n\n\n\nExample\n\nlibrary(TidyDensity)\nlibrary(dplyr)\nlibrary(ggplot2)\n\ntidy_multi_single_dist(\n  .tidy_dist = \"tidy_normal\",\n  .param_list = list(\n    .n = 250,\n    .mean = 0,\n    .sd = c(.025, .05, .1, .15),\n    .num_sims = 25\n  )\n) %>%\n  tidy_random_walk(.initial_value = 1000, .value_type = \"cum_prod\") %>%\n  tidy_random_walk_autoplot() +\n  facet_wrap(~ dist_name, scales = \"free\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-09/index.html",
    "href": "posts/rtip-2023-01-09/index.html",
    "title": "New Release of {healthyR.ts}",
    "section": "",
    "text": "Introduction\nHello R users!\nI am excited to announce a new update to the {healthyR.ts} package: the ts_brownian_motion() function.\nThis function allows you to easily simulate brownian motion, also known as a Wiener process, using just a few parameters. You can specify the length of the simulation using the ‘.time’ parameter, the number of simulations to run using the ‘.num_sims’ parameter, the time step size (standard deviation) using the ‘.delta_time’ parameter, and the initial value (which is set to 0 by default) using the ‘.initial_value’ parameter.\nBut what is brownian motion, and why might you want to simulate it? Brownian motion is a random process that describes the movement of particles suspended in a fluid. It is named after the botanist Robert Brown, who observed the random movement of pollen grains suspended in water under a microscope in the 19th century.\nIn finance, brownian motion is often used to model the movement of stock prices over time. By simulating brownian motion, you can get a sense of how prices might fluctuate in the future, and use this information to inform your investment decisions.\nI hope that the ts_brownian_motion() function will be a useful tool for anyone interested in simulating brownian motion, whether for financial modeling or any other application. Give it a try and see what you can do with it!\nRight now the function is a bit slow at .num_sims > 500 so I am working on optimizing it. I will also later on be introducing the Geometric Brownian Motion to {healthyR.ts}\nAs always, we welcome feedback and suggestions for new features and improvements. Thank you for using the {healthyR.ts} package, and happy simulating!\n\n\nFunction\nHere is the full function call:\n\nts_brownian_motion(\n  .time = 100,\n  .num_sims = 10,\n  .delta_time = 1,\n  .initial_value = 0\n)\n\n\n\nExample\nA simple example of the output.\n\nlibrary(healthyR.ts)\n\nts_brownian_motion()\n\n# A tibble: 1,010 × 3\n   sim_number     t     y\n   <fct>      <dbl> <dbl>\n 1 1              0  0   \n 2 1              1  1.46\n 3 1              2  2.68\n 4 1              3  2.78\n 5 1              4  3.07\n 6 1              5  3.43\n 7 1              6  3.05\n 8 1              7  4.43\n 9 1              8  6.04\n10 1              9  6.89\n# … with 1,000 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-10/index.html",
    "href": "posts/rtip-2023-01-10/index.html",
    "title": "Optimal Break Points for Histograms with {healthyR}",
    "section": "",
    "text": "Introduction\nHistogram binning is a technique used in data visualization to group continuous data into a set of discrete bins, or intervals. The purpose of histogram binning is to represent the distribution of a dataset in a graphical format, allowing for easy identification of patterns and outliers. However, there are several challenges that can arise when working with histogram binning.\nOne major challenge is determining the appropriate number of bins to use. If there are too few bins, the histogram may not accurately represent the underlying distribution of the data. On the other hand, if there are too many bins, the histogram may become cluttered and difficult to interpret. To overcome this challenge, there are several strategies that can be employed, such as the Scott’s normal reference rule, Freedman-Diaconis rule, or the Rice rule, to determine the optimal number of bins.\nAnother challenge with histogram binning is dealing with outliers. If a dataset has outliers, they can greatly skew the distribution of the data and make it difficult to interpret the histogram. One strategy to handle outliers is to use a log-scale on the x-axis, which can help to reduce their impact on the histogram. Alternatively, one could remove the outlier data points before creating the histogram.\nA further challenge is to choose the width of the bin that best represents the data. Too narrow bins might cause overfitting and too wide bins may cause loss of information. Different widths of bin can lead to a different representation of the data and hence a different conclusion. To overcome this, one could use the Freedman-Diaconis rule which take into consideration the range and the size of the sample to provide a robust and adaptive way to choose the width of the bin\nA simple solution to these challenges is the opt_bin() function in the {healthyR} library for R. This function uses an optimal binning algorithm to automatically determine the number of bins and bin widths that best represent the data. This can save a lot of time and effort when working with histograms and can help to ensure that the resulting histograms are accurate and easy to interpret.\nIn conclusion, histogram binning is a useful technique for visualizing the distribution of data, but it can be challenging to determine the appropriate number of bins and bin widths. Strategies such as Scott’s normal reference rule, Freedman-Diaconis rule, or the Rice rule can be used to determine the optimal number of bins. Outliers and bin width selection also can be challenges to take into account, and a function such as opt_bin() in the {healthyR} library can be used to overcome these challenges and create high-quality histograms with ease.\n\n\nFunction\nHere is the full call.\n\nopt_bin(.data, .value_col, .iters = 30)\n\nHere are the arguments that get passed to the parameters.\n\n.data - The data set in question\n.value_col - The column that holds the values\n.iters - How many times the cost function loop should run\n\nNow under I will provide some code and it’s explanation under the hood that exaplains how this works.\nHere is a breakdown of what each part of the code is doing:\n\nn <- 2:iters: this line creates a sequence of numbers starting at 2 and ending at the number specified by the variable “iters”\nc <- base::numeric(base::length(n)) and d <- c: These lines create two empty numeric vectors (arrays) called “c” and “d” with the same length as the sequence “n”\nfor (i in 1:length(n)) {...}: This is a for loop that iterates through each number in the sequence “n”\nd[i] <- diff(range( data ) ) / n[i]: Inside the loop, this line calculates the width of the bin for the current iteration by dividing the range of the data by the current number in the sequence “n”\nhp <- graphics::hist(data, breaks = edges, plot = FALSE) and ki <- hp$counts: This creates a histogram of the data using the current bin width and then gets the count of data points in each bin\nk <- mean(ki) and v <- sum((ki-k)^2/n[i]): this line uses the counts from the previous step to calculate the average count across all bins (k) and the variance of the counts across all bins (v)\nc[i] <- (2*k - v)/d[i]^2: this line calculates the cost function for the current bin width, which is based on k and v\nidx <- which.min(c) and opt_d <- d[idx]: this line finds the index in the “c” vector where the cost function is the lowest and stores that value in the variable “idx”\nedges <- seq(min(data), max(data), length = n[idx]) and edges <- tibble::as_tibble(edges): this line creates a new sequence of numbers representing the edges of the bins with the optimal bin width\nreturn(edges): this line returns the sequence of optimal bin edges that the function has determined\n\nOverall, this function will return an optimal set of bin edges for a histogram of the given data, the function uses an iterative process and consider the balance between the number of bins and the width of the bins to find the optimal width of the bins for the histogram. This could save a lot of time and effort for the data analysts as it can help to ensure that the resulting histograms are accurate and easy to interpret.\n\n\nExample\nLet’s look at some examples.\n\nlibrary(healthyR)\nlibrary(tidyverse)\n\ndf_tbl <- rnorm(n = 1000, mean = 0, sd = 1)\ndf_tbl <- df_tbl %>%\n  as_tibble()\n\nopt_bin(\n  .data = df_tbl,\n  .value_col = value\n  , .iters = 100\n)\n\n# A tibble: 6 × 1\n   value\n   <dbl>\n1 -3.04 \n2 -1.81 \n3 -0.585\n4  0.644\n5  1.87 \n6  3.10 \n\n\nNow lets user a smaller n to see how the output changes\n\nopt_bin(\n  .data = as_tibble(rnorm(n = 50)),\n  value,\n  100\n)\n\n# A tibble: 11 × 1\n     value\n     <dbl>\n 1 -1.69  \n 2 -1.24  \n 3 -0.797 \n 4 -0.351 \n 5  0.0944\n 6  0.540 \n 7  0.986 \n 8  1.43  \n 9  1.88  \n10  2.32  \n11  2.77  \n\n\nLet’s visualize.\n\nrn <- rnorm(50)\nhist(rn)\n\n\n\n\nNow let’s use opt_bin()\n\nhist(rn, breaks = opt_bin(as_tibble(rn), value) %>% pull())\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-12/index.html",
    "href": "posts/rtip-2023-01-12/index.html",
    "title": "An Update on {tidyAML}",
    "section": "",
    "text": "Introduction\nI have been doing a lot of work on a new package called {tidyAML}. {tidyAML} is a new R package that makes it easy to use the {tidymodels} ecosystem to perform automated machine learning (AutoML). This package provides a simple and intuitive interface that allows users to quickly generate machine learning models without worrying about the underlying details. It also includes a safety mechanism that ensures that the package will fail gracefully if any required extension packages are not installed on the user’s machine. With {tidyAML}, users can easily build high-quality machine learning models in just a few lines of code. Whether you are a beginner or an experienced machine learning practitioner, {tidyAML} has something to offer.\nSome ideas are that we should be able to generate regression models on the fly without having to actually go through the process of building the specification, especially if it is a non-tuning model, meaning we are not planing on tuning hyper-parameters like penalty and cost.\nThe idea is not to re-write the excellent work the {tidymodels} team has done (because it’s not possible) but rather to try and make an enhanced easy to use set of functions that do what they say and can generate many models and predictions at once.\nThis is similar to the great {h2o} package, but, {tidyAML} does not require java to be setup properly like {h2o} because {tidyAML} is built on {tidymodels}.\nThis package is not yet release, so you can only install from GitHub with the following:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"spsanderson/tidyAML\")\n\n\n\nExample\n\nlibrary(tidyAML)\n\nfast_regression_parsnip_spec_tbl(.parsnip_fns = \"linear_reg\")\n\n# A tibble: 11 × 5\n   .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n       <int> <chr>           <chr>         <chr>        <list>    \n 1         1 lm              regression    linear_reg   <spec[+]> \n 2         2 brulee          regression    linear_reg   <spec[+]> \n 3         3 gee             regression    linear_reg   <spec[+]> \n 4         4 glm             regression    linear_reg   <spec[+]> \n 5         5 glmer           regression    linear_reg   <spec[+]> \n 6         6 glmnet          regression    linear_reg   <spec[+]> \n 7         7 gls             regression    linear_reg   <spec[+]> \n 8         8 lme             regression    linear_reg   <spec[+]> \n 9         9 lmer            regression    linear_reg   <spec[+]> \n10        10 stan            regression    linear_reg   <spec[+]> \n11        11 stan_glmer      regression    linear_reg   <spec[+]> \n\nfast_regression_parsnip_spec_tbl(.parsnip_eng = c(\"lm\",\"glm\"))\n\n# A tibble: 3 × 5\n  .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n      <int> <chr>           <chr>         <chr>        <list>    \n1         1 lm              regression    linear_reg   <spec[+]> \n2         2 glm             regression    linear_reg   <spec[+]> \n3         3 glm             regression    poisson_reg  <spec[+]> \n\nfast_regression_parsnip_spec_tbl(.parsnip_eng = c(\"lm\",\"glm\",\"gee\"), \n                                 .parsnip_fns = \"linear_reg\")\n\n# A tibble: 3 × 5\n  .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n      <int> <chr>           <chr>         <chr>        <list>    \n1         1 lm              regression    linear_reg   <spec[+]> \n2         2 gee             regression    linear_reg   <spec[+]> \n3         3 glm             regression    linear_reg   <spec[+]> \n\n\nAs shown we can easily select the models we want either by choosing the supported parsnip function like linear_reg() or by choose the desired engine, you can also use them both in conjunction with each other!\nNow, what if you want to create a non-tuning model spec without using the fast_regression_parsnip_spec_tbl() function. Well, you can. The function is called create_model_spec().\n\ncreate_model_spec(\n .parsnip_eng = list(\"lm\",\"glm\",\"glmnet\",\"cubist\"),\n .parsnip_fns = list(\n      rep(\n        \"linear_reg\", 3),\n        \"cubist_rules\"\n     )\n )\n\n# A tibble: 4 × 4\n  .parsnip_engine .parsnip_mode .parsnip_fns .model_spec\n  <chr>           <chr>         <chr>        <list>     \n1 lm              regression    linear_reg   <spec[+]>  \n2 glm             regression    linear_reg   <spec[+]>  \n3 glmnet          regression    linear_reg   <spec[+]>  \n4 cubist          regression    cubist_rules <spec[+]>  \n\ncreate_model_spec(\n .parsnip_eng = list(\"lm\",\"glm\",\"glmnet\",\"cubist\"),\n .parsnip_fns = list(\n      rep(\n        \"linear_reg\", 3),\n        \"cubist_rules\"\n     ),\n .return_tibble = FALSE\n )\n\n$.parsnip_engine\n$.parsnip_engine[[1]]\n[1] \"lm\"\n\n$.parsnip_engine[[2]]\n[1] \"glm\"\n\n$.parsnip_engine[[3]]\n[1] \"glmnet\"\n\n$.parsnip_engine[[4]]\n[1] \"cubist\"\n\n\n$.parsnip_mode\n$.parsnip_mode[[1]]\n[1] \"regression\"\n\n\n$.parsnip_fns\n$.parsnip_fns[[1]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[2]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[3]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[4]]\n[1] \"cubist_rules\"\n\n\n$.model_spec\n$.model_spec[[1]]\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n$.model_spec[[2]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\n$.model_spec[[3]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glmnet \n\n\n$.model_spec[[4]]\nCubist Model Specification (regression)\n\nComputational engine: cubist \n\n\nNow the reason we are here. Let’s take a look at the first function for modeling with tidyAML, fast_regression().\n\nlibrary(recipes)\nlibrary(dplyr)\nlibrary(purrr)\n\nrec_obj <- recipe(mpg ~ ., data = mtcars)\nfrt_tbl <- fast_regression(\n  .data = mtcars, \n  .rec_obj = rec_obj, \n  .parsnip_eng = c(\"lm\",\"glm\"),\n  .parsnip_fns = \"linear_reg\"\n)\n\nglimpse(frt_tbl)\n\nRows: 2\nColumns: 8\n$ .model_id       <int> 1, 2\n$ .parsnip_engine <chr> \"lm\", \"glm\"\n$ .parsnip_mode   <chr> \"regression\", \"regression\"\n$ .parsnip_fns    <chr> \"linear_reg\", \"linear_reg\"\n$ model_spec      <list> [~NULL, ~NULL, NULL, regression, TRUE, NULL, lm, TRUE]…\n$ wflw            <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ fitted_wflw     <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ pred_wflw       <list> [<tbl_df[24 x 1]>], [<tbl_df[24 x 1]>]\n\n\nNow lets take a look at a few different things in the frt_tbl.\n\nnames(frt_tbl)\n\n[1] \".model_id\"       \".parsnip_engine\" \".parsnip_mode\"   \".parsnip_fns\"   \n[5] \"model_spec\"      \"wflw\"            \"fitted_wflw\"     \"pred_wflw\"      \n\n\nLet’s look at a single model spec.\n\nfrt_tbl %>% slice(1) %>% select(model_spec) %>% pull() %>% pluck(1)\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nNow the wflw column.\n\nfrt_tbl %>% slice(1) %>% select(wflw) %>% pull() %>% pluck(1)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nThe fitted wflw object.\n\nfrt_tbl %>% slice(1) %>% select(fitted_wflw) %>% pull() %>% pluck(1)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)          cyl         disp           hp         drat           wt  \n   11.77621      0.59296      0.01626     -0.03191     -0.55350     -5.30785  \n       qsec           vs           am         gear         carb  \n    0.97840      2.64023      1.68549      0.87059      0.58785  \n\nfrt_tbl %>% slice(1) %>% select(fitted_wflw) %>% pull() %>% pluck(1) %>%\n  broom::glance() %>%\n  glimpse()\n\nRows: 1\nColumns: 12\n$ r.squared     <dbl> 0.9085669\n$ adj.r.squared <dbl> 0.8382337\n$ sigma         <dbl> 2.337527\n$ statistic     <dbl> 12.91804\n$ p.value       <dbl> 3.367361e-05\n$ df            <dbl> 10\n$ logLik        <dbl> -47.07551\n$ AIC           <dbl> 118.151\n$ BIC           <dbl> 132.2877\n$ deviance      <dbl> 71.03241\n$ df.residual   <int> 13\n$ nobs          <int> 24\n\n\nAnd finally the predictions (this one I am probably going to change up).\n\nfrt_tbl %>% slice(1) %>% select(pred_wflw) %>% pull() %>% pluck(1)\n\n# A tibble: 24 × 1\n   .pred\n   <dbl>\n 1  17.4\n 2  28.4\n 3  17.2\n 4  10.7\n 5  13.4\n 6  17.0\n 7  22.8\n 8  14.3\n 9  22.4\n10  15.5\n# … with 14 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-16/index.html",
    "href": "posts/rtip-2023-01-16/index.html",
    "title": "Auto K-Means with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nToday’s post is going to center around the automatic k-means functionality of {healthyR.ai}. I am not going to get into what it is or how it works, but rather the function call itself and how it works and what it puts out. The function is called hai_kmeans_automl. This function is a wrapper around the h2o::h2o.kmeans() function, but also does some processing to enhance the output at the end. Let’s get to it!\n\n\nFunction\nHere is the full function call.\n\nhai_kmeans_automl(\n  .data,\n  .split_ratio = 0.8,\n  .seed = 1234,\n  .centers = 10,\n  .standardize = TRUE,\n  .print_model_summary = TRUE,\n  .predictors,\n  .categorical_encoding = \"auto\",\n  .initialization_mode = \"Furthest\",\n  .max_iterations = 100\n)\n\nNow let’s go over the function arguments:\n\n.data - The data that is to be passed for clustering.\n.split_ratio - The ratio for training and testing splits.\n.seed - The default is 1234, but can be set to any integer.\n.centers - The default is 1. Specify the number of clusters (groups of data) in a data set.\n.standardize - The default is set to TRUE. When TRUE all numeric columns will be set to zero mean and unit variance.\n.print_model_summary - This is a boolean and controls if the model summary is printed to the console. The default is TRUE.\n.predictors - This must be in the form of c(“column_1”, “column_2”, … “column_n”)\n.categorical_encoding - Can be one of the following:\n\n“auto”\n“enum”\n“one_hot_explicit”\n“binary”\n“eigen”\n“label_encoder”\n“sort_by_response”\n“enum_limited”\n\n.initialization_mode - This can be one of the following:\n\n“Random”\n“Furthest (default)\n“PlusPlus”\n\n.max_iterations - The default is 100. This specifies the number of training iterations\n\n\n\nExamples\nTime for some examples.\n\nlibrary(healthyR.ai)\nlibrary(h2o)\n\nh2o.init()\n\noutput <- hai_kmeans_automl(\n  .data = iris,\n  .predictors = c(\n    \"Sepal.Width\", \"Sepal.Length\", \"Petal.Width\", \"Petal.Length\"\n    ),\n  .standardize = TRUE,\n  .split_ratio = .8\n)\n\nh2o.shutdown(prompt = FALSE)\n\nNow let’s take a look at the output. There are going to be 4 pieces of main output. Here they are:\n\ndata\nauto_kmeans_obj\nmodel_id\nscree_plt\n\nLet’s take a look at each one. First the data output which itself has 6 different objects in it.\n\noutput$data\n\n$splits\n$splits$training_tbl\n# A tibble: 123 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          5           3.6          1.4         0.2 setosa \n 5          5.4         3.9          1.7         0.4 setosa \n 6          4.6         3.4          1.4         0.3 setosa \n 7          5           3.4          1.5         0.2 setosa \n 8          4.4         2.9          1.4         0.2 setosa \n 9          5.4         3.7          1.5         0.2 setosa \n10          4.8         3.4          1.6         0.2 setosa \n# … with 113 more rows\n\n$splits$validate_tbl\n# A tibble: 27 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          4.6         3.1          1.5         0.2 setosa \n 2          4.9         3.1          1.5         0.1 setosa \n 3          5.8         4            1.2         0.2 setosa \n 4          5.1         3.5          1.4         0.3 setosa \n 5          5.7         3.8          1.7         0.3 setosa \n 6          5.1         3.8          1.5         0.3 setosa \n 7          5.4         3.4          1.7         0.2 setosa \n 8          5.1         3.7          1.5         0.4 setosa \n 9          5           3.4          1.6         0.4 setosa \n10          4.7         3.2          1.6         0.2 setosa \n# … with 17 more rows\n\n\n$metrics\n$metrics$training_metrics\n# A tibble: 2 × 3\n  centroid  size within_cluster_sum_of_squares\n     <dbl> <dbl>                         <dbl>\n1        1    87                         145. \n2        2    36                          38.3\n\n$metrics$validation_metrics\n# A tibble: 2 × 3\n  centroid  size within_cluster_sum_of_squares\n     <dbl> <dbl>                         <dbl>\n1        1    13                          35.7\n2        2    14                          13.4\n\n$metrics$cv_metric_summary\n# A tibble: 5 × 8\n  metric_name   mean    sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid cv_5_va…¹\n  <chr>        <dbl> <dbl>      <dbl>      <dbl>      <dbl>      <dbl>     <dbl>\n1 betweenss     62.5 12.4        66.7       72.7       71.6       42.5      59.1\n2 mse          NaN    0         NaN        NaN        NaN        NaN       NaN  \n3 rmse         NaN    0         NaN        NaN        NaN        NaN       NaN  \n4 tot_withinss  33.6  7.36       45.6       29.7       34.8       26.5      31.3\n5 totss         96.1 17.1       112.       102.       106.        69.0      90.3\n# … with abbreviated variable name ¹​cv_5_valid\n\n\n$original_data\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n[150 rows x 5 columns] \n\n$scree_data_tbl\n# A tibble: 2 × 2\n  centers   wss\n    <dbl> <dbl>\n1       1  745.\n2       2  226.\n\n$scoring_history_tbl\n# A tibble: 6 × 6\n  timestamp           duration     iterations number_of_clusters numbe…¹ withi…²\n  <chr>               <chr>             <dbl>              <dbl>   <dbl>   <dbl>\n1 2023-01-16 09:41:25 \" 0.241 sec\"          0                  0     NaN    NaN \n2 2023-01-16 09:41:25 \" 0.246 sec\"          1                  1     123   1003.\n3 2023-01-16 09:41:25 \" 0.247 sec\"          2                  1       0    488 \n4 2023-01-16 09:41:25 \" 0.250 sec\"          3                  2      26    311.\n5 2023-01-16 09:41:25 \" 0.251 sec\"          4                  2       2    184.\n6 2023-01-16 09:41:25 \" 0.251 sec\"          5                  2       0    183.\n# … with abbreviated variable names ¹​number_of_reassigned_observations,\n#   ²​within_cluster_sum_of_squares\n\n$model_summary_tbl\n# A tibble: 7 × 2\n  name                           value\n  <chr>                          <dbl>\n1 number_of_rows                  123 \n2 number_of_clusters                2 \n3 number_of_categorical_columns     0 \n4 number_of_iterations              5 \n5 within_cluster_sum_of_squares   183.\n6 total_sum_of_squares            488 \n7 between_cluster_sum_of_squares  305.\n\n\nNow lets take a look at the auto_kmeans_obj\n\noutput$auto_kmeans_obj\n\nModel Details:\n==============\n\nH2OClusteringModel: kmeans\nModel ID:  KMeans_model_R_1673880074548_1 \nModel Summary: \n  number_of_rows number_of_clusters number_of_categorical_columns\n1            123                  2                             0\n  number_of_iterations within_cluster_sum_of_squares total_sum_of_squares\n1                    5                     183.42511            488.00000\n  between_cluster_sum_of_squares\n1                      304.57489\n\n\nH2OClusteringMetrics: kmeans\n** Reported on training data. **\n\n\nTotal Within SS:  183.4251\nBetween SS:  304.5749\nTotal SS:  488 \nCentroid Statistics: \n  centroid     size within_cluster_sum_of_squares\n1        1 87.00000                     145.14706\n2        2 36.00000                      38.27805\n\nH2OClusteringMetrics: kmeans\n** Reported on validation data. **\n\n\nTotal Within SS:  49.05625\nBetween SS:  53.9303\nTotal SS:  102.9865 \nCentroid Statistics: \n  centroid     size within_cluster_sum_of_squares\n1        1 13.00000                      35.67618\n2        2 14.00000                      13.38007\n\nH2OClusteringMetrics: kmeans\n** Reported on cross-validation data. **\n** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n\n\nTotal Within SS:  167.8887\nBetween SS:  320.1113\nTotal SS:  488 \nCentroid statistics are not available.\nCross-Validation Metrics Summary: \n                  mean        sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\nbetweenss    62.496490 12.417224  66.671760  72.679665  71.595474  42.470100\nmse                 NA  0.000000         NA         NA         NA         NA\nrmse                NA  0.000000         NA         NA         NA         NA\ntot_withinss 33.577747  7.357984  45.607327  29.705257  34.811970  26.512373\ntotss        96.074234 17.148642 112.279080 102.384926 106.407450  68.982475\n             cv_5_valid\nbetweenss     59.065456\nmse                  NA\nrmse                 NA\ntot_withinss  31.251806\ntotss         90.317260\n\n\nThe model id:\n\noutput$model_id\n\n[1] \"KMeans_model_R_1673880074548_1\"\n\n\nAnd finally the scree_plt.\n\noutput$scree_plt\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-17/index.html",
    "href": "posts/rtip-2023-01-17/index.html",
    "title": "Augmenting a Brownian Motion to a Time Series with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nTime series analysis is a crucial tool for forecasting and understanding trends in various industries, including finance, economics, and engineering. However, traditional time series analysis methods can be limiting, and they may not always capture the complex dynamics of real-world data. That’s where the R package {healthyR.ts} comes in.\nThe {healthyR.ts} package is a powerful tool for time series analysis that offers a wide range of functions for cleaning, transforming, and analyzing time series data. One of its standout features is the ts_brownian_motion_augment() function, which allows you to add a brownian motion to a given time series dataset. This powerful tool can be used to simulate more realistic and complex scenarios, making it an invaluable tool for forecasters and data analysts.\nBrownian motion is a random walk process that can be used to model the movement of particles in a fluid. It has been widely used in mathematical finance, physics, and engineering to model the random movements of stock prices, pollutant concentrations, and other phenomena. By adding a brownian motion to a time series dataset, the ts_brownian_motion_augment() function allows users to capture the unpredictable and random nature of real-world data, making time series analysis more accurate and reliable.\nThe ts_brownian_motion_augment() function is easy to use and requires no prior knowledge of brownian motion or advanced mathematics. With just a few lines of code, users can quickly add a brownian motion to their time series dataset and begin analyzing the data with greater precision and confidence.\nThis set of functionality will be included in the next release which will be coming soon as it also speeds up the current ts_brownian_motion() function by 49x!\n\n\nFunction\nHere is the full function call.\n\nts_brownian_motion_augment(\n  .data,\n  .date_col,\n  .value_col,\n  .time = 100,\n  .num_sims = 10,\n  .delta_time = NULL\n)\n\nLet’s take a look at the arguments for the parameters.\n\n.data - The data.frame/tibble being augmented.\n.date_col - The column that holds the date.\n.value_col - The value that is going to get augmented. The last value of this column becomes the initial value internally.\n.time - How many time steps ahead.\n.num_sims - How many simulations should be run.\n.delta_time - Time step size.\n\n\n\nExample\nNow for an example.\n\nlibrary(tidyquant)\nlibrary(dplyr)\nlibrary(timetk)\nlibrary(ggplot2)\nlibrary(healthyR.ts)\n\ndf <- FANG %>%\n  filter(symbol == \"FB\") %>%\n  select(symbol, date, adjusted) %>%\n  filter_by_time(.date_var = date, .start_date = \"2016-01-01\") %>%\n  tq_mutate(select = adjusted, mutate_fun = periodReturn,\n            period = \"daily\", type = \"log\",\n            col_rename = \"daily_returns\")\n\nLet’s take a look at our initial data.\n\ndf\n\n# A tibble: 252 × 4\n   symbol date       adjusted daily_returns\n   <chr>  <date>        <dbl>         <dbl>\n 1 FB     2016-01-04    102.        0      \n 2 FB     2016-01-05    103.        0.00498\n 3 FB     2016-01-06    103.        0.00233\n 4 FB     2016-01-07     97.9      -0.0503 \n 5 FB     2016-01-08     97.3      -0.00604\n 6 FB     2016-01-11     97.5       0.00185\n 7 FB     2016-01-12     99.4       0.0189 \n 8 FB     2016-01-13     95.4      -0.0404 \n 9 FB     2016-01-14     98.4       0.0302 \n10 FB     2016-01-15     95.0      -0.0352 \n# … with 242 more rows\n\n\nNow let’s augment it with the brownian motion and see that data set before we visualize it.\n\ndf %>%\n  ts_brownian_motion_augment(\n    .date_col = date,\n    .num_sims = 50,\n    .value_col = daily_returns,\n    .delta_time = 0.00005\n  )\n\n# A tibble: 5,302 × 3\n   sim_number  date       daily_returns\n   <fct>       <date>             <dbl>\n 1 actual_data 2016-01-04       0      \n 2 actual_data 2016-01-05       0.00498\n 3 actual_data 2016-01-06       0.00233\n 4 actual_data 2016-01-07      -0.0503 \n 5 actual_data 2016-01-08      -0.00604\n 6 actual_data 2016-01-11       0.00185\n 7 actual_data 2016-01-12       0.0189 \n 8 actual_data 2016-01-13      -0.0404 \n 9 actual_data 2016-01-14       0.0302 \n10 actual_data 2016-01-15      -0.0352 \n# … with 5,292 more rows\n\n\nAs you see the function preserves the names of the input columns!\nNow, let’s see it!\n\ndf %>%\n  ts_brownian_motion_augment(\n    .date_col = date,\n    .num_sims = 50,\n    .value_col = daily_returns,\n    .delta_time = 0.00005\n  ) %>%\n  ggplot(aes(x = date, y = daily_returns\n             , group = sim_number, color = sim_number)) +\n  geom_line() +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(\n    title = \"FB Log Daily Returns for 2016\",\n    x = \"Date\",\n    y = \"Log Daily Returns\"\n  )\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-18/index.html",
    "href": "posts/rtip-2023-01-18/index.html",
    "title": "Geometric Brownian Motion with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nGeometric Brownian motion (GBM) is a widely used model in financial analysis for modeling the behavior of stock prices. It is a stochastic process that describes the evolution of a stock price over time, assuming that the stock price follows a random walk with a drift term and a volatility term.\nOne of the advantages of GBM is that it can capture the randomness and volatility of stock prices, which is a key feature of financial markets. GBM can also be used to estimate the expected return and volatility of a stock, which are important inputs for financial decision making.\nAnother advantage of GBM is that it can be used to generate simulations of future stock prices. These simulations can be used to estimate the probability of different outcomes, such as the probability of a stock price reaching a certain level in the future. This can be useful for risk management and for evaluating investment strategies.\nGBM is also very easy to implement, making it a popular choice among financial analysts and traders.\nThe equation for GBM is: \\[\ndS(t) = μS(t)dt + σS(t)dW(t)\n\\] Where:\n\\(dS(t)\\) is the change in the stock price at time \\(t\\)\n\\(S(t)\\) is the stock price at time \\(t\\)\n\\(μ\\) is the expected return of the stock\n\\(σ\\) is the volatility of the stock\n\\(dW(t)\\) is a Wiener process (a random variable that describes the rate of change of a random variable over time)\nIt’s important to keep in mind that GBM is a model and not always a perfect fit to real-world stock prices. However, it’s a widely accepted model due to its capability to captures the key characteristics of stock prices and its mathematical tractability.\nAttention R users! Are you looking for a reliable and accurate way to model stock prices? We have some exciting news for you! The next release of the R package {healthyR.ts} will include a new function, ts_geometric_brownian_motion(). This powerful function utilizes the geometric Brownian motion model to simulate stock prices, providing you with valuable insights and predictions for your financial analysis.\n\n\nFunction\nLet’s take a look at the full function call.\n\nts_geometric_brownian_motion(\n  .num_sims = 100,\n  .time = 25,\n  .mean = 0,\n  .sigma = 0.1,\n  .initial_value = 100,\n  .delta_time = 1/365,\n  .return_tibble = TRUE\n)\n\nNow let’s go over the arguments to the parameters.\n\n.num_sims - Total number of simulations.\n.time - Total time of the simulation.\n.mean - Expected return\n.sigma - Volatility\n.initial_value - Integer representing the initial value.\n.delta_time - Time step size.\n.return_tibble The default is TRUE. If set to FALSE then an object of class matrix will be returned.\n\n\n\nExample\nLet’s go over a few examples.\n\nlibrary(healthyR.ts)\n\nts_geometric_brownian_motion()\n\n# A tibble: 2,600 × 3\n   sim_number        t     y\n   <fct>         <int> <dbl>\n 1 sim_number 1      1   100\n 2 sim_number 2      1   100\n 3 sim_number 3      1   100\n 4 sim_number 4      1   100\n 5 sim_number 5      1   100\n 6 sim_number 6      1   100\n 7 sim_number 7      1   100\n 8 sim_number 8      1   100\n 9 sim_number 9      1   100\n10 sim_number 10     1   100\n# … with 2,590 more rows\n\n\nNow without returning a tibble object.\n\nts_geometric_brownian_motion(.num_sims = 5, .return_tibble = FALSE)\n\n      sim_number 1 sim_number 2 sim_number 3 sim_number 4 sim_number 5\n [1,]    100.00000     100.0000    100.00000    100.00000     100.0000\n [2,]    101.04170     100.6583    100.46420     99.68513     100.3776\n [3,]    101.58155     100.8959    100.03621     98.91656     101.5732\n [4,]    100.91680     100.7494     99.47735     98.57117     101.1525\n [5,]     99.96787     101.3298     98.70899     99.03101     101.1557\n [6,]     99.29069     101.4187     98.32176     98.33018     101.5584\n [7,]     99.40451     101.5124     98.26237     97.79356     101.4934\n [8,]     99.35345     101.0328     98.69587     97.46604     101.9630\n [9,]     97.94177     100.9534     98.32630     96.95231     102.1643\n[10,]     97.95812     101.3813     98.36934     96.64048     101.8546\n[11,]     98.47820     101.8262     98.21492     96.12851     102.5529\n[12,]     99.53016     102.5522     97.92270     95.97443     102.8912\n[13,]     98.82850     102.7482     96.66348     96.26008     103.1899\n[14,]     99.87335     102.9351     96.69635     96.15058     103.9259\n[15,]    101.03605     103.3796     96.60162     96.63562     103.3790\n[16,]    101.83475     103.1900     97.63875     96.00162     103.0422\n[17,]    102.10155     103.5851     97.12873     95.99579     103.0913\n[18,]    102.16085     103.2966     96.26772     95.95174     103.7034\n[19,]    102.35736     103.7429     96.37355     96.02805     102.8406\n[20,]    102.49297     104.5301     96.44318     96.28293     103.3507\n[21,]    102.36953     105.1809     96.87639     97.32625     104.0307\n[22,]    103.30672     104.7480     96.90017     97.16507     104.0751\n[23,]    103.55433     104.9848     97.40063     97.49375     102.6901\n[24,]    103.44429     104.3553     97.35982     97.39390     102.8163\n[25,]    103.23952     102.9840     97.30287     97.66737     103.2160\n[26,]    103.48365     103.6117     97.96290     97.91773     103.0579\nattr(,\".time\")\n[1] 25\nattr(,\".num_sims\")\n[1] 5\nattr(,\".mean\")\n[1] 0\nattr(,\".sigma\")\n[1] 0.1\nattr(,\".initial_value\")\n[1] 100\nattr(,\".delta_time\")\n[1] 0.002739726\nattr(,\".return_tibble\")\n[1] FALSE\n\n\nLet’s visualize the GBM at different levels of volatility.\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\ngbm <- rbind(\n  ts_geometric_brownian_motion(.sigma = 0.05) %>%\n    mutate(volatility = as.factor(\"A) Sigma = 5%\")),\n  ts_geometric_brownian_motion(.sigma = 0.1) %>%\n    mutate(volatility = as.factor(\"B) Sigma = 10%\")),\n  ts_geometric_brownian_motion(.sigma = .15) %>%\n    mutate(volatility = as.factor(\"C) Sigma = 15%\")),\n  ts_geometric_brownian_motion(.sigma = .2) %>%\n    mutate(volatility = as.factor(\"D) Sigma = 20%\"))\n)\n\ngbm %>%\n  ggplot(aes(x = t, y = y, group = sim_number, color = sim_number)) + \n  facet_wrap(~ volatility, scales = \"free\") +\n  geom_line() +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-19/index.html",
    "href": "posts/rtip-2023-01-19/index.html",
    "title": "Boilerplate XGBoost with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nXGBoost, short for “eXtreme Gradient Boosting,” is a powerful and popular machine learning library that is specifically designed for gradient boosting. It is an open-source library and is available in many programming languages, including R.\nGradient boosting is a technique that combines the predictions of multiple weak models to create a strong, more accurate model. XGBoost is an optimized version of gradient boosting that is designed to run faster and more efficiently than other implementations.\nLet’s take a look at a simple example of how to use XGBoost in R. We will use the iris dataset, a well-known dataset that contains 150 observations of iris flowers, each with four features (sepal length, sepal width, petal length, and petal width) and one target variable (the species of iris). Our goal is to train a model to predict the species of an iris flower based on its features.\nFirst, we need to install the “xgboost” package in R:\n\ninstall.packages(\"xgboost\")\n\nNext, we load the iris dataset and split it into training and test sets:\n\ndata(iris)\nset.seed(123)\nindices <- sample(1:nrow(iris), 0.8*nrow(iris))\ntrain_data <- iris[indices, 1:4]\ntrain_label <- iris[indices, 5]\ntest_data <- iris[-indices, 1:4]\ntest_label <- iris[-indices, 5]\n\nNow we can train our XGBoost model:\n\nlibrary(xgboost)\nxgb_model <- xgboost(\n  data = train_data, \n  label = train_label, \n  nrounds = 100, \n  objective = \"multi:softmax\", \n  num_class = 3\n  )\n\nHere, we specified the training data, labels, number of rounds (iterations) to run, the objective (multiclass classification) and the number of classes.\nFinally, we can use the trained model to make predictions on the test set:\n\npredictions <- predict(xgb_model, test_data)\n\nWe can also evaluate the performance of our model by comparing the predicted labels to the true labels using metrics such as accuracy:\n\naccuracy <- mean(predictions == test_label)\n\nIn this example, we used XGBoost to train a model to predict the species of iris flowers based on their features. We saw that XGBoost is a powerful and efficient library for gradient boosting, and it can be easily integrated into a R script.\nKeep in mind that this is a simple example, and in real-world scenarios, more preprocessing and parameter tuning is necessary to achieve optimal performance. Also, the dataset is small, and the number of rounds used is also small, which is not ideal for real-world scenarios. But this example shows the basic usage of XGBoost in R.\nOk, so, what’s the point? Is there a possibly easier way to do this…yes! You can use the boilerplace function hai_auto_xgboost() and it’s data prep helper hai_xgboost_data_prepper() from the {healthyR.ai} library. Let’s see how that works.\n\n\nFunction\nHere is the data prepper function and it’s arguments.\n\nhai_xgboost_data_prepper(.data, .recipe_formula)\n\n\n.data - The data that you are passing to the function. Can be any type of data that is accepted by the data parameter of the recipes::reciep() function.\n.recipe_formula - The formula that is going to be passed. For example if you are using the diamonds data then the formula would most likely be something like price ~ .\n\nHere is the boilerplate function\n\nhai_auto_xgboost(\n  .data,\n  .rec_obj,\n  .splits_obj = NULL,\n  .rsamp_obj = NULL,\n  .tune = TRUE,\n  .grid_size = 10,\n  .num_cores = 1,\n  .best_metric = \"f_meas\",\n  .model_type = \"classification\"\n)\n\nHere are it’s arguments.\n\n.data - The data being passed to the function. The time-series object.\n.rec_obj - This is the recipe object you want to use. You can use hai_xgboost_data_prepper() an automatic recipe_object.\n.splits_obj - NULL is the default, when NULL then one will be created.\n.rsamp_obj - NULL is the default, when NULL then one will be created. It will default to creating an rsample::mc_cv() object.\n.tune - Default is TRUE, this will create a tuning grid and tuned workflow\n.grid_size - Default is 10\n.num_cores - Default is 1\n.best_metric - Default is “f_meas”. You can choose a metric depending on the model_type used. If regression then see hai_default_regression_metric_set(), if classification then see hai_default_classification_metric_set().\n.model_type - Default is classification, can also be regression.\n\n\n\nExample\nLet’s take a look at an example and it’s output. This is using {parsnip} under the hood.\n\nlibrary(healthyR.ai)\n\ndata <- iris\n\nrec_obj <- hai_xgboost_data_prepper(data, Species ~ .)\n\nauto_xgb <- hai_auto_xgboost(\n  .data = data,\n  .rec_obj = rec_obj,\n  .best_metric = \"f_meas\",\n  .num_cores = 1\n)\n\nThere are three main outputs to this function, which are:\n\nrecipe_info\nmodel_info\ntuned_info\n\nLet’s take a look at each. First the recipe_info\n\nauto_xgb$recipe_info\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nOperations:\n\nFactor variables from tidyselect::vars_select_helpers$where(is.charac...\nNovel factor level assignment for recipes::all_nominal_predictors()\nDummy variables from recipes::all_nominal_predictors()\nZero variance filter on recipes::all_predictors()\n\nNow the model_info\n\nauto_xgb$model_info\n\n$model_spec\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  trees = tune::tune()\n  min_n = tune::tune()\n  tree_depth = tune::tune()\n  learn_rate = tune::tune()\n  loss_reduction = tune::tune()\n  sample_size = tune::tune()\n\nComputational engine: xgboost \n\n\n$wflw\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_string2factor()\n• step_novel()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  trees = tune::tune()\n  min_n = tune::tune()\n  tree_depth = tune::tune()\n  learn_rate = tune::tune()\n  loss_reduction = tune::tune()\n  sample_size = tune::tune()\n\nComputational engine: xgboost \n\n\n$fitted_wflw\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_string2factor()\n• step_novel()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\n##### xgb.Booster\nraw: 2.5 Mb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.10962507492329, max_depth = 13L, \n    gamma = 0.000498577409120534, colsample_bytree = 1, colsample_bynode = 1, \n    min_child_weight = 3L, subsample = 0.594320066112559), data = x$data, \n    nrounds = 1240L, watchlist = x$watchlist, verbose = 0, nthread = 1, \n    objective = \"multi:softprob\", num_class = 3L)\nparams (as set within xgb.train):\n  eta = \"0.10962507492329\", max_depth = \"13\", gamma = \"0.000498577409120534\", colsample_bytree = \"1\", colsample_bynode = \"1\", min_child_weight = \"3\", subsample = \"0.594320066112559\", nthread = \"1\", objective = \"multi:softprob\", num_class = \"3\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 4 \nniter: 1240\nnfeatures : 4 \nevaluation_log:\n    iter training_mlogloss\n       1        0.96929822\n       2        0.85785438\n---                       \n    1239        0.07815044\n    1240        0.07808817\n\n$was_tuned\n[1] \"tuned\"\n\nNow the tuned_info\n\nauto_xgb$tuned_info\n\n$tuning_grid\n# A tibble: 10 × 6\n   trees min_n tree_depth learn_rate loss_reduction sample_size\n   <int> <int>      <int>      <dbl>          <dbl>       <dbl>\n 1   926     6          2    0.0246        2.21e- 1       0.952\n 2  1510    25         14    0.00189       1.01e+ 1       0.424\n 3  1077    29          9    0.195         1.34e- 5       0.319\n 4   795    32          3    0.00102       1.64e- 3       0.686\n 5   368    22          4    0.00549       2.97e- 7       0.735\n 6  1240     3         13    0.110         4.99e- 4       0.594\n 7  1839    18          5    0.0501        1.67e- 7       0.273\n 8   139    11         10    0.0153        1.17e- 2       0.483\n 9   470    40          8    0.0906        6.79e-10       0.168\n10  1732    16         11    0.00667       9.19e- 9       0.883\n\n$cv_obj\n# Monte Carlo cross-validation (0.75/0.25) with 25 resamples  \n# A tibble: 25 × 2\n   splits          id        \n   <list>          <chr>     \n 1 <split [84/28]> Resample01\n 2 <split [84/28]> Resample02\n 3 <split [84/28]> Resample03\n 4 <split [84/28]> Resample04\n 5 <split [84/28]> Resample05\n 6 <split [84/28]> Resample06\n 7 <split [84/28]> Resample07\n 8 <split [84/28]> Resample08\n 9 <split [84/28]> Resample09\n10 <split [84/28]> Resample10\n# … with 15 more rows\n\n$tuned_results\n# Tuning results\n# Monte Carlo cross-validation (0.75/0.25) with 25 resamples  \n# A tibble: 25 × 4\n   splits          id         .metrics            .notes          \n   <list>          <chr>      <list>              <list>          \n 1 <split [84/28]> Resample01 <tibble [110 × 10]> <tibble [1 × 3]>\n 2 <split [84/28]> Resample02 <tibble [110 × 10]> <tibble [1 × 3]>\n 3 <split [84/28]> Resample03 <tibble [110 × 10]> <tibble [1 × 3]>\n 4 <split [84/28]> Resample04 <tibble [110 × 10]> <tibble [1 × 3]>\n 5 <split [84/28]> Resample05 <tibble [110 × 10]> <tibble [1 × 3]>\n 6 <split [84/28]> Resample06 <tibble [110 × 10]> <tibble [1 × 3]>\n 7 <split [84/28]> Resample07 <tibble [110 × 10]> <tibble [1 × 3]>\n 8 <split [84/28]> Resample08 <tibble [110 × 10]> <tibble [1 × 3]>\n 9 <split [84/28]> Resample09 <tibble [110 × 10]> <tibble [1 × 3]>\n10 <split [84/28]> Resample10 <tibble [110 × 10]> <tibble [1 × 3]>\n# … with 15 more rows\n\nThere were issues with some computations:\n\n  - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...   - Warning(s) x1: While computing multiclass `precision()`, some levels had no pred...\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n$grid_size\n[1] 10\n\n$best_metric\n[1] \"f_meas\"\n\n$best_result_set\n# A tibble: 1 × 12\n  trees min_n tree_depth learn_rate loss_r…¹ sampl…² .metric .esti…³  mean     n\n  <int> <int>      <int>      <dbl>    <dbl>   <dbl> <chr>   <chr>   <dbl> <int>\n1  1240     3         13      0.110 0.000499   0.594 f_meas  macro   0.944    25\n# … with 2 more variables: std_err <dbl>, .config <chr>, and abbreviated\n#   variable names ¹​loss_reduction, ²​sample_size, ³​.estimator\n\n$tuning_grid_plot\n\n\n\n\nTuning Grid\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-23/index.html",
    "href": "posts/rtip-2023-01-23/index.html",
    "title": "ADF and Phillips-Perron Tests for Stationarity using lists",
    "section": "",
    "text": "Introduction\nA time series is a set of data points collected at regular intervals of time. Sometimes, the data points in a time series change over time in a predictable way. This is called a stationary time series. Other times, the data points change in an unpredictable way. This is called a non-stationary time series.\nImagine you are playing a game of catch with a friend. If you throw the ball back and forth at the same speed and distance, that’s like a stationary time series. But if you keep throwing the ball harder and farther, that’s like a non-stationary time series.\nThere are two tests that we can use to see if a time series is stationary or non-stationary. The first test is called the ADF test, which stands for Augmented Dickey-Fuller test. The second test is called the Phillips-Perron test.\nThe ADF test looks at the data points and checks to see if the average value of the data points is the same over time. If the average value is the same, then the time series is stationary. If the average value is not the same, then the time series is non-stationary.\nThe Phillips-Perron test is similar to the ADF test, but it is a bit more advanced. It checks to see if the data points are changing in a predictable way. If the data points are changing in a predictable way, then the time series is stationary. If the data points are changing in an unpredictable way, then the time series is non-stationary.\nSo, in short, The ADF test checks if the mean of the time series is constant over time and Phillips-Perron test checks if the variance of the time series is constant over time.\nNow, you can use these two tests to see if the time series you are studying is stationary or non-stationary, just like how you can use the game of catch to see if your throws are the same or different.\n\n\nFunction\nTo perform these test we can use two libraries, one is the {tseries} library for the adf.test() and the other is the {aTSA} for the pp.test()\nLet’s see some examples.\n\n\nExamples\nLet’s first make our time series obejcts and place them in a list.\n\nlibrary(tseries)\nlibrary(aTSA)\n\n# create time series objects\nts1 <- ts(rnorm(100), start = c(1990,1), frequency = 12)\nts2 <- ts(rnorm(100), start = c(1995,1), frequency = 12)\nts3 <- ts(rnorm(100), start = c(2000,1), frequency = 12)\n\n# create list of time series\nts_list <- list(ts1, ts2, ts3)\n\nNow let’s make our functions.\n\n# function to test for stationarity\nadf_is_stationary <- function(x) {\n  adf.test(x)$p.value > 0.05\n}\n\npp_is_stationary <- function(x) {\n  pp_df <- pp.test(x) |> as.data.frame() \n  pp_df$p.value > 0.05\n}\n\nTime to use lapply()!\n\n# apply function to each time series in list\nlapply(ts_list, adf_is_stationary)\n\nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag    ADF p.value\n[1,]   0 -10.97    0.01\n[2,]   1  -7.70    0.01\n[3,]   2  -5.23    0.01\n[4,]   3  -4.05    0.01\n[5,]   4  -4.03    0.01\nType 2: with drift no trend \n     lag    ADF p.value\n[1,]   0 -11.48    0.01\n[2,]   1  -8.32    0.01\n[3,]   2  -5.81    0.01\n[4,]   3  -4.59    0.01\n[5,]   4  -4.63    0.01\nType 3: with drift and trend \n     lag    ADF p.value\n[1,]   0 -11.42    0.01\n[2,]   1  -8.28    0.01\n[3,]   2  -5.77    0.01\n[4,]   3  -4.56    0.01\n[5,]   4  -4.59    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag    ADF p.value\n[1,]   0 -10.60    0.01\n[2,]   1  -7.88    0.01\n[3,]   2  -5.96    0.01\n[4,]   3  -5.26    0.01\n[5,]   4  -4.90    0.01\nType 2: with drift no trend \n     lag    ADF p.value\n[1,]   0 -10.64    0.01\n[2,]   1  -7.98    0.01\n[3,]   2  -6.08    0.01\n[4,]   3  -5.41    0.01\n[5,]   4  -5.08    0.01\nType 3: with drift and trend \n     lag    ADF p.value\n[1,]   0 -10.58    0.01\n[2,]   1  -7.94    0.01\n[3,]   2  -6.06    0.01\n[4,]   3  -5.39    0.01\n[5,]   4  -5.07    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag   ADF p.value\n[1,]   0 -9.19    0.01\n[2,]   1 -6.65    0.01\n[3,]   2 -5.41    0.01\n[4,]   3 -5.33    0.01\n[5,]   4 -4.77    0.01\nType 2: with drift no trend \n     lag   ADF p.value\n[1,]   0 -9.14    0.01\n[2,]   1 -6.62    0.01\n[3,]   2 -5.39    0.01\n[4,]   3 -5.30    0.01\n[5,]   4 -4.75    0.01\nType 3: with drift and trend \n     lag   ADF p.value\n[1,]   0 -9.11    0.01\n[2,]   1 -6.59    0.01\n[3,]   2 -5.36    0.01\n[4,]   3 -5.29    0.01\n[5,]   4 -4.73    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \n\n\n[[1]]\nlogical(0)\n\n[[2]]\nlogical(0)\n\n[[3]]\nlogical(0)\n\nlapply(ts_list, pp_is_stationary)\n\nPhillips-Perron Unit Root Test \nalternative: stationary \n \nType 1: no drift no trend \n lag Z_rho p.value\n   3  -111    0.01\n----- \n Type 2: with drift no trend \n lag Z_rho p.value\n   3  -110    0.01\n----- \n Type 3: with drift and trend \n lag Z_rho p.value\n   3  -110    0.01\n--------------- \nNote: p-value = 0.01 means p.value <= 0.01 \nPhillips-Perron Unit Root Test \nalternative: stationary \n \nType 1: no drift no trend \n lag Z_rho p.value\n   3  -101    0.01\n----- \n Type 2: with drift no trend \n lag Z_rho p.value\n   3  -101    0.01\n----- \n Type 3: with drift and trend \n lag Z_rho p.value\n   3  -101    0.01\n--------------- \nNote: p-value = 0.01 means p.value <= 0.01 \nPhillips-Perron Unit Root Test \nalternative: stationary \n \nType 1: no drift no trend \n lag Z_rho p.value\n   3 -92.9    0.01\n----- \n Type 2: with drift no trend \n lag Z_rho p.value\n   3 -92.9    0.01\n----- \n Type 3: with drift and trend \n lag Z_rho p.value\n   3   -93    0.01\n--------------- \nNote: p-value = 0.01 means p.value <= 0.01 \n\n\n[[1]]\n[1] FALSE FALSE FALSE\n\n[[2]]\n[1] FALSE FALSE FALSE\n\n[[3]]\n[1] FALSE FALSE FALSE\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-24/index.html",
    "href": "posts/rtip-2023-01-24/index.html",
    "title": "Making Non Stationary Data Stationary",
    "section": "",
    "text": "Introduction\nIn the most basic sense for time series, a series is stationary if the properties of the generating process (the process that generates the data) do not change over time, the process remains constant. This does not mean the data does not change, it simply means the process does not change. You can bake a vanilla cake or a chocolate cake but you still cook it in the oven.\nA non-stationary time series is like a toy car that doesn’t run in a straight line. Sometimes it goes fast and sometimes it goes slow, so it’s hard to predict what it will do next. But, just like how you can fix a toy car by adjusting it, we can fix a non-stationary time series by making it “stationary.”\nOne way we can do this is by taking the difference in the time series vector. This is like taking the toy car apart and looking at how each piece moves. By subtracting one piece from another, we can see if they are moving at the same speed or not. If they are not, we can adjust them so they are moving at the same speed. This makes it easier to predict what the toy car will do next because it’s moving at a steady pace.\nAnother way we can make a non-stationary time series stationary is by taking the second difference of the log of the data. This is like looking at the toy car from a different angle. By taking the log of the data, we can see how much each piece has changed over time. Then, by taking the second difference, we can see if the changes are happening at the same rate or not. If they are not, we can adjust them so they are happening at the same rate.\nIn simple terms, these methods help to stabilize the time series by making the data move at a consistent speed, which allows for better predictions.\nIn summary, a non-stationary time series is like a toy car that doesn’t run in a straight line. By taking the difference in the time series vector or taking the second difference of the log of the data, we can fix the toy car and make it run in a straight line. This is helpful for making accurate predictions.\n\n\nFunction\nWe are going to use the adf.test() function from the {aTSA} library. Here is the function:\n\nadf.test(x, nlag = NULL, output = TRUE)\n\nHere are the arugments to the parameters.\n\nx- a numeric vector or time series.\nalternative - the lag order with default to calculate the test statistic. See details for the default.\noutput - a logical value indicating to print the test results in R console. The default is TRUE.\n\n\n\nExamples\nAs an example, we are going to use the R built in data set AirPassengers as our timeseries. This data is both cyclical and trending so it is good for this purpose.\n\nlibrary(aTSA)\n\nplot(AirPassengers)\n\n\n\n\nNow that we know what it looks like, lets see if it is stationary right off the bat.\n\nadf.test(AirPassengers)\n\nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag      ADF p.value\n[1,]   0  0.04712   0.657\n[2,]   1 -0.35240   0.542\n[3,]   2 -0.00582   0.641\n[4,]   3  0.26034   0.718\n[5,]   4  0.82238   0.879\nType 2: with drift no trend \n     lag    ADF p.value\n[1,]   0 -1.748   0.427\n[2,]   1 -2.345   0.194\n[3,]   2 -1.811   0.402\n[4,]   3 -1.536   0.509\n[5,]   4 -0.986   0.701\nType 3: with drift and trend \n     lag   ADF p.value\n[1,]   0 -4.64    0.01\n[2,]   1 -7.65    0.01\n[3,]   2 -7.09    0.01\n[4,]   3 -6.94    0.01\n[5,]   4 -5.95    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \n\n\nSo we can see that right off the bat that “Type 1” and “Type 2” fail as there is significant trend in this data as we can plainly see. Let’s see what happens when we take a simpmle diff() of the series.\n\nplot(diff(AirPassengers))\n\n\n\n\nLooking like its still going to fail, but let’s run the test anyways.\n\nadf.test(diff(AirPassengers))\n\nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag   ADF p.value\n[1,]   0 -8.58    0.01\n[2,]   1 -8.68    0.01\n[3,]   2 -8.13    0.01\n[4,]   3 -8.48    0.01\n[5,]   4 -6.59    0.01\nType 2: with drift no trend \n     lag   ADF p.value\n[1,]   0 -8.58    0.01\n[2,]   1 -8.69    0.01\n[3,]   2 -8.17    0.01\n[4,]   3 -8.60    0.01\n[5,]   4 -6.70    0.01\nType 3: with drift and trend \n     lag   ADF p.value\n[1,]   0 -8.55    0.01\n[2,]   1 -8.66    0.01\n[3,]   2 -8.14    0.01\n[4,]   3 -8.57    0.01\n[5,]   4 -6.69    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \n\n\nThe adf.test comes back with a p.value <= 0.01 as the data is no longer presenting a trend, but as we can plainly see, the data has non constant variance overtime which we know we need. Here we will use the {TidyDensity} package to use the cvar() (cumulative variance) function to see the ongoing variance.\n\nlibrary(TidyDensity)\n\nplot(cvar(diff(AirPassengers)), type = \"l\")\n\n\n\n\nReject the null that the data is stationary. So lets proceed with a diff diff log of the data and see what we get. First let’s visualize.\n\nplot(diff(diff(log(AirPassengers))))\n\n\n\nplot(cvar(diff(diff(log(AirPassengers)))), type = \"l\")\n\n\n\n\nLooking good!\n\nadf.test(diff(diff(log(AirPassengers))))\n\nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag    ADF p.value\n[1,]   0 -15.90    0.01\n[2,]   1 -12.78    0.01\n[3,]   2  -9.28    0.01\n[4,]   3 -10.76    0.01\n[5,]   4  -9.72    0.01\nType 2: with drift no trend \n     lag    ADF p.value\n[1,]   0 -15.85    0.01\n[2,]   1 -12.73    0.01\n[3,]   2  -9.24    0.01\n[4,]   3 -10.73    0.01\n[5,]   4  -9.68    0.01\nType 3: with drift and trend \n     lag    ADF p.value\n[1,]   0 -15.79    0.01\n[2,]   1 -12.68    0.01\n[3,]   2  -9.21    0.01\n[4,]   3 -10.68    0.01\n[5,]   4  -9.64    0.01\n---- \nNote: in fact, p.value = 0.01 means p.value <= 0.01 \n\n\nVoila!\n\n\nReferences\n\nhttps://towardsdatascience.com/stationarity-in-time-series-analysis-90c94f27322\nhttps://www.statology.org/dickey-fuller-test-in-r/"
  },
  {
    "objectID": "posts/rtip-2023-01-25/index.html",
    "href": "posts/rtip-2023-01-25/index.html",
    "title": "Simplifying List Filtering in R with purrr’s keep()",
    "section": "",
    "text": "Introduction\nThe {purrr} package in R is a powerful tool for working with lists and other data structures. One particularly useful function in the package is keep(), which allows you to filter a list by keeping only the elements that meet certain conditions.\nThe keep() function takes two arguments: the list to filter, and a function that returns a logical value indicating whether each element of the list should be kept. The function can be specified as an anonymous function or a named function, and it should take a single argument (the current element of the list).\nFor example, let’s say we have a list of numbers and we want to keep only the even numbers. We could use the keep() function with an anonymous function that checks the remainder of the current element divided by 2:\n\nlibrary(purrr)\n\nnumbers <- c(1, 2, 3, 4, 5, 6)\neven_numbers <- keep(numbers, function(x) x %% 2 == 0)\neven_numbers\n\n[1] 2 4 6\n\n\nWe see that this keeps [1] 2 4 6.\nThe purrr package also provides a convenient shorthand for this operation, .p, which can be used inside the keep function to return the element.\n\neven_numbers <- keep(numbers, ~ .x %% 2 == 0)\neven_numbers\n\n[1] 2 4 6\n\n\nYou can also use the keep() function to filter a list of other types of objects, such as strings or lists. For example, you could use it to keep only the strings that are longer than a certain length:\n\nwords <- c(\"cat\", \"dog\", \"elephant\", \"bird\")\nlong_words <- keep(words, function(x) nchar(x) > 4)\nlong_words\n\n[1] \"elephant\"\n\n\nWe see that this keeps “elephant” & “bird”.\nIn summary, the {purrr} package’s keep() function is a powerful tool for filtering lists in R, and the .p parameter can be used as a shorthand. It can be used to keep only the items in a list that meet a user-given condition, and it can be used with a variety of data types.\n\n\nFunction\nHere is the keep() function and it’s parameters.\n\nkeep(.x, .p, ...)\n\nHere are the arguments to the parameters.\n\n.x - A list or vector.\n.p - A predicate function (i.e. a function that returns either TRUE or FALSE) specified in one of the following ways:\n\nA named function, e.g. is.character.\nAn anonymous function, e.g. \\(x) all(x < 0) or function(x) all(x < 0).\nA formula, e.g. ~ all(.x < 0). You must use .x to refer to the first argument). Only recommended if you require backward compatibility with older versions of R.\n\n... - Additional arguments passed on to .p.\n\n\n\nExamples\nI recently came across wanting to filter a list that is given as an argument to a parameter. The function I am working for my upcoming {tidyAML} package has a function called create_workflow_set() that has a parameter .recipe_list which is set to list(). The user must only place recipes in this list or else I want it to fail. So I was able to write a quick check using keep() like so:\n\n# Checks ----\n# only keep() recipes\nrec_list <- purrr::keep(rec_list, ~ inherits(.x, \"recipe\"))\n\nVoila!"
  },
  {
    "objectID": "posts/tidydensity-20221007/index.html",
    "href": "posts/tidydensity-20221007/index.html",
    "title": "TidyDensity Primer",
    "section": "",
    "text": "This is going to serve as a sort of primer for the {TidyDensity} package.\nThe goal of {TidyDensity} is to make working with random numbers from different distributions easy. All tidy_ distribution functions provide the following components:\n\n[r_]\n[d_]\n[q_]\n[p_]\n\n\nInstallation\nYou can install the released version of {TidyDensity} from CRAN with:\ninstall.packages(\"TidyDensity\")\nAnd the development version from GitHub with:\n# install.packages(\"devtools\")\ndevtools::install_github(\"spsanderson/TidyDensity\")\n\n\nExample Data\nThis is a basic example which shows you how to solve a common problem, which is, how do we generate randomly generated data from a normal distribution of some mean, and some standard deviation with n points and sims number of simulations?\nWith the function tidy_normal() we can generate such data. All functions that are condsidered tidy_ distribution functions, meaning those that generate randomly generated data from some distribution, have the same API call structure.\nFor example, using tidy_normal() the full function call at it’s default is as follows:\ntidy_normal(.n = 50, .mean = 0, .sd = 1, .num_sims = 1).\nWhat this means is that we want to generate 50 points from a standard normal distribution of mean 0 and with a standard deviation of 1, and we want to generate a single simulation of this data.\nLet’s see an example below:\n\nsuppressPackageStartupMessages(library(TidyDensity))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(ggplot2))\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\nset.seed(123)\ntidy_normal()\n\n# A tibble: 50 × 7\n   sim_number     x       y    dx       dy     p       q\n   <fct>      <int>   <dbl> <dbl>    <dbl> <dbl>   <dbl>\n 1 1              1 -0.560  -3.11 0.000256 0.288 -0.560 \n 2 1              2 -0.230  -2.98 0.000691 0.409 -0.230 \n 3 1              3  1.56   -2.85 0.00167  0.940  1.56  \n 4 1              4  0.0705 -2.72 0.00362  0.528  0.0705\n 5 1              5  0.129  -2.59 0.00707  0.551  0.129 \n 6 1              6  1.72   -2.45 0.0125   0.957  1.72  \n 7 1              7  0.461  -2.32 0.0201   0.678  0.461 \n 8 1              8 -1.27   -2.19 0.0298   0.103 -1.27  \n 9 1              9 -0.687  -2.06 0.0415   0.246 -0.687 \n10 1             10 -0.446  -1.93 0.0552   0.328 -0.446 \n# … with 40 more rows\n\n\nWhat comes back we see is a tibble. This is true for all functions in the {TidyDensity} library. It was a goal to return items that are consistent with the tidyverse.\nNow let’s talk a bit about what was actually returned. There are a few columns that are returned, these are referred to as the r, d, p, and q\n\n[r_] Shows as y and is the randomly generated data from the underlying distribution.\n[d_] Two components come back, dx and dy where these are generated from the [stats::density()] function with n set to .n from the function input.\n[p_] Shows as p and is the results of the p_ function, in this case pnorm() where the x of the input goes from 0-1 with .n points.\n[q_] Shows as q and is the results of the q_ function, in this case qnorm() where the x of the input goes from 0-1 with .n points.\n\nNow you will also see two more columns, namely, sim_number a factor column and x an integer column. The sim_number column represents the current simulation for which data was drawn, and the x represents the nth point in that simulation.\n\n\nVisualization Example\nWith data typically comes the need to see it! Show me the data! TidyDensity has a variety of autoplot functionality that will present only data from a tidy_ distribution function. We will take a look at output from tidy_normal() and set a see otherwise everytime this site is rendered the data would change.\n\nset.seed(123)\ntn <- tidy_normal(.n = 100, .num_sims = 6)\ntidy_autoplot(tn, .plot_type = \"density\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"quantile\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"probability\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"qq\")\n\n\n\n\nWe can see that the plots are faily informative. There are the regular density plot, the quantile plot, probability and qq plots. The title and subtitle of these plots are generated from attributes that are attached to the output of the tidy_ distribution function. Let’s take a look at the attributes of tn\n\nattributes(tn)\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n[163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n[181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n[199] 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n[217] 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234\n[235] 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252\n[253] 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270\n[271] 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288\n[289] 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306\n[307] 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324\n[325] 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342\n[343] 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360\n[361] 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378\n[379] 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396\n[397] 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414\n[415] 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432\n[433] 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450\n[451] 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468\n[469] 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486\n[487] 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504\n[505] 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522\n[523] 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540\n[541] 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558\n[559] 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576\n[577] 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594\n[595] 595 596 597 598 599 600\n\n$names\n[1] \"sim_number\" \"x\"          \"y\"          \"dx\"         \"dy\"        \n[6] \"p\"          \"q\"         \n\n$distribution_family_type\n[1] \"continuous\"\n\n$.mean\n[1] 0\n\n$.sd\n[1] 1\n\n$.n\n[1] 100\n\n$.num_sims\n[1] 6\n\n$tibble_type\n[1] \"tidy_gaussian\"\n\n$ps\n  [1] 0.00000000 0.01010101 0.02020202 0.03030303 0.04040404 0.05050505\n  [7] 0.06060606 0.07070707 0.08080808 0.09090909 0.10101010 0.11111111\n [13] 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717\n [19] 0.18181818 0.19191919 0.20202020 0.21212121 0.22222222 0.23232323\n [25] 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929\n [31] 0.30303030 0.31313131 0.32323232 0.33333333 0.34343434 0.35353535\n [37] 0.36363636 0.37373737 0.38383838 0.39393939 0.40404040 0.41414141\n [43] 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747\n [49] 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354\n [55] 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.59595960\n [61] 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566\n [67] 0.66666667 0.67676768 0.68686869 0.69696970 0.70707071 0.71717172\n [73] 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778\n [79] 0.78787879 0.79797980 0.80808081 0.81818182 0.82828283 0.83838384\n [85] 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.89898990\n [91] 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596\n [97] 0.96969697 0.97979798 0.98989899 1.00000000\n\n$qs\n  [1] 0.00000000 0.01010101 0.02020202 0.03030303 0.04040404 0.05050505\n  [7] 0.06060606 0.07070707 0.08080808 0.09090909 0.10101010 0.11111111\n [13] 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717\n [19] 0.18181818 0.19191919 0.20202020 0.21212121 0.22222222 0.23232323\n [25] 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929\n [31] 0.30303030 0.31313131 0.32323232 0.33333333 0.34343434 0.35353535\n [37] 0.36363636 0.37373737 0.38383838 0.39393939 0.40404040 0.41414141\n [43] 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747\n [49] 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354\n [55] 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.59595960\n [61] 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566\n [67] 0.66666667 0.67676768 0.68686869 0.69696970 0.70707071 0.71717172\n [73] 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778\n [79] 0.78787879 0.79797980 0.80808081 0.81818182 0.82828283 0.83838384\n [85] 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.89898990\n [91] 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596\n [97] 0.96969697 0.97979798 0.98989899 1.00000000\n\n$param_grid\n# A tibble: 1 × 2\n  .mean   .sd\n  <dbl> <dbl>\n1     0     1\n\n$param_grid_txt\n[1] \"c(0, 1)\"\n\n$dist_with_params\n[1] \"Gaussian c(0, 1)\"\n\n\nI won’t go over them but as you can see, the attribute list can get long and has a lot of great information in it.\nNow what if we have simulations over 9? The legend would get fairly large making the visualization difficult to read.\nLet’s take a look at 20 simulations.\n\ntn <- tidy_normal(.n = 100, .num_sims = 20)\ntidy_autoplot(tn, .plot_type = \"density\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"quantile\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"probability\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"qq\")\n\n\n\n\nWe see that the legend disappears! That’s great, but what if we still want to see what simulation is what? Well, make the plot interactive!\n\ntidy_autoplot(tn, .interactive = TRUE)"
  },
  {
    "objectID": "posts/weekly-rtip-2022-12-23/index.html",
    "href": "posts/weekly-rtip-2022-12-23/index.html",
    "title": "Simulating Time Series Model Forecasts with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nTime series models are a powerful tool for forecasting future values of a time-dependent variable. These models are commonly used in a variety of fields, including finance, economics, and engineering, to predict future outcomes based on past data.\nOne important aspect of time series modeling is the ability to simulate model forecasts. This allows us to evaluate the performance of different forecasting methods and to compare the results of different models. Simulating forecasts also allows us to assess the uncertainty associated with our predictions, which can be especially useful when making important decisions based on the forecast.\nThere are several benefits to simulating time series model forecasts:\n\nImproved accuracy: By simulating forecasts, we can identify the best forecasting method for a given time series and optimize its parameters. This can lead to more accurate forecasts, especially for long-term predictions.\nEnhanced understanding: Simulating forecasts helps us to understand how different factors, such as seasonality and trend, affect the prediction. This understanding can inform our decision-making and allow us to make more informed predictions.\nImproved communication: Simulating forecasts allows us to present the uncertainty associated with our predictions, which can be useful for communicating the potential risks and benefits of different courses of action.\n\nThe R package {healthyR.ts} includes a function called ts_forecast_simulator() that can be used to simulate time series model forecasts. This function allows users to specify the forecasting method, the number of simulations to run, and the length of the forecast horizon. It also provides options for visualizing the results, including plots of the forecast distribution and summary statistics such as the mean and standard deviation of the forecasts.\nIn summary, simulating time series model forecasts is a valuable tool for improving the accuracy and understanding of our predictions, as well as for communicating the uncertainty associated with these forecasts. The ts_forecast_simulator() function in the {healthyR.ts} package is a useful tool for performing these simulations in R.\n\n\nFunction\nLet’s take a look at the full function call.\n\nts_forecast_simulator(\n  .model,\n  .data,\n  .ext_reg = NULL,\n  .frequency = NULL,\n  .bootstrap = TRUE,\n  .horizon = 4,\n  .iterations = 25,\n  .sim_color = \"steelblue\",\n  .alpha = 0.05\n)\n\nNow let’s take a look at the arguments that get provided to the parameters.\n\n.model - A forecasting model of one of the following from the forecast package:\n\nArima\nauto.arima\nets\nnnetar\nArima() with xreg\n\n.data - The data that is used for the .model parameter. This is used with timetk::tk_index()\n.ext_reg - A tibble or matrix of future xregs that should be the same length as the horizon you want to forecast.\n.frequency - This is for the conversion of an internal table and should match the time frequency of the data.\n.bootstrap - A boolean value of TRUE/FALSE. From forecast::simulate.Arima() Do simulation using resampled errors rather than normally distributed errors.\n.horizon - An integer defining the forecast horizon.\n.iterations - An integer, set the number of iterations of the simulation.\n.sim_color - Set the color of the simulation paths lines.\n.alpha - Set the opacity level of the simulation path lines.\n\nGreat, now let’s take a look at examples.\n\n\nExamples\n\nlibrary(healthyR.ts)\nlibrary(forecast)\n\nfit <- auto.arima(AirPassengers)\ndata_tbl <- ts_to_tbl(AirPassengers)\n\n# Simulate 50 possible forecast paths, with .horizon of 12 months\noutput <- ts_forecast_simulator(\n  .model        = fit\n  , .horizon    = 12\n  , .iterations = 50\n  , .data       = data_tbl\n)\n\nOk, so now we have our output object, which is a list object. Let’s see what it contains.\n\n\n\nForecast Simulation Output\n\n\nNow, let’s explore each element.\nFirst the forecast simulation data.\n\noutput$forecast_sim\n\n            sim_1    sim_2    sim_3    sim_4    sim_5    sim_6    sim_7\nJan 1961 445.7399 434.7175 462.6173 447.8849 453.5069 443.0944 464.4749\nFeb 1961 424.1606 423.3814 431.9512 420.4460 426.8245 423.4784 443.1545\nMar 1961 444.0015 467.6276 459.3452 450.2936 453.6116 461.3751 462.4351\nApr 1961 490.2370 504.9129 492.0632 491.2198 494.6883 489.6674 498.7902\nMay 1961 502.0907 517.7794 504.5420 503.7614 504.8544 504.1816 521.9044\nJun 1961 552.6359 588.4650 560.1199 567.5266 563.5353 552.8411 543.5723\nJul 1961 655.1872 666.8450 642.9804 653.4262 646.1559 648.1923 639.6587\nAug 1961 633.4657 635.5080 644.1405 650.1333 631.2688 630.4410 623.3784\nSep 1961 536.3259 551.6068 548.9176 554.8289 533.3672 553.5334 529.2182\nOct 1961 486.2815 513.2851 514.8842 513.2190 481.4636 495.1295 490.7402\nNov 1961 406.9061 428.0550 437.2211 435.3443 426.4892 425.9885 416.1940\nDec 1961 454.1048 478.1804 477.3923 475.0052 504.0761 486.3584 472.2933\n            sim_8    sim_9   sim_10   sim_11   sim_12   sim_13   sim_14\nJan 1961 444.3152 444.3203 453.5722 438.0253 438.0253 425.2956 458.6829\nFeb 1961 418.1653 418.2569 423.9229 413.1814 404.0688 403.3318 445.9250\nMar 1961 450.8166 437.2694 452.5463 433.7963 450.5365 433.6445 448.2279\nApr 1961 489.4801 501.5545 504.4428 488.2243 503.9232 476.7711 495.1634\nMay 1961 499.7523 499.2059 508.6748 496.3915 520.8837 489.1480 502.1013\nJun 1961 562.7791 574.2660 571.6261 560.3126 578.8797 564.5855 581.8097\nJul 1961 653.2719 655.1924 655.0359 647.2704 664.1505 660.7023 654.6549\nAug 1961 649.6533 639.4611 655.1187 610.5014 661.2034 606.4399 654.0751\nSep 1961 542.5110 548.6498 560.1903 505.3303 552.4413 515.2890 547.9581\nOct 1961 505.5032 490.4686 495.1026 464.1546 503.7735 479.0465 501.3405\nNov 1961 423.0761 414.9520 445.9182 399.1634 435.6692 405.7617 447.0409\nDec 1961 454.4311 452.3740 473.5319 435.0425 468.3699 449.6422 474.2666\n           sim_15   sim_16   sim_17   sim_18   sim_19   sim_20   sim_21\nJan 1961 455.4787 451.0585 459.4839 444.3242 435.1001 443.7523 444.9274\nFeb 1961 439.4881 422.0447 442.7488 427.5273 412.6110 430.0844 418.5412\nMar 1961 473.0626 449.9922 479.0381 454.4156 443.6824 454.0246 455.8716\nApr 1961 497.0203 501.5911 517.4542 495.3593 476.1640 489.3426 492.7721\nMay 1961 515.2215 526.5615 506.5715 487.1920 504.6323 500.4116 506.1908\nJun 1961 576.5544 581.4492 560.9466 560.1496 557.4915 551.0780 580.6906\nJul 1961 662.4222 675.8571 643.9878 645.8877 634.9354 649.0339 655.5332\nAug 1961 648.2923 653.8785 639.1892 624.4315 643.3871 630.6072 638.5270\nSep 1961 548.8370 547.7029 548.0178 529.7305 552.4617 533.3734 541.2910\nOct 1961 500.8130 500.6410 509.7552 481.4202 500.3374 484.8628 513.4386\nNov 1961 448.3088 427.7597 434.1320 424.8617 468.9838 421.0139 460.0366\nDec 1961 479.2161 468.6392 473.0734 460.6548 492.5949 445.3285 483.7701\n           sim_22   sim_23   sim_24   sim_25   sim_26   sim_27   sim_28\nJan 1961 431.9061 436.9803 445.6030 470.3241 438.1303 438.0355 442.8814\nFeb 1961 405.5096 406.4255 431.2780 451.1454 438.4957 414.3992 406.9653\nMar 1961 437.8384 437.3900 455.2998 472.2864 457.3227 452.7172 439.0830\nApr 1961 480.2212 479.1317 491.4019 520.8334 492.1962 497.7494 495.6963\nMay 1961 507.5101 491.4699 502.3217 522.6236 504.1139 498.8398 490.4093\nJun 1961 565.3556 555.2957 562.9435 569.6062 575.3107 565.8427 558.4052\nJul 1961 631.8589 643.0705 649.3241 656.5773 699.1111 660.5835 656.7441\nAug 1961 636.9621 620.3416 635.3089 658.5513 666.7588 655.0455 634.5556\nSep 1961 533.4397 546.0061 537.0957 552.6849 578.5525 563.2023 557.6656\nOct 1961 493.0531 483.6816 489.7577 517.3445 535.8427 509.2783 506.7000\nNov 1961 421.6430 429.6623 419.7854 427.1834 456.2713 429.0018 434.0667\nDec 1961 452.2728 457.5337 460.9375 470.2428 514.2062 482.2928 490.3966\n           sim_29   sim_30   sim_31   sim_32   sim_33   sim_34   sim_35\nJan 1961 455.3775 444.3272 461.2236 433.8962 464.4749 438.0355 439.1887\nFeb 1961 426.9441 418.2812 447.4852 424.0317 409.7277 415.1395 433.4188\nMar 1961 457.7136 440.5773 468.7090 447.5754 438.9636 443.0306 420.3961\nApr 1961 505.5824 483.8879 508.5020 490.2527 475.6410 497.7693 480.1376\nMay 1961 502.1128 495.2827 526.9527 488.3082 510.2163 524.8456 504.3970\nJun 1961 564.2560 565.1836 571.4699 554.6782 548.1972 582.0468 557.3177\nJul 1961 675.1975 656.7991 674.3476 643.9736 631.1844 665.9546 650.1157\nAug 1961 642.3301 604.3293 649.4941 612.8896 632.0786 640.5119 626.3405\nSep 1961 546.0228 520.2840 550.9040 533.3240 544.4996 540.7844 555.0268\nOct 1961 517.2212 480.8524 494.4332 467.0373 505.1177 495.0939 511.8599\nNov 1961 436.9353 409.0263 423.5813 407.0598 433.9868 420.4208 435.3652\nDec 1961 477.7085 444.2295 463.2785 447.8678 471.4819 477.0888 486.3902\n           sim_36   sim_37   sim_38   sim_39   sim_40   sim_41   sim_42\nJan 1961 458.2243 453.5069 442.9633 437.7823 456.7181 439.1887 444.2746\nFeb 1961 437.0087 430.5385 443.4436 434.5281 441.0503 415.1167 410.6375\nMar 1961 450.4477 456.5806 462.1142 454.3194 453.8004 433.9958 450.8734\nApr 1961 507.2234 499.3962 524.0761 495.3350 498.6223 465.6067 489.4286\nMay 1961 521.1416 518.5159 541.6983 514.5651 504.7369 474.7817 504.5533\nJun 1961 579.8129 580.3206 598.3710 558.8196 564.7209 544.2263 569.3150\nJul 1961 685.2838 663.2947 685.4005 652.3353 664.4497 622.2188 654.6807\nAug 1961 671.5436 671.9516 655.8918 630.8036 628.7970 615.9232 637.1259\nSep 1961 566.4307 567.5127 576.3038 539.0021 523.1752 530.4702 525.4079\nOct 1961 506.4949 510.5275 513.7364 488.4346 483.6546 479.5688 482.5080\nNov 1961 455.9362 453.4418 460.7156 417.2549 413.5535 408.8823 397.0385\nDec 1961 487.4205 481.4040 488.4877 455.6214 481.9331 453.4355 448.3142\n           sim_43   sim_44   sim_45   sim_46   sim_47   sim_48   sim_49\nJan 1961 456.7181 448.9127 444.1944 439.1887 444.3762 443.9540 449.6957\nFeb 1961 415.5481 417.4993 432.0998 418.8621 411.7694 419.4677 432.5796\nMar 1961 441.1190 446.8376 453.2327 448.5254 448.2663 470.9662 455.3371\nApr 1961 484.0184 474.0491 510.7521 487.2088 508.1952 505.5636 510.9074\nMay 1961 474.5845 493.5429 520.5754 493.9236 521.0746 494.3940 527.4129\nJun 1961 540.3995 553.2761 571.1053 570.3663 579.1209 553.5803 571.8349\nJul 1961 649.1646 657.5268 653.2605 657.5574 676.0876 607.8539 654.0949\nAug 1961 643.3467 635.9533 636.3980 656.9203 673.6242 590.6108 632.1774\nSep 1961 539.6460 532.2568 541.7711 546.2181 564.7882 492.5588 501.7329\nOct 1961 490.9799 483.5566 507.1510 500.1024 505.8764 471.2807 448.5220\nNov 1961 412.2859 415.6316 428.0756 413.5517 434.7768 384.6290 382.9796\nDec 1961 461.1189 422.9218 457.3822 459.4905 484.2661 434.8731 429.3802\n           sim_50\nJan 1961 442.9633\nFeb 1961 382.3990\nMar 1961 411.8363\nApr 1961 459.1659\nMay 1961 458.3228\nJun 1961 524.1910\nJul 1961 615.5164\nAug 1961 614.8716\nSep 1961 506.6345\nOct 1961 444.9304\nNov 1961 383.3882\nDec 1961 426.0341\n\noutput$forecast_sim_tbl\n\n# A tibble: 600 × 4\n       x     y n        id\n   <dbl> <dbl> <chr> <int>\n 1 1961.  446. sim_1     1\n 2 1961.  424. sim_1     2\n 3 1961.  444. sim_1     3\n 4 1961.  490. sim_1     4\n 5 1961.  502. sim_1     5\n 6 1961.  553. sim_1     6\n 7 1962.  655. sim_1     7\n 8 1962.  633. sim_1     8\n 9 1962.  536. sim_1     9\n10 1962.  486. sim_1    10\n# … with 590 more rows\n\n\nThe time series that was used.\n\noutput$time_series\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\n\nThe fitted values in two different formats\n\noutput$fitted_values\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1949 111.9353 117.9664 131.9662 128.9774 120.9892 134.9782 147.9692 147.9731\n1950 115.4270 121.3807 138.5312 137.2522 127.5180 139.7865 158.9159 166.3207\n1951 132.8130 151.5147 164.7662 165.5934 153.1413 186.9835 201.0564 197.1598\n1952 171.2150 174.9573 205.3873 181.9983 189.5111 191.9692 229.3659 230.2887\n1953 197.4932 205.0669 211.9492 214.7030 229.4790 261.9978 259.3789 272.3394\n1954 205.8921 206.1940 236.4993 236.1932 227.3492 247.7618 281.4459 303.3972\n1955 229.7559 221.0431 274.5806 260.1968 270.5411 299.1164 345.0987 346.2332\n1956 283.9172 273.9637 307.7212 313.9008 312.5977 358.8094 415.5070 394.8641\n1957 313.6903 307.2137 343.5753 347.2471 353.0923 409.4687 455.6903 452.6105\n1958 346.2681 328.3116 377.2767 360.7205 361.5534 432.0632 479.8147 490.8438\n1959 346.5194 335.2349 385.8185 384.9676 406.8419 485.3013 531.0698 553.0149\n1960 418.4120 397.2579 454.0138 420.2105 468.2327 523.6974 603.6761 623.9211\n          Sep      Oct      Nov      Dec\n1949 135.9874 119.0049 104.0187 118.0778\n1950 156.2023 139.1631 119.1047 128.9974\n1951 185.1990 158.4085 140.6879 169.2312\n1952 220.7255 190.3056 172.8446 191.9035\n1953 238.8716 218.7628 194.3686 207.1910\n1954 261.6971 232.5912 199.3564 222.5606\n1955 309.8394 277.5670 246.5073 264.0691\n1956 363.2702 318.0959 271.0990 310.9291\n1957 409.6920 354.9782 312.2741 341.2514\n1958 438.0323 360.3301 317.3131 346.5759\n1959 454.6235 412.1130 357.5358 384.6475\n1960 513.8591 450.7760 410.8955 439.9468\n\noutput$fitted_values_tbl\n\n# A tibble: 144 × 2\n   index     value\n   <yearmon> <dbl>\n 1 Jan 1949   112.\n 2 Feb 1949   118.\n 3 Mar 1949   132.\n 4 Apr 1949   129.\n 5 May 1949   121.\n 6 Jun 1949   135.\n 7 Jul 1949   148.\n 8 Aug 1949   148.\n 9 Sep 1949   136.\n10 Oct 1949   119.\n# … with 134 more rows\n\n\nThe residual values in two different formats\n\noutput$residual_values\n\n               Jan           Feb           Mar           Apr           May\n1949   0.064663218   0.033565844   0.033806149   0.022551853   0.010753877\n1950  -0.426993657   4.619296276   2.468817592  -2.252222539  -2.517970381\n1951  12.187004737  -1.514734464  13.233788623  -2.593406722  18.858703025\n1952  -0.215049450   5.042657391 -12.387298452  -0.998279160  -6.511066526\n1953  -1.493243603  -9.066863638  24.050789583  20.296981218  -0.479038408\n1954  -1.892145409 -18.194023466  -1.499295070  -9.193240870   6.650775967\n1955  12.244087324  11.956865808  -7.580632008   8.803192574  -0.541058565\n1956   0.082775370   3.036286487   9.278782426  -0.900756132   5.402300489\n1957   1.309660582  -6.213658164  12.424731881   0.752860797   1.907693986\n1958  -6.268081829 -10.311568418 -15.276674427 -12.720532082   1.446551672\n1959  13.480639348   6.765147100  20.181485627  11.032373098  13.158068772\n1960  -1.412019863  -6.257914445 -35.013829003  40.789527421   3.767286916\n               Jun           Jul           Aug           Sep           Oct\n1949   0.021825883   0.030792890   0.026927580   0.012574909  -0.004856125\n1950   9.213518228  11.084130208   3.679258769   1.797714396  -6.163078961\n1951  -8.983477860  -2.056432959   1.840247841  -1.199018264   3.591516610\n1952  26.030752750   0.634055619  11.711292178 -11.725472510   0.694363698\n1953 -18.997830355   4.621111100  -0.339432546  -1.871647726  -7.762821892\n1954  16.238163730  20.554095186 -10.397216564  -2.697121208  -3.591173672\n1955  15.883576851  18.901348128   0.766758698   2.160561460  -3.567021842\n1956  15.190550278  -2.507027255  10.135908860  -8.270245332 -12.095862166\n1957  12.531339373   9.309735715  14.389487605  -5.692026179  -7.978198549\n1958   2.936791273  11.185298228  14.156225803 -34.032306461  -1.330101426\n1959 -13.301336524  16.930226311   5.985059029   8.376509562  -5.112953069\n1960  11.302583143  18.323916561 -17.921058274  -5.859106651  10.223989361\n               Nov           Dec\n1949  -0.018746667  -0.077775679\n1950  -5.104668785  11.002554045\n1951   5.312079856  -3.231170377\n1952  -0.844622054   2.096450492\n1953 -14.368618246  -6.190983141\n1954   3.643587684   6.439397882\n1955  -9.507327199  13.930943896\n1956  -0.099013624  -4.929146809\n1957  -7.274091927  -5.251369244\n1958  -7.313133943  -9.575869505\n1959   4.464243872  20.352533059\n1960 -20.895479201  -7.946822359\n\noutput$residual_values_tbl\n\n# A tibble: 144 × 2\n   index        value\n   <yearmon>    <dbl>\n 1 Jan 1949   0.0647 \n 2 Feb 1949   0.0336 \n 3 Mar 1949   0.0338 \n 4 Apr 1949   0.0226 \n 5 May 1949   0.0108 \n 6 Jun 1949   0.0218 \n 7 Jul 1949   0.0308 \n 8 Aug 1949   0.0269 \n 9 Sep 1949   0.0126 \n10 Oct 1949  -0.00486\n# … with 134 more rows\n\n\nThe input data itself\n\noutput$input_data\n\n# A tibble: 144 × 2\n   index     value\n   <yearmon> <dbl>\n 1 Jan 1949    112\n 2 Feb 1949    118\n 3 Mar 1949    132\n 4 Apr 1949    129\n 5 May 1949    121\n 6 Jun 1949    135\n 7 Jul 1949    148\n 8 Aug 1949    148\n 9 Sep 1949    136\n10 Oct 1949    119\n# … with 134 more rows\n\n\nThe time series simulations\n\noutput$sim_ts_tbl\n\n# A tibble: 600 × 5\n   index         x     y n        id\n   <yearmon> <dbl> <dbl> <chr> <int>\n 1 Jan 1961  1961.  446. sim_1     1\n 2 Feb 1961  1961.  424. sim_1     2\n 3 Mar 1961  1961.  444. sim_1     3\n 4 Apr 1961  1961.  490. sim_1     4\n 5 May 1961  1961.  502. sim_1     5\n 6 Jun 1961  1961.  553. sim_1     6\n 7 Jul 1961  1962.  655. sim_1     7\n 8 Aug 1961  1962.  633. sim_1     8\n 9 Sep 1961  1962.  536. sim_1     9\n10 Oct 1961  1962.  486. sim_1    10\n# … with 590 more rows\n\n\nNow, the visuals, first the static ggplot\n\noutput$ggplot\n\n\n\n\nThe interactive plotly plot.\n\noutput$plotly_plot\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-2023-01-06/index.html",
    "href": "posts/weekly-rtip-2023-01-06/index.html",
    "title": "Brownian Motion",
    "section": "",
    "text": "Introduction\nBrownian motion, also known as the random motion of particles suspended in a fluid, is a phenomenon that was first described by Scottish botanist Robert Brown in 1827. It occurs when a particle is subjected to a series of random collisions with the molecules in the fluid.\nThe motion of the particle can be described mathematically using the following equation:\n\\[ \\frac{dx_t}{dt} = \\mu + \\sigma \\cdot W_t \\]\nWhere \\(x_t\\) represents the position of the particle at time t, \\(\\mu\\) is the drift coefficient, \\(\\sigma\\) is the diffusion coefficient, and \\(W_t\\) is a Wiener process (a type of random process).\nBrownian motion has a number of important applications, including in the field of finance. It is used to model the random movements of financial assets, such as stocks, over time. It can also be used to estimate the volatility of an asset, as well as to calculate the prices of financial derivatives such as options.\nIn physics, Brownian motion is used to study the behavior of small particles suspended in a fluid, as well as to understand the properties of fluids at the molecular level. It has also been used to study the motion of biological molecules, such as proteins, within cells.\nOverall, Brownian motion is a fundamental concept that has wide-ranging applications in a variety of fields, including finance, physics, and biology.\n\n\nFunction\nLet’s take a look at a function to produce such results. This type of functionality will be coming to my R packages {TidyDensity} and to {healthyR.ts}\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\nlibrary(forcats)\n\nbrownian_motion <- function(T, N, delta_t) {\n  # T: total time of the simulation (in seconds)\n  # N: number of simulations to generate\n  # delta_t: time step size (in seconds)\n  \n  # Initialize empty data.frame to store the simulations\n  sim_data <- data.frame()\n  \n  # Generate N simulations\n  for (i in 1:N) {\n    # Initialize the current simulation with a starting value of 0\n    sim <- c(0)\n    \n    # Generate the brownian motion values for each time step\n    for (t in 1:(T / delta_t)) {\n      sim <- c(sim, sim[t] + rnorm(1, mean = 0, sd = sqrt(delta_t)))\n    }\n    \n    # Bind the time steps, simulation values, and simulation number together\n    # in a data.frame and add it to the result\n    sim_data <- rbind(\n      sim_data, \n      data.frame(\n        t = seq(0, T, delta_t), \n        y = sim, \n        sim_number = i\n        )\n      ) %>%\n      as_tibble()\n  }\n  \n  sim_data <- sim_data %>%\n    mutate(sim_number = as_factor(sim_number)\n                  )\n  return(sim_data)\n}\n\nWe see that the internal variable sim is set to 0, this in the future will be set to an initial value that a user can provide.\n\n\nExamples\nLet’s take a look at a couple of examples.\n\nbrownian_motion(40, 25, .2) %>%\n  ggplot(aes(x = t, y = y, group = sim_number, color = sim_number)) +\n  geom_line() +\n  geom_smooth(se = FALSE, \n              aes(group = FALSE), \n              color = \"black\", \n              linetype = \"dashed\") +\n  theme_minimal() +\n  labs(\n    x = \"Time t\",\n    y = \"Brownian Motion Value y\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nNow lets take a look at the change in a few different ones at the same time.\n\nbm_tbl <- rbind(\n  brownian_motion(40, 25, .2) %>%\n    mutate(label = \"20% Volatility\"),\n  brownian_motion(40, 25, .1) %>%\n    mutate(label = \"10% Volatility\"),\n  brownian_motion(40, 25, .05) %>%\n    mutate(label = \"5% Volatility\"),\n  brownian_motion(40, 25, .025) %>%\n    mutate(label = \"2.5% Volatility\")\n)\n\nggplot(bm_tbl, aes(x = t, y = y, group = sim_number, color = sim_number)) +\n  geom_line() +\n  facet_wrap(~ label, scales = \"free\") +\n    geom_smooth(se = FALSE, \n              aes(group = FALSE), \n              color = \"black\", \n              linetype = \"dashed\") +\n  theme_minimal() +\n    labs(\n    x = \"Time t\",\n    y = \"Brownian Motion Value y\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html",
    "title": "PCA with healthyR.ai",
    "section": "",
    "text": "In this post we are going to talk about how you can perform principal component analysis in R with {healthyR.ai} in a tidyverse compliant fashion.\nThe specific function we are going to discuss on this post is pca_your_recipe()"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#library-load",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#library-load",
    "title": "PCA with healthyR.ai",
    "section": "Library Load",
    "text": "Library Load\n\npacman::p_load(\n  \"healthyR.ai\",\n  \"healthyR.data\",\n  \"timetk\",\n  \"dplyr\",\n  \"purrr\",\n  \"rsample\",\n  \"recipes\"\n)\n\nNow that we have our libraries loaded lets get the data."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#data",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#data",
    "title": "PCA with healthyR.ai",
    "section": "Data",
    "text": "Data\n\ndata_tbl <- healthyR_data %>%\n  select(visit_end_date_time) %>%\n  summarise_by_time(\n    .date_var = visit_end_date_time,\n    .by       = \"month\",\n    value     = n()\n  ) %>%\n  set_names(\"date_col\", \"value\") %>%\n  filter_by_time(\n    .date_var = date_col,\n    .start_date = \"2013\",\n    .end_date = \"2020\"\n  )\n\nhead(data_tbl, 5)\n\n# A tibble: 5 × 2\n  date_col            value\n  <dttm>              <int>\n1 2013-01-01 00:00:00  2082\n2 2013-02-01 00:00:00  1719\n3 2013-03-01 00:00:00  1796\n4 2013-04-01 00:00:00  1865\n5 2013-05-01 00:00:00  2028\n\n\nNow for the splits object."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#splits",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#splits",
    "title": "PCA with healthyR.ai",
    "section": "Splits",
    "text": "Splits\n\nsplits <- initial_split(data = data_tbl, prop = 0.8)\n\nsplits\n\n<Training/Testing/Total>\n<76/19/95>"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#recipe-and-output",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#recipe-and-output",
    "title": "PCA with healthyR.ai",
    "section": "Recipe and Output",
    "text": "Recipe and Output\nNow it is time for the recipe and the output objects.\n\nrec_obj <- recipe(value ~ ., training(splits)) %>%\n  step_timeseries_signature(date_col) %>%\n  step_rm(matches(\"(iso$)|(xts$)|(hour)|(min)|(sec)|(am.pm)\"))\n\noutput_list <- pca_your_recipe(rec_obj, .data = data_tbl)"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-transform",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-transform",
    "title": "PCA with healthyR.ai",
    "section": "PCA Transform",
    "text": "PCA Transform\n\noutput_list$pca_transform\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          1\n\nOperations:\n\nTimeseries signature features from date_col\nVariables removed matches(\"(iso$)|(xts$)|(hour)|(min)|(sec)|(am.pm)\")\nCentering for recipes::all_numeric()\nScaling for recipes::all_numeric()\nSparse, unbalanced variable filter on recipes::all_numeric()\nPCA extraction with recipes::all_numeric_predictors()"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#variable-loadings",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#variable-loadings",
    "title": "PCA with healthyR.ai",
    "section": "Variable Loadings",
    "text": "Variable Loadings\n\noutput_list$variable_loadings\n\n# A tibble: 169 × 4\n   terms                 value component id       \n   <chr>                 <dbl> <chr>     <chr>    \n 1 date_col_index.num -0.00137 PC1       pca_bVc37\n 2 date_col_year       0.0529  PC1       pca_bVc37\n 3 date_col_half      -0.385   PC1       pca_bVc37\n 4 date_col_quarter   -0.434   PC1       pca_bVc37\n 5 date_col_month     -0.437   PC1       pca_bVc37\n 6 date_col_wday      -0.0159  PC1       pca_bVc37\n 7 date_col_qday      -0.0608  PC1       pca_bVc37\n 8 date_col_yday      -0.437   PC1       pca_bVc37\n 9 date_col_mweek      0.0537  PC1       pca_bVc37\n10 date_col_week      -0.438   PC1       pca_bVc37\n# … with 159 more rows"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#variable-variance",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#variable-variance",
    "title": "PCA with healthyR.ai",
    "section": "Variable Variance",
    "text": "Variable Variance\n\noutput_list$variable_variance\n\n# A tibble: 52 × 4\n   terms       value component id       \n   <chr>       <dbl>     <int> <chr>    \n 1 variance 5.14             1 pca_bVc37\n 2 variance 2.08             2 pca_bVc37\n 3 variance 1.47             3 pca_bVc37\n 4 variance 1.40             4 pca_bVc37\n 5 variance 1.07             5 pca_bVc37\n 6 variance 0.684            6 pca_bVc37\n 7 variance 0.583            7 pca_bVc37\n 8 variance 0.519            8 pca_bVc37\n 9 variance 0.0534           9 pca_bVc37\n10 variance 0.000231        10 pca_bVc37\n# … with 42 more rows"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-estimates",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-estimates",
    "title": "PCA with healthyR.ai",
    "section": "PCA Estimates",
    "text": "PCA Estimates\n\noutput_list$pca_estimates\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          1\n\nTraining data contained 76 data points and no missing data.\n\nOperations:\n\nTimeseries signature features from date_col [trained]\nVariables removed date_col_year.iso, date_col_month.xts, date_col_hour, d... [trained]\nCentering for value, date_col_index.num, date_col_year, date_... [trained]\nScaling for value, date_col_index.num, date_col_year, date_... [trained]\nSparse, unbalanced variable filter removed date_col_day, date_col_mday, date_col_m... [trained]\nPCA extraction with date_col_index.num, date_col_year, date_col_half... [trained]"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-juiced-estimates",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-juiced-estimates",
    "title": "PCA with healthyR.ai",
    "section": "PCA Juiced Estimates",
    "text": "PCA Juiced Estimates\n\noutput_list$pca_juiced_estimates\n\n# A tibble: 76 × 8\n   date_col              value date_col…¹ date_…²     PC1     PC2     PC3    PC4\n   <dttm>                <dbl> <ord>      <ord>     <dbl>   <dbl>   <dbl>  <dbl>\n 1 2018-06-01 00:00:00  0.676  June       Friday   0.733   1.23   -1.30    0.536\n 2 2016-01-01 00:00:00 -0.133  January    Friday   3.51   -0.102  -0.500  -0.556\n 3 2013-05-01 00:00:00  1.67   May        Wednes…  1.15   -2.07   -1.68   -0.319\n 4 2018-10-01 00:00:00  0.337  October    Monday  -2.30    0.499   1.91   -1.92 \n 5 2016-09-01 00:00:00 -0.130  September  Thursd… -1.08    0.0972  0.410   2.64 \n 6 2016-07-01 00:00:00 -0.364  July       Friday  -0.0591  0.0211 -0.333   1.11 \n 7 2020-02-01 00:00:00 -0.646  February   Saturd…  3.08    2.48   -0.0249  0.214\n 8 2020-08-01 00:00:00 -1.42   August     Saturd… -0.491   2.60    0.142   1.88 \n 9 2018-03-01 00:00:00  0.243  March      Thursd…  2.74    0.848  -1.53    0.260\n10 2015-05-01 00:00:00 -0.0148 May        Friday   1.18   -0.636  -1.88   -0.147\n# … with 66 more rows, and abbreviated variable names ¹​date_col_month.lbl,\n#   ²​date_col_wday.lbl"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-baked-data",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-baked-data",
    "title": "PCA with healthyR.ai",
    "section": "PCA Baked Data",
    "text": "PCA Baked Data\n\noutput_list$pca_baked_data\n\n# A tibble: 95 × 8\n   date_col            value date_col_month…¹ date_…²    PC1   PC2    PC3    PC4\n   <dttm>              <dbl> <ord>            <ord>    <dbl> <dbl>  <dbl>  <dbl>\n 1 2013-01-01 00:00:00 1.86  January          Tuesday  3.60  -2.64  1.13  -0.871\n 2 2013-02-01 00:00:00 0.596 February         Friday   2.93  -1.76 -0.532  0.424\n 3 2013-03-01 00:00:00 0.864 March            Friday   2.62  -1.95 -2.24   0.643\n 4 2013-04-01 00:00:00 1.10  April            Monday   2.09  -2.68  1.39  -0.867\n 5 2013-05-01 00:00:00 1.67  May              Wednes…  1.15  -2.07 -1.68  -0.319\n 6 2013-06-01 00:00:00 0.923 June             Saturd…  0.612 -1.57 -2.01   0.919\n 7 2013-07-01 00:00:00 1.29  July             Monday  -0.669 -2.41  2.29  -0.420\n 8 2013-08-01 00:00:00 1.18  August           Thursd… -0.628 -1.76 -0.165  1.96 \n 9 2013-09-01 00:00:00 0.714 September        Sunday  -1.11  -2.19  0.910  2.25 \n10 2013-10-01 00:00:00 1.40  October          Tuesday -2.55  -1.91 -0.128 -1.48 \n# … with 85 more rows, and abbreviated variable names ¹​date_col_month.lbl,\n#   ²​date_col_wday.lbl"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-variance-data-frame",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-variance-data-frame",
    "title": "PCA with healthyR.ai",
    "section": "PCA Variance Data Frame",
    "text": "PCA Variance Data Frame\n\noutput_list$pca_variance_df\n\n# A tibble: 13 × 6\n   PC    var_explained var_pct_txt cum_var_pct cum_var_pct_txt ou_threshold\n   <chr>         <dbl> <chr>             <dbl> <chr>           <fct>       \n 1 PC1   0.395         39.55%            0.395 39.55%          Under       \n 2 PC2   0.160         16.02%            0.556 55.57%          Under       \n 3 PC3   0.113         11.29%            0.669 66.86%          Under       \n 4 PC4   0.107         10.75%            0.776 77.61%          Over        \n 5 PC5   0.0824        8.24%             0.858 85.85%          Over        \n 6 PC6   0.0526        5.26%             0.911 91.11%          Over        \n 7 PC7   0.0449        4.49%             0.956 95.59%          Over        \n 8 PC8   0.0400        4.00%             0.996 99.59%          Over        \n 9 PC9   0.00411       0.41%             1.00  100.00%         Over        \n10 PC10  0.0000178     0.00%             1.00  100.00%         Over        \n11 PC11  0.000000712   0.00%             1.00  100.00%         Over        \n12 PC12  0.000000273   0.00%             1.00  100.00%         Over        \n13 PC13  0.00000000196 0.00%             1     100.00%         Over"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-rotation-data-frame",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-rotation-data-frame",
    "title": "PCA with healthyR.ai",
    "section": "PCA Rotation Data Frame",
    "text": "PCA Rotation Data Frame\n\noutput_list$pca_rotation_df\n\n# A tibble: 13 × 13\n        PC1      PC2      PC3     PC4     PC5     PC6      PC7     PC8      PC9\n      <dbl>    <dbl>    <dbl>   <dbl>   <dbl>   <dbl>    <dbl>   <dbl>    <dbl>\n 1 -0.00137  0.671    0.116   -0.0521 -0.183   0.0165  0.0471   0.0277 -0.0177 \n 2  0.0529   0.667    0.116   -0.0610 -0.173   0.0146  0.0466   0.0313  0.00904\n 3 -0.385    0.0115   0.222    0.173   0.140   0.217   0.250   -0.0112  0.802  \n 4 -0.434    0.00824  0.0752  -0.0427  0.0615  0.135   0.00843  0.0332 -0.272  \n 5 -0.437    0.00278 -0.00879  0.0737 -0.0734  0.0167  0.00135 -0.0264 -0.213  \n 6 -0.0159   0.265   -0.406    0.274   0.468   0.254  -0.520   -0.366   0.0484 \n 7 -0.0608  -0.0200  -0.325    0.480  -0.542  -0.476  -0.0318  -0.244   0.187  \n 8 -0.437    0.00435 -0.00655  0.0740 -0.0733  0.0149  0.00231 -0.0303 -0.216  \n 9  0.0537  -0.164    0.562   -0.0242 -0.414   0.267  -0.572   -0.285   0.0519 \n10 -0.438    0.00638 -0.00736  0.0676 -0.0704  0.0236  0.00470 -0.0266 -0.219  \n11  0.250   -0.0238   0.208    0.420   0.0436  0.301   0.544   -0.492  -0.293  \n12 -0.0474   0.0762   0.516    0.142   0.460  -0.676  -0.108   -0.144  -0.0636 \n13  0.123    0.00776  0.150    0.666   0.0183  0.152  -0.161    0.676  -0.111  \n# … with 4 more variables: PC10 <dbl>, PC11 <dbl>, PC12 <dbl>, PC13 <dbl>"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-variance-scree-plot",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-variance-scree-plot",
    "title": "PCA with healthyR.ai",
    "section": "PCA Variance Scree Plot",
    "text": "PCA Variance Scree Plot\n\noutput_list$pca_variance_scree_plt"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-loadings-plot",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-loadings-plot",
    "title": "PCA with healthyR.ai",
    "section": "PCA Loadings Plot",
    "text": "PCA Loadings Plot\n\noutput_list$pca_loadings_plt\n\n\n\noutput_list$pca_loadings_plotly"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#top-n-loadings-plots",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#top-n-loadings-plots",
    "title": "PCA with healthyR.ai",
    "section": "Top N Loadings Plots",
    "text": "Top N Loadings Plots\n\noutput_list$pca_top_n_loadings_plt\n\n\n\noutput_list$pca_top_n_plotly"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-12-02/index.html",
    "href": "posts/weekly-rtip-healthyrai-2022-12-02/index.html",
    "title": "Auto KNN with {healthyR.ai}",
    "section": "",
    "text": "Minimal coding ML is not something that is unheard of and is rather prolific, think h2o and pycaret just to name two. There is also no shortage available for R with the h2o interface, and tidyfit. There are also similar low-code workflows in my r package {healthyR.ai}. Today I will specifically go through the workflow for Automatic KNN classification for the Iris data set where we will classify the Species."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#recipe-output",
    "href": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#recipe-output",
    "title": "Auto KNN with {healthyR.ai}",
    "section": "Recipe Output",
    "text": "Recipe Output\n\nauto_knn$recipe_info\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nOperations:\n\nNovel factor level assignment for recipes::all_nominal_predictors()\nDummy variables from recipes::all_nominal_predictors()\nZero variance filter on recipes::all_predictors()\nCentering and scaling for recipes::all_numeric()"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#model-info",
    "href": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#model-info",
    "title": "Auto KNN with {healthyR.ai}",
    "section": "Model Info",
    "text": "Model Info\n\nauto_knn$model_info$was_tuned\n\n[1] \"tuned\"\n\n\n\nauto_knn$model_info$model_spec\n\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = tune::tune()\n  weight_func = tune::tune()\n  dist_power = tune::tune()\n\nComputational engine: kknn \n\n\n\nauto_knn$model_info$wflw\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_novel()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = tune::tune()\n  weight_func = tune::tune()\n  dist_power = tune::tune()\n\nComputational engine: kknn \n\n\n\nauto_knn$model_info$fitted_wflw\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_novel()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nkknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(13L,     data, 5), distance = ~1.69935879141092, kernel = ~\"rank\")\n\nType of response variable: nominal\nMinimal misclassification: 0.03571429\nBest kernel: rank\nBest k: 13"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#tuning-info",
    "href": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#tuning-info",
    "title": "Auto KNN with {healthyR.ai}",
    "section": "Tuning Info",
    "text": "Tuning Info\n\nauto_knn$tuned_info$tuning_grid\n\n# A tibble: 10 × 3\n   neighbors weight_func  dist_power\n       <int> <chr>             <dbl>\n 1         4 triangular        0.764\n 2        11 rectangular       0.219\n 3         5 gaussian          1.35 \n 4        14 triweight         0.351\n 5         5 biweight          1.05 \n 6         9 optimal           1.87 \n 7         7 cos               0.665\n 8        11 inv               1.18 \n 9        13 rank              1.70 \n10         1 epanechnikov      1.58 \n\n\n\nauto_knn$tuned_info$cv_obj\n\n# Monte Carlo cross-validation (0.75/0.25) with 25 resamples  \n# A tibble: 25 × 2\n   splits          id        \n   <list>          <chr>     \n 1 <split [84/28]> Resample01\n 2 <split [84/28]> Resample02\n 3 <split [84/28]> Resample03\n 4 <split [84/28]> Resample04\n 5 <split [84/28]> Resample05\n 6 <split [84/28]> Resample06\n 7 <split [84/28]> Resample07\n 8 <split [84/28]> Resample08\n 9 <split [84/28]> Resample09\n10 <split [84/28]> Resample10\n# … with 15 more rows\n\n\n\nauto_knn$tuned_info$tuned_results\n\n# Tuning results\n# Monte Carlo cross-validation (0.75/0.25) with 25 resamples  \n# A tibble: 25 × 4\n   splits          id         .metrics           .notes          \n   <list>          <chr>      <list>             <list>          \n 1 <split [84/28]> Resample01 <tibble [110 × 7]> <tibble [0 × 3]>\n 2 <split [84/28]> Resample02 <tibble [110 × 7]> <tibble [0 × 3]>\n 3 <split [84/28]> Resample03 <tibble [110 × 7]> <tibble [0 × 3]>\n 4 <split [84/28]> Resample04 <tibble [110 × 7]> <tibble [0 × 3]>\n 5 <split [84/28]> Resample05 <tibble [110 × 7]> <tibble [0 × 3]>\n 6 <split [84/28]> Resample06 <tibble [110 × 7]> <tibble [0 × 3]>\n 7 <split [84/28]> Resample07 <tibble [110 × 7]> <tibble [0 × 3]>\n 8 <split [84/28]> Resample08 <tibble [110 × 7]> <tibble [0 × 3]>\n 9 <split [84/28]> Resample09 <tibble [110 × 7]> <tibble [0 × 3]>\n10 <split [84/28]> Resample10 <tibble [110 × 7]> <tibble [0 × 3]>\n# … with 15 more rows\n\n\n\nauto_knn$tuned_info$grid_size\n\n[1] 10\n\n\n\nauto_knn$tuned_info$best_metric\n\n[1] \"f_meas\"\n\n\n\nauto_knn$tuned_info$best_result_set\n\n# A tibble: 1 × 9\n  neighbors weight_func dist_power .metric .estima…¹  mean     n std_err .config\n      <int> <chr>            <dbl> <chr>   <chr>     <dbl> <int>   <dbl> <chr>  \n1        13 rank              1.70 f_meas  macro     0.957    25 0.00655 Prepro…\n# … with abbreviated variable name ¹​.estimator\n\n\n\nauto_knn$tuned_info$tuning_grid_plot\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\nauto_knn$tuned_info$plotly_grid_plot\n\n\n\n\n\nVoila!\nThank you for reading."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html",
    "title": "Time Series Lag Correlation Plots",
    "section": "",
    "text": "In time series analysis there is something called a lag. This simply means we take a look at some past event from some point in time t. This is a non-statistical method for looking at a relationship between a timeseries and its lags.\n{healthyR.ts} has a function called ts_lag_correlation(). This function, as described by it’s name, provides more than just a simple lag plot.\nThis function provides a lot of extra information for the end user. First let’s go over the function call."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#function-call",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#function-call",
    "title": "Time Series Lag Correlation Plots",
    "section": "Function Call",
    "text": "Function Call\nHere is the full call:\n\nts_lag_correlation(\n  .data,\n  .date_col,\n  .value_col,\n  .lags = 1,\n  .heatmap_color_low = \"white\",\n  .heatmap_color_hi = \"steelblue\"\n)\n\nHere are the arguments that get supplied to the different parameters.\n\n.data - A tibble of time series data\n.date_col - A date column\n.value_col - The value column being analyzed\n.lags - This is a vector of integer lags, ie 1 or c(1,6,12)\n.heatmap_color_low - What color should the low values of the heatmap of the correlation matrix be, the default is ‘white’\n.heatmap_color_hi - What color should the low values of the heatmap of the correlation matrix be, the default is ‘steelblue’"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#function-return",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#function-return",
    "title": "Time Series Lag Correlation Plots",
    "section": "Function Return",
    "text": "Function Return\nThe function itself returns a list object. The list has the following elements in it:\nData Elements\n\nlag_list\nlag_tbl\ncorrelation_lag_matrix\ncorrelation_lag_tbl\n\nPlot Elements\n\nlag_plot\nplotly_lag_plot\ncorrelation_heatmap\nplotly_heatmap"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#data-elements",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#data-elements",
    "title": "Time Series Lag Correlation Plots",
    "section": "Data Elements",
    "text": "Data Elements\nHere are the data elements.\n\noutput$data$lag_list\n\n[[1]]\n# A tibble: 143 × 3\n   lag   value lagged_value\n   <fct> <dbl>        <dbl>\n 1 1       118          112\n 2 1       132          118\n 3 1       129          132\n 4 1       121          129\n 5 1       135          121\n 6 1       148          135\n 7 1       148          148\n 8 1       136          148\n 9 1       119          136\n10 1       104          119\n# … with 133 more rows\n\n[[2]]\n# A tibble: 141 × 3\n   lag   value lagged_value\n   <fct> <dbl>        <dbl>\n 1 3       129          112\n 2 3       121          118\n 3 3       135          132\n 4 3       148          129\n 5 3       148          121\n 6 3       136          135\n 7 3       119          148\n 8 3       104          148\n 9 3       118          136\n10 3       115          119\n# … with 131 more rows\n\n[[3]]\n# A tibble: 138 × 3\n   lag   value lagged_value\n   <fct> <dbl>        <dbl>\n 1 6       148          112\n 2 6       148          118\n 3 6       136          132\n 4 6       119          129\n 5 6       104          121\n 6 6       118          135\n 7 6       115          148\n 8 6       126          148\n 9 6       141          136\n10 6       135          119\n# … with 128 more rows\n\n[[4]]\n# A tibble: 132 × 3\n   lag   value lagged_value\n   <fct> <dbl>        <dbl>\n 1 12      115          112\n 2 12      126          118\n 3 12      141          132\n 4 12      135          129\n 5 12      125          121\n 6 12      149          135\n 7 12      170          148\n 8 12      170          148\n 9 12      158          136\n10 12      133          119\n# … with 122 more rows\n\n\nThis is a list of all the tibbles of the different lags that were chosen.\n\noutput$data$lag_tbl\n\n# A tibble: 554 × 4\n   lag   value lagged_value lag_title\n   <fct> <dbl>        <dbl> <fct>    \n 1 1       118          112 Lag: 1   \n 2 1       132          118 Lag: 1   \n 3 1       129          132 Lag: 1   \n 4 1       121          129 Lag: 1   \n 5 1       135          121 Lag: 1   \n 6 1       148          135 Lag: 1   \n 7 1       148          148 Lag: 1   \n 8 1       136          148 Lag: 1   \n 9 1       119          136 Lag: 1   \n10 1       104          119 Lag: 1   \n# … with 544 more rows\n\n\nThis is the long lag tibble with all of the lags in it.\n\noutput$data$correlation_lag_matrix\n\n                value value_lag1 value_lag3 value_lag6 value_lag12\nvalue       1.0000000  0.9542938  0.8186636  0.7657001   0.9905274\nvalue_lag1  0.9542938  1.0000000  0.8828054  0.7726530   0.9492382\nvalue_lag3  0.8186636  0.8828054  1.0000000  0.8349550   0.8218493\nvalue_lag6  0.7657001  0.7726530  0.8349550  1.0000000   0.7780911\nvalue_lag12 0.9905274  0.9492382  0.8218493  0.7780911   1.0000000\n\n\nThis is the correlation matrix.\n\noutput$data$correlation_lag_tbl\n\n# A tibble: 25 × 3\n   name        data_names value\n   <fct>       <fct>      <dbl>\n 1 value       value      1    \n 2 value_lag1  value      0.954\n 3 value_lag3  value      0.819\n 4 value_lag6  value      0.766\n 5 value_lag12 value      0.991\n 6 value       value_lag1 0.954\n 7 value_lag1  value_lag1 1    \n 8 value_lag3  value_lag1 0.883\n 9 value_lag6  value_lag1 0.773\n10 value_lag12 value_lag1 0.949\n# … with 15 more rows\n\n\nThis is the correlation lag tibble"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#plot-elements",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#plot-elements",
    "title": "Time Series Lag Correlation Plots",
    "section": "Plot Elements",
    "text": "Plot Elements\n\noutput$plots$lag_plot\n\n\n\n\nThe Lag Plot itself.\n\noutput$plots$plotly_lag_plot\n\n\n\n\n\nA plotly version of the lag plot.\n\noutput$plots$correlation_heatmap\n\n\n\n\nA heatmap of the correlations.\n\noutput$plots$plotly_heatmap\n\n\n\n\n\nA plotly version of the correlation heatmap.\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-12-09/index.html",
    "href": "posts/weekly-rtip-healthyrts-2022-12-09/index.html",
    "title": "Create QQ Plots for Time Series Models with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nA Q-Q plot, or quantile-quantile plot, is a graphical tool for comparing two sets of data to assess whether they come from the same distribution. In the context of time series modeling, a Q-Q plot can be used to check whether the residuals of a fitted time series model follow the normal distribution. This is important because many time series models, such as the autoregressive moving average (ARMA) model, assume that the residuals are normally distributed.\nTo create a Q-Q plot, the data are first sorted in ascending order and then divided into quantiles. The quantiles of the first dataset are then plotted against the quantiles of the second dataset. If the two datasets come from the same distribution, the points on the Q-Q plot will fall approximately on a straight line. Deviations from this line can indicate departures from the assumed distribution.\nFor example, if we have a time series dataset and we fit an ARMA model to it, we can use a Q-Q plot to check whether the residuals of the fitted model are normally distributed. If the Q-Q plot shows that the residuals do not follow the normal distribution, we may need to consider using a different time series model that does not assume normality of the residuals.\nIn summary, Q-Q plots are a useful tool for assessing the distribution of a dataset and for checking whether a time series model has produced satisfactory residuals.\nIn the R package {healthyR.ts} there is a function to view the QQ plot. This function is called ts_qq_plot() and it is meant to work with a calibration tibble from the excellent {modeltime} which is a {parsnip} extension package.\n\n\nFunction\nLet’s take a look at the full function call and the arguments that get provided to the parameters.\n\nts_qq_plot(\n  .calibration_tbl, \n  .model_id = NULL, \n  .interactive = FALSE\n  )\n\n\n.calibration_tbl - A calibrated modeltime table.\n.model_id - The id of a particular model from a calibration tibble. If there are multiple models in the tibble and this remains NULL then the plot will be returned using ggplot2::facet_grid(~ .model_id)\n.interactive - A boolean with a default value of FALSE. TRUE will produce an interactive plotly plot.\n\n\n\nExamples\nLet’s work through an example, and since we already spoke about ARMA let’s try out an ARMA model.\n\nlibrary(healthyR.ts)\nlibrary(modeltime)\nlibrary(timetk)\nlibrary(rsample)\nlibrary(workflows)\nlibrary(recipes)\nlibrary(parsnip)\n\ndata_tbl <- ts_to_tbl(AirPassengers) %>%\n  select(-index)\n\nsplits <- time_series_split(\n  data_tbl,\n  date_var = date_col,\n  assess = \"12 months\",\n  cumulative = TRUE\n)\n\nrec_obj <- recipe(value ~ ., training(splits))\n\nmodel_spec_arima <- arima_reg() %>%\n  set_engine(engine = \"auto_arima\")\n\nwflw_fit_arima <- workflow() %>%\n  add_recipe(rec_obj) %>%\n  add_model(model_spec_arima) %>%\n  fit(training(splits))\n\nmodel_tbl <- modeltime_table(wflw_fit_arima)\n\ncalibration_tbl <- model_tbl %>%\n  modeltime_calibrate(new_data = testing(splits))\n\nts_qq_plot(calibration_tbl, .interactive = TRUE)"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-12-16/index.html",
    "href": "posts/weekly-rtip-healthyrts-2022-12-16/index.html",
    "title": "Model Scedacity Plots with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nScedacity plots are a useful tool for evaluating the performance of time series models and identifying trends or patterns in the data. They are a type of scatter plot that compares the predicted values produced by a model to the observed values in the data, with a diagonal reference line indicating perfect agreement between the two.\nThe {healthyR.ts} package in R provides a convenient function for creating scedacity plots for time series data, called ts_scedacity_scatter_plot(). This function takes as input a calibration tibble which you would get from using the {modeltime} library, and produces a scedacity plot showing the predicted values against the observed values.\nOne of the main benefits of using a scedacity plot is that it allows you to visualize the accuracy of the model’s predictions. If the points on the plot fall close to the reference line, it indicates that the model is able to accurately predict the values in the data. On the other hand, if the points are scattered far from the reference line, it suggests that the model is not performing well and may need to be improved or refined.\nIn addition to evaluating the accuracy of the model, scedacity plots can also be used to identify trends or patterns in the data. For example, if there is a clear upward or downward trend in the points on the plot, it may indicate that the model is over- or under-estimating the values in the data. By identifying these trends, you can adjust the model or try different approaches to improve its performance.\nOverall, scedacity plots are a useful tool for evaluating the performance of time series models and identifying trends or patterns in the data. The ts_scedacity_scatter_plot() function in the {healthyR.ts} package makes it easy to create these plots and gain insights into the performance of your time series models.\n\n\nFunction\nLet’s take a look at the full function call.\n\nts_scedacity_scatter_plot(\n  .calibration_tbl,\n  .model_id = NULL,\n  .interactive = FALSE\n)\n\nLet’s take a look at the arguments that get provided to the parameters.\n\n.calibration_tbl - A calibrated modeltime table.\n.model_id - The id of a particular model from a calibration tibble. If there are multiple models in the tibble and this remains NULL then the plot will be returned using ggplot2::facet_grid(~ .model_id)\n.interactive - A boolean with a default value of FALSE. TRUE will produce an interactive plotly plot.\n\n\n\nExample\nNow for an example.\n\nlibrary(healthyR.ts)\nlibrary(dplyr)\nlibrary(timetk)\nlibrary(modeltime)\nlibrary(rsample)\nlibrary(workflows)\nlibrary(parsnip)\nlibrary(recipes)\n\ndata_tbl <- ts_to_tbl(AirPassengers) %>%\n  select(-index)\n\nsplits <- time_series_split(\n  data_tbl,\n  date_var = date_col,\n  assess = \"12 months\",\n  cumulative = TRUE\n)\n\nrec_obj <- recipe(value ~ ., training(splits))\n\nmodel_spec_arima <- arima_reg() %>%\n  set_engine(engine = \"auto_arima\")\n\nmodel_spec_mars <- mars(mode = \"regression\") %>%\n  set_engine(\"earth\")\n\nwflw_fit_arima <- workflow() %>%\n  add_recipe(rec_obj) %>%\n  add_model(model_spec_arima) %>%\n  fit(training(splits))\n\nwflw_fit_mars <- workflow() %>%\n  add_recipe(rec_obj) %>%\n  add_model(model_spec_mars) %>%\n  fit(training(splits))\n\nmodel_tbl <- modeltime_table(wflw_fit_arima, wflw_fit_mars)\n\ncalibration_tbl <- model_tbl %>%\n  modeltime_calibrate(new_data = testing(splits))\n\nts_scedacity_scatter_plot(calibration_tbl)\n\n\n\n\nNow the interactive plot.\n\nts_scedacity_scatter_plot(calibration_tbl, .interactive = TRUE)\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-12-30/index.html",
    "href": "posts/weekly-rtip-healthyrts-2022-12-30/index.html",
    "title": "Event Analysis with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nTime-to-event analysis, also known as survival analysis, is a statistical technique used to analyze the length of time until an event occurs. This type of analysis is often used in fields such as healthcare, engineering, and finance to understand the factors that influence the likelihood of an event occurring and to make predictions about future events.\nIn economics, an event study is a statistical technique used to analyze the effect of a specific event on a particular market or financial instrument. Event studies are commonly used in finance to understand how events, such as the announcement of a new product, the release of financial earnings, or a change in government policy, may impact the price or performance of a company’s stock or other financial instruments.\nTo conduct an event study, analysts typically collect data on the performance of a market or financial instrument before and after the event in question. This data is then used to estimate the effect of the event on the market or instrument.\nThere are several different methods that can be used to conduct an event study, including the market model, the abnormal return method, and the buy-and-hold abnormal return method. These methods allow analysts to quantify the effect of the event on the market or instrument and to identify any changes in market behavior that may have occurred as a result of the event.\nOverall, event studies are a valuable tool for understanding how specific events may impact financial markets and instruments, and are widely used in finance and economics to inform investment decisions and to better understand market behavior.\nIn this post we are going to examine a function from the R package {healthyR.ts} has a function called ts_time_event_analysis_tbl() that will help us understand what happens after a specified event, in this instance it will always be some percentage decrease or increase in a value.\nThere is a great article from Investopedia on this economic topic here\n\n\nFunction\nThe function is ts_time_event_analysis_tbl() and it’s complimentary plotting function ts_event_analysis_plot().\nHere is the tibble data return function.\n\nts_time_event_analysis_tbl(\n  .data,\n  .date_col,\n  .value_col,\n  .percent_change = 0.05,\n  .horizon = 12,\n  .precision = 2,\n  .direction = \"forward\",\n  .filter_non_event_groups = TRUE\n)\n\nLet’s take a look at the arguments to the parameters for this one.\n\n.data - The date.frame/tibble that holds the data.\n.date_col - The column with the date value.\n.value_col - The column with the value you are measuring.\n.percent_change - This defaults to 0.05 which is a 5% increase in the value_col.\n.horizon - How far do you want to look back or ahead.\n.precision - The default is 2 which means it rounds the lagged 1 value percent change to 2 decimal points. You may want more for more finely tuned results, this will result in fewer groupings.\n.direction - The default is forward. You can supply either forward, backwards or both.\nfilter_non_event_groups - The default is TRUE, this drops groupings with no events on the rare occasion it does occur.\n\nNow the plotting function.\n\nts_event_analysis_plot(\n  .data,\n  .plot_type = \"mean\",\n  .plot_ci = TRUE,\n  .interactive = FALSE\n)\n\n\n.data - The data that comes from the ts_time_event_analysis_tbl()\n.plot_type - The default is “mean” which will show the mean event change of the output from the analysis tibble. The possible values for this are: mean, median, and individual.\n.plot_ci - The default is TRUE. This will only work if you choose one of the aggregate plots of either “mean” or “median”\n.interactive - The default is FALSE. TRUE will return a plotly plot.\n\n\n\nExamples\nLet’s go through a couple examples using the AirPassengers data. We will first transform it into a tibble and then we will use a look period of 6. Let’s see the data output and then we will visualize.\n\nlibrary(healthyR.ts)\nlibrary(dplyr)\n\ndf <- ts_to_tbl(AirPassengers) %>% select(-index)\n\nevent_tbl <- ts_time_event_analysis_tbl(\n  .data = df,\n  .horizon = 6,\n  .date_col = date_col,\n  .value_col = value,\n  .direction = \"both\"\n)\n\nglimpse(event_tbl)\n\nRows: 33\nColumns: 18\n$ rowid                <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15…\n$ date_col             <date> 1949-02-01, 1949-03-01, 1949-04-01, 1949-05-01, …\n$ value                <dbl> 118, 132, 129, 121, 135, 148, 148, 199, 184, 162,…\n$ lag_val              <dbl> 112, 118, 132, 129, 121, 135, 148, 199, 199, 184,…\n$ adj_diff             <dbl> 6, 14, -3, -8, 14, 13, 0, 0, -15, -22, -16, 20, 5…\n$ relative_change_raw  <dbl> 0.05357143, 0.11864407, -0.02272727, -0.06201550,…\n$ relative_change      <dbl> 0.05, 0.12, -0.02, -0.06, 0.12, 0.10, 0.00, 0.00,…\n$ pct_chg_mark         <lgl> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ event_base_change    <dbl> 0.00000000, 0.11864407, -0.02272727, -0.06201550,…\n$ group_number         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2…\n$ numeric_group_number <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2…\n$ group_event_number   <int> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ x                    <int> 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1…\n$ mean_event_change    <dbl> 0.00000000, 0.03849647, -0.06815622, -0.04991040,…\n$ median_event_change  <dbl> 0.00000000, 0.07222222, -0.06217617, -0.06201550,…\n$ event_change_ci_low  <dbl> 0.00000000, -0.06799693, -0.11669576, -0.09692794…\n$ event_change_ci_high <dbl> 0.000000000, 0.116322976, -0.024699717, 0.0073964…\n$ event_type           <fct> Before, Before, Before, Before, Before, Before, A…\n\n\nLet’s visualize!\n\nts_event_analysis_plot(\n  .data = event_tbl\n)\n\n\n\n\nLet’s see the median now.\n\nts_event_analysis_plot(\n  .data = event_tbl,\n  .plot_type = \"median\"\n)\n\n\n\n\nNow let’s see it as an interactive plot.\n\nts_event_analysis_plot(\n  .data = event_tbl,\n  .plot_type = \"median\",\n  .interactive = TRUE\n)\n\n\n\n\n\nNow let’s see all the individual groups.\n\nts_event_analysis_plot(\n  .data = event_tbl,\n  .plot_type = \"individual\",\n  .interactive = TRUE\n)\n\n\n\n\n\nSingle direction plotting.\n\nts_time_event_analysis_tbl(\n  .data = df,\n  .horizon = 6,\n  .date_col = date_col,\n  .value_col = value,\n  .direction = \"backward\"\n) %>%\n  ts_event_analysis_plot()\n\n\n\n\nAnd…\n\nts_time_event_analysis_tbl(\n  .data = df,\n  .horizon = 6,\n  .date_col = date_col,\n  .value_col = value,\n  .direction = \"forward\"\n) %>%\n  ts_event_analysis_plot()\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-lists-2023-01-20/index.html",
    "href": "posts/weekly-rtip-lists-2023-01-20/index.html",
    "title": "Another Post on Lists",
    "section": "",
    "text": "Introduction\nManipulating lists in R is a powerful tool for organizing and analyzing data. Here are a few common ways to manipulate lists:\n\nIndexing: Lists can be indexed using square brackets “[ ]” and numeric indices. For example, to access the first element of a list called “mylist”, you would use the expression “mylist[1]”.\nSubsetting: Lists can be subsetted using the same square bracket notation, but with a logical vector indicating which elements to keep. For example, to select all elements of “mylist” that are greater than 5, you would use the expression “mylist[mylist > 5]”.\nModifying elements: Elements of a list can be modified by assigning new values to them using the assignment operator “<-”. For example, to change the third element of “mylist” to 10, you would use the expression “mylist[3] <- 10”.\nAdding elements: New elements can be added to a list using the concatenation operator “c()” or the “append()” function. For example, to add the number 7 to the end of “mylist”, you would use the expression “mylist <- c(mylist, 7)”.\nRemoving elements: Elements can be removed from a list using the “-” operator. For example, to remove the second element of “mylist”, you would use the expression “mylist <- mylist[-2]”.\n\n\n\nExamples\nHere is an example of how these methods can be used to manipulate a list in R:\n\nmylist <- list(1,2,3,4,5)\n\n# Indexing\nmylist[[1]] # Returns 1\n\n[1] 1\n\n# Subsetting\nmylist[mylist > 3] # Returns 4 & 5\n\n[[1]]\n[1] 4\n\n[[2]]\n[1] 5\n\n# Modifying elements\nmylist[[3]] <- 10\nmylist # Returns 1 2 10 4 5 7\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 10\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\n# Adding elements\nmylist <- c(mylist, 7)\nmylist # Returns 1 2 10 4 5 7\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 10\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\n[[6]]\n[1] 7\n\n# Removing elements\nmylist[-3]\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 4\n\n[[4]]\n[1] 5\n\n[[5]]\n[1] 7\n\nmylist # Returns 1 2 4 5 7\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 10\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\n[[6]]\n[1] 7\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-tidyaml-2023-01-13/index.html",
    "href": "posts/weekly-rtip-tidyaml-2023-01-13/index.html",
    "title": "The building of {tidyAML}",
    "section": "",
    "text": "Introduction\nYesterday I posted on An Update to {tidyAML} where I was discussing some of my thought process and how things could potentially work for the package.\nToday I want to showcase how the function fast_regression_parsnip_spec_tbl() and it’s complimentary function fast_classification_parsnip_spec_tbl() actually work or maybe don’t work for that matter.\nWe are going to pick on fast_regression_parsnip_spec_tbl() in today’s post. The point of it is that it creates a tibble of parsnip regression model specifications. This will create a tibble of 46 different regression model specifications which can be filtered. The model specs are created first and then filtered out. This will only create models for regression problems. To find all of the supported models in this package you can visit the parsnip search page\n\n\nFunction\nFirst let’s take a look at the function call itself.\n\nfast_regression_parsnip_spec_tbl(\n  .parsnip_fns = \"all\", \n  .parsnip_eng = \"all\"\n  )\n\nNow let’s take a look at the arguments:\n\n.parsnip_fns - The default for this is set to all. This means that all of the parsnip linear regression functions will be used, for example linear_reg(), or cubist_rules. You can also choose to pass a c() vector like c(\"linear_reg\",\"cubist_rules\")\n.parsnip_eng - The default for this is set to all. This means that all of the parsnip linear regression engines will be used, for example lm, or glm. You can also choose to pass a c() vector like c('lm', 'glm')\n\nThe workhorse to this function is the internal_make_spec_tbl() function. This is the one that will be the subject of the post. Let’s take a look at it’s inner workings, afterall this is open source.\n\ninternal_make_spec_tbl <- function(.data){\n\n  # Checks ----\n  df <- dplyr::as_tibble(.data)\n\n  nms <- unique(names(df))\n\n  if (!\".parsnip_engine\" %in% nms | !\".parsnip_mode\" %in% nms | !\".parsnip_fns\" %in% nms){\n    rlang::abort(\n      message = \"The model tibble must come from the class/reg to parsnip function.\",\n      use_cli_format = TRUE\n    )\n  }\n\n  # Make tibble ----\n  mod_spec_tbl <- df %>%\n    dplyr::mutate(\n      model_spec = purrr::pmap(\n        dplyr::cur_data(),\n        ~ match.fun(..3)(mode = ..2, engine = ..1)\n      )\n    ) %>%\n    # add .model_id column\n    dplyr::mutate(.model_id = dplyr::row_number()) %>%\n    dplyr::select(.model_id, dplyr::everything())\n\n  # Return ----\n  return(mod_spec_tbl)\n\n}\n\nLet’s examine this (and it is currently changing form in a github issue). Firstly, we are taking in a data.frame/tibble that has to have certain names in it (this is going to change and look for a class instead). Once this determination is TRUE we then proceed to the meat and potatoes of it. The internal mod_spec_tbl is made using mutate, pmap, cur_data and match.fun. What this does essentially is the following:\n\nmutate a column called model_spec\nUse the {purrr} function pmap which maps over several columns in parallel to create the model spec.\nInside of the pmap we use cur_data() to get the current line where we match the function using match.fun (which takes a character string of the function, this means the library needs to be loaded) we supply the column it is in and then we supply the arguments we want.\nWe give it a numeric model id\nWe then ensure that the .model_id column is first.\n\n\n\nExample\nLet’s see it in action!\n\nlibrary(tidyAML) # Not yet available, you can install from GitHub though\n\nfast_regression_parsnip_spec_tbl()\n\n# A tibble: 46 × 5\n   .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n       <int> <chr>           <chr>         <chr>        <list>    \n 1         1 lm              regression    linear_reg   <spec[+]> \n 2         2 brulee          regression    linear_reg   <spec[+]> \n 3         3 gee             regression    linear_reg   <spec[+]> \n 4         4 glm             regression    linear_reg   <spec[+]> \n 5         5 glmer           regression    linear_reg   <spec[+]> \n 6         6 glmnet          regression    linear_reg   <spec[+]> \n 7         7 gls             regression    linear_reg   <spec[+]> \n 8         8 lme             regression    linear_reg   <spec[+]> \n 9         9 lmer            regression    linear_reg   <spec[+]> \n10        10 stan            regression    linear_reg   <spec[+]> \n# … with 36 more rows\n\n\nSo we see we get a nicely generated tibble of output that matchs a model spec to the .model_id and to the appropriate parsnip engine and mode\nWe can also choose the models we may want by giving either arguments to the .parsnip_engine parameter or .parsnip_fns or both.\n\nlibrary(dplyr)\n\nfast_regression_parsnip_spec_tbl(.parsnip_fns = \"linear_reg\")\n\n# A tibble: 11 × 5\n   .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n       <int> <chr>           <chr>         <chr>        <list>    \n 1         1 lm              regression    linear_reg   <spec[+]> \n 2         2 brulee          regression    linear_reg   <spec[+]> \n 3         3 gee             regression    linear_reg   <spec[+]> \n 4         4 glm             regression    linear_reg   <spec[+]> \n 5         5 glmer           regression    linear_reg   <spec[+]> \n 6         6 glmnet          regression    linear_reg   <spec[+]> \n 7         7 gls             regression    linear_reg   <spec[+]> \n 8         8 lme             regression    linear_reg   <spec[+]> \n 9         9 lmer            regression    linear_reg   <spec[+]> \n10        10 stan            regression    linear_reg   <spec[+]> \n11        11 stan_glmer      regression    linear_reg   <spec[+]> \n\nfast_regression_parsnip_spec_tbl(.parsnip_eng = c(\"lm\",\"glm\"))\n\n# A tibble: 3 × 5\n  .model_id .parsnip_engine .parsnip_mode .parsnip_fns model_spec\n      <int> <chr>           <chr>         <chr>        <list>    \n1         1 lm              regression    linear_reg   <spec[+]> \n2         2 glm             regression    linear_reg   <spec[+]> \n3         3 glm             regression    poisson_reg  <spec[+]> \n\nfast_regression_parsnip_spec_tbl(.parsnip_eng = \"glm\") %>%\n  pull(model_spec)\n\n[[1]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\n[[2]]\nPoisson Regression Model Specification (regression)\n\nComputational engine: glm \n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-tidydensity-2022-11-23/index.html",
    "href": "posts/weekly-rtip-tidydensity-2022-11-23/index.html",
    "title": "Summary Statistics with {TidyDensity}",
    "section": "",
    "text": "Many times someone may want to see a summary or cumulative statistic for a given set of data or even from several simulations of data. I went over bootstrap plotting earlier this month, and this is a form of what we will go over today although slightly more restrictive.\nI have decided to make today my weekly r-tip because tomorrow is Thanksgiving here in the US and I am taking an extended holiday so I won’t be back until Monday.\nToday’s function and weekly tip is on tidy_stat_tbl(). It is meant to be used with a tidy_ distribution function. Let’s take a look."
  },
  {
    "objectID": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#single-simulation",
    "href": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#single-simulation",
    "title": "Summary Statistics with {TidyDensity}",
    "section": "Single Simulation",
    "text": "Single Simulation\nLet’s go over some examples. Firstly, we will go over all the different .return_type’s of a single simulation of tidy_normal() using the quantile function.\nVector Output BE CAREFUL IT USES SAPPLY\n\nlibrary(TidyDensity)\n\nset.seed(123)\ntn <- tidy_normal()\n\ntidy_stat_tbl(\n  .data = tn,\n  .x = y,\n  .return_type = \"vector\",\n  .fns = quantile,\n  na.rm = TRUE,\n  probs = c(0.025, 0.5, 0.975)\n  )\n\n      sim_number_1\n2.5%   -1.59190149\n50%    -0.07264039\n97.5%   1.77074730\n\n\nList Output with lapply\n\ntidy_stat_tbl(\n  tn, y, quantile, \"list\", na.rm = TRUE\n)\n\n$sim_number_1\n         0%         25%         50%         75%        100% \n-1.96661716 -0.55931702 -0.07264039  0.69817699  2.16895597 \n\ntidy_stat_tbl(\n  tn, y, quantile, \"list\", na.rm = TRUE, \n  probs = c(0.025, 0.5, 0.975)\n)\n\n$sim_number_1\n       2.5%         50%       97.5% \n-1.59190149 -0.07264039  1.77074730 \n\n\nTibble output with tibble\n\ntidy_stat_tbl(\n  tn, y, quantile, \"tibble\", na.rm = TRUE\n)\n\n# A tibble: 5 × 3\n  sim_number name  quantile\n  <fct>      <chr>    <dbl>\n1 1          0%     -1.97  \n2 1          25%    -0.559 \n3 1          50%    -0.0726\n4 1          75%     0.698 \n5 1          100%    2.17  \n\ntidy_stat_tbl(\n  tn, y, quantile, \"tibble\", na.rm = TRUE, \n  probs = c(0.025, 0.5, 0.975)\n)\n\n# A tibble: 3 × 3\n  sim_number name  quantile\n  <fct>      <chr>    <dbl>\n1 1          2.5%   -1.59  \n2 1          50%    -0.0726\n3 1          97.5%   1.77  \n\n\nTibble output with data.table The output object is a tibble but data.table is used to perform the calculations which can be magnitudes faster when simulations are large. I will showcase down the post.\n\ntidy_stat_tbl(\n  tn, y, quantile, .use_data_table = TRUE, na.rm = TRUE\n)\n\n# A tibble: 5 × 3\n  sim_number name  quantile\n  <fct>      <fct>    <dbl>\n1 1          0%     -1.97  \n2 1          25%    -0.559 \n3 1          50%    -0.0726\n4 1          75%     0.698 \n5 1          100%    2.17  \n\ntidy_stat_tbl(\n  tn, y, quantile, .use_data_table = TRUE, na.rm = TRUE, \n  probs = c(0.025, 0.5, 0.975)\n)\n\n# A tibble: 3 × 3\n  sim_number name  quantile\n  <fct>      <fct>    <dbl>\n1 1          2.5%   -1.59  \n2 1          50%    -0.0726\n3 1          97.5%   1.77  \n\n\nNow let’s take a look with multiple simulations."
  },
  {
    "objectID": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#multiple-simulations",
    "href": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#multiple-simulations",
    "title": "Summary Statistics with {TidyDensity}",
    "section": "Multiple Simulations",
    "text": "Multiple Simulations\nLet’s set our simulation count to 5. While this is not a large amount it will serve as a good illustration on the outputs.\n\nns <- 5\nf  <- quantile\nnr <- TRUE\np  <- c(0.025, 0.975)\n\nOk let’s run the same simulations but with the updated params.\nVector Output BE CAREFUL IT USES SAPPLY\n\nset.seed(123)\ntn <- tidy_normal(.num_sims = ns)\n\ntidy_stat_tbl(\n  .data = tn,\n  .x = y,\n  .return_type = \"vector\",\n  .fns = f,\n  na.rm = nr,\n  probs = p\n  )\n\n      sim_number_1 sim_number_2 sim_number_3 sim_number_4 sim_number_5\n2.5%     -1.591901    -1.474945    -1.656679    -1.258156    -1.309749\n97.5%     1.770747     1.933653     1.894424     2.098923     1.943384\n\ntidy_stat_tbl(\n  tn, y, .return_type = \"vector\",\n  .fns = f, na.rm = nr\n)\n\n     sim_number_1 sim_number_2 sim_number_3 sim_number_4 sim_number_5\n0%    -1.96661716   -2.3091689   -2.0532472  -1.31080153   -1.3598407\n25%   -0.55931702   -0.3612969   -0.9505826  -0.49541417   -0.7140627\n50%   -0.07264039    0.1525789   -0.3048700  -0.07675993   -0.2240352\n75%    0.69817699    0.6294358    0.2900859   0.55145766    0.5287605\n100%   2.16895597    2.1873330    2.1001089   3.24103993    2.1988103\n\n\nList Output with lapply\n\ntidy_stat_tbl(\n  tn, y, f, \"list\", na.rm = nr\n)\n\n$sim_number_1\n         0%         25%         50%         75%        100% \n-1.96661716 -0.55931702 -0.07264039  0.69817699  2.16895597 \n\n$sim_number_2\n        0%        25%        50%        75%       100% \n-2.3091689 -0.3612969  0.1525789  0.6294358  2.1873330 \n\n$sim_number_3\n        0%        25%        50%        75%       100% \n-2.0532472 -0.9505826 -0.3048700  0.2900859  2.1001089 \n\n$sim_number_4\n         0%         25%         50%         75%        100% \n-1.31080153 -0.49541417 -0.07675993  0.55145766  3.24103993 \n\n$sim_number_5\n        0%        25%        50%        75%       100% \n-1.3598407 -0.7140627 -0.2240352  0.5287605  2.1988103 \n\ntidy_stat_tbl(\n  tn, y, f, \"list\", na.rm = nr, \n  probs = p\n)\n\n$sim_number_1\n     2.5%     97.5% \n-1.591901  1.770747 \n\n$sim_number_2\n     2.5%     97.5% \n-1.474945  1.933653 \n\n$sim_number_3\n     2.5%     97.5% \n-1.656679  1.894424 \n\n$sim_number_4\n     2.5%     97.5% \n-1.258156  2.098923 \n\n$sim_number_5\n     2.5%     97.5% \n-1.309749  1.943384 \n\n\nTibble output with tibble\n\ntidy_stat_tbl(\n  tn, y, f, \"tibble\", na.rm = nr\n)\n\n# A tibble: 25 × 3\n   sim_number name        f\n   <fct>      <chr>   <dbl>\n 1 1          0%    -1.97  \n 2 1          25%   -0.559 \n 3 1          50%   -0.0726\n 4 1          75%    0.698 \n 5 1          100%   2.17  \n 6 2          0%    -2.31  \n 7 2          25%   -0.361 \n 8 2          50%    0.153 \n 9 2          75%    0.629 \n10 2          100%   2.19  \n# … with 15 more rows\n\ntidy_stat_tbl(\n  tn, y, f, \"tibble\", na.rm = nr, \n  probs = p\n)\n\n# A tibble: 10 × 3\n   sim_number name      f\n   <fct>      <chr> <dbl>\n 1 1          2.5%  -1.59\n 2 1          97.5%  1.77\n 3 2          2.5%  -1.47\n 4 2          97.5%  1.93\n 5 3          2.5%  -1.66\n 6 3          97.5%  1.89\n 7 4          2.5%  -1.26\n 8 4          97.5%  2.10\n 9 5          2.5%  -1.31\n10 5          97.5%  1.94\n\n\nTibble output with data.table The output object is a tibble but data.table is used to perform the calculations which can be magnitudes faster when simulations are large. I will showcase down the post.\n\ntidy_stat_tbl(\n  tn, y, f, .use_data_table = TRUE, na.rm = nr\n)\n\n# A tibble: 25 × 3\n   sim_number name        f\n   <fct>      <fct>   <dbl>\n 1 1          0%    -1.97  \n 2 1          25%   -0.559 \n 3 1          50%   -0.0726\n 4 1          75%    0.698 \n 5 1          100%   2.17  \n 6 2          0%    -2.31  \n 7 2          25%   -0.361 \n 8 2          50%    0.153 \n 9 2          75%    0.629 \n10 2          100%   2.19  \n# … with 15 more rows\n\ntidy_stat_tbl(\n  tn, y, f, .use_data_table = TRUE, na.rm = nr, \n  probs = p\n)\n\n# A tibble: 10 × 3\n   sim_number name      f\n   <fct>      <fct> <dbl>\n 1 1          2.5%  -1.59\n 2 1          97.5%  1.77\n 3 2          2.5%  -1.47\n 4 2          97.5%  1.93\n 5 3          2.5%  -1.66\n 6 3          97.5%  1.89\n 7 4          2.5%  -1.26\n 8 4          97.5%  2.10\n 9 5          2.5%  -1.31\n10 5          97.5%  1.94\n\n\nOk, now that we have shown that, let’s ratchet up the simulations so we can see the true difference in using the .use_data_tbl parameter when simulations are large. We are going to use {rbenchmark} for"
  },
  {
    "objectID": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#benchmarking",
    "href": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#benchmarking",
    "title": "Summary Statistics with {TidyDensity}",
    "section": "Benchmarking",
    "text": "Benchmarking\nHere we go. We are going to make a tidy_bootstrap() of the mtcars$mpg data which will produce 2000 simulations, we will replicate this 25 times.\n\nlibrary(rbenchmark)\nlibrary(TidyDensity)\nlibrary(dplyr)\n\n# Get the interesting vector, well for this anyways\nx <- mtcars$mpg\n\n# Bootstrap the vector (2k simulations is default)\ntb <- tidy_bootstrap(x) %>%\n  bootstrap_unnest_tbl()\n\nbenchmark(\n  \"tibble\" = {\n    tidy_stat_tbl(tb, y, IQR, \"tibble\")\n  },\n  \"data.table\" = {\n    tidy_stat_tbl(tb, y, IQR, .use_data_table = TRUE, type = 7)\n  },\n  \"sapply\" = {\n    tidy_stat_tbl(tb, y, IQR, \"vector\")\n  },\n  \"lapply\" = {\n    tidy_stat_tbl(tb, y, IQR, \"list\")\n  },\n  replications = 25,\n  columns = c(\"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\"  )\n) %>%\n  arrange(relative)\n\n        test replications elapsed relative user.self sys.self\n1 data.table           25    4.11    1.000      3.33     0.11\n2     lapply           25   24.14    5.873     20.02     0.38\n3     sapply           25   25.11    6.109     21.01     0.28\n4     tibble           25   33.18    8.073     27.45     0.51\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-update-2022-11-18/index.html",
    "href": "posts/weekly-rtip-update-2022-11-18/index.html",
    "title": "Updates to {healthyverse} packages",
    "section": "",
    "text": "I have made several updates to {healthyverse}, this has resulted in new releases to CRAN for {healthyR.ai}, {healthyR.ts}, and {TidyDesnsity}."
  },
  {
    "objectID": "posts/weekly-rtip-update-2022-11-18/index.html#tidydensity-1",
    "href": "posts/weekly-rtip-update-2022-11-18/index.html#tidydensity-1",
    "title": "Updates to {healthyverse} packages",
    "section": "TidyDensity",
    "text": "TidyDensity\nFor TidyDensity a new distribution was added, welcome tidy_bernoulli(). This distribution also comes with the standard util_distname_param_estimate() and the util_distname_stats_tbl() functions. Let’s take a look at the function calls.\n\ntidy_bernoulli(.n = 50, .prob = 0.1, .num_sims = 1)\n\nutil_bernoulli_param_estimate(.x, .auto_gen_empirical = TRUE)\n\nutil_bernoulli_stats_tbl(.data)\n\nLet’s see them in use.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\ntb <- tidy_bernoulli()\n\ntb\n\n# A tibble: 50 × 7\n   sim_number     x     y      dx     dy     p     q\n   <fct>      <int> <int>   <dbl>  <dbl> <dbl> <dbl>\n 1 1              1     0 -0.338  0.0366   0.9     0\n 2 1              2     0 -0.304  0.0866   0.9     0\n 3 1              3     0 -0.270  0.187    0.9     0\n 4 1              4     0 -0.236  0.369    0.9     0\n 5 1              5     0 -0.201  0.663    0.9     0\n 6 1              6     0 -0.167  1.09     0.9     0\n 7 1              7     0 -0.133  1.63     0.9     0\n 8 1              8     1 -0.0988 2.22     1       1\n 9 1              9     0 -0.0646 2.76     0.9     0\n10 1             10     0 -0.0304 3.14     0.9     0\n# … with 40 more rows\n\nutil_bernoulli_param_estimate(tb$y)\n\n$combined_data_tbl\n# A tibble: 100 × 8\n   sim_number     x     y      dx     dy     p     q dist_type\n   <fct>      <int> <dbl>   <dbl>  <dbl> <dbl> <dbl> <fct>    \n 1 1              1     0 -0.338  0.0366  0.92     0 Empirical\n 2 1              2     0 -0.304  0.0866  0.92     0 Empirical\n 3 1              3     0 -0.270  0.187   0.92     0 Empirical\n 4 1              4     0 -0.236  0.369   0.92     0 Empirical\n 5 1              5     0 -0.201  0.663   0.92     0 Empirical\n 6 1              6     0 -0.167  1.09    0.92     0 Empirical\n 7 1              7     0 -0.133  1.63    0.92     0 Empirical\n 8 1              8     1 -0.0988 2.22    1        0 Empirical\n 9 1              9     0 -0.0646 2.76    0.92     0 Empirical\n10 1             10     0 -0.0304 3.14    0.92     0 Empirical\n# … with 90 more rows\n\n$parameter_tbl\n# A tibble: 1 × 8\n  dist_type samp_size   min   max  mean variance sum_x  prob\n  <chr>         <int> <dbl> <dbl> <dbl>    <dbl> <dbl> <dbl>\n1 Bernoulli        50     0     1  0.08   0.0736     4  0.08\n\nutil_bernoulli_stats_tbl(tb) %>%\n  glimpse()\n\nRows: 1\nColumns: 18\n$ tidy_function      <chr> \"tidy_bernoulli\"\n$ function_call      <chr> \"Bernoulli c(0.1)\"\n$ distribution       <chr> \"Bernoulli\"\n$ distribution_type  <chr> \"discrete\"\n$ points             <dbl> 50\n$ simulations        <dbl> 1\n$ mean               <dbl> 0.1\n$ mode               <chr> \"0\"\n$ coeff_var          <dbl> 0.09\n$ skewness           <dbl> 2.666667\n$ kurtosis           <dbl> 5.111111\n$ mad                <dbl> 0.5\n$ entropy            <dbl> 0.325083\n$ fisher_information <dbl> 11.11111\n$ computed_std_skew  <dbl> 3.096281\n$ computed_std_kurt  <dbl> 10.58696\n$ ci_lo              <dbl> 0\n$ ci_hi              <dbl> 1"
  },
  {
    "objectID": "posts/weekly-rtip-update-2022-11-18/index.html#healthyr.ai-1",
    "href": "posts/weekly-rtip-update-2022-11-18/index.html#healthyr.ai-1",
    "title": "Updates to {healthyverse} packages",
    "section": "healthyR.ai",
    "text": "healthyR.ai\nThis was a minor patch release that exported some previously internal only functions and fixed an error with the custom recipe steps. One of the functions that has been exported is hai_data_impute()\nLet’s take a look.\n\nhai_data_impute(\n  .recipe_object = NULL,\n  ...,\n  .seed_value = 123,\n  .type_of_imputation = \"mean\",\n  .number_of_trees = 25,\n  .neighbors = 5,\n  .mean_trim = 0,\n  .roll_statistic,\n  .roll_window = 5\n)\n\nLet’s take a look at an example of it’s use.\n\nlibrary(dplyr)\nlibrary(recipes)\nlibrary(healthyR.ai)\n\ndate_seq <- seq.Date(from = as.Date(\"2013-01-01\"), length.out = 100, by = \"month\")\nval_seq <- rep(c(rnorm(9), NA), times = 10)\ndf_tbl <- tibble(\n  date_col = date_seq,\n  value    = val_seq\n)\n\ndf_tbl\n\n# A tibble: 100 × 2\n   date_col     value\n   <date>       <dbl>\n 1 2013-01-01  0.325 \n 2 2013-02-01 -0.308 \n 3 2013-03-01  0.562 \n 4 2013-04-01  0.0805\n 5 2013-05-01 -0.528 \n 6 2013-06-01 -2.49  \n 7 2013-07-01 -0.964 \n 8 2013-08-01 -0.337 \n 9 2013-09-01 -1.08  \n10 2013-10-01 NA     \n# … with 90 more rows\n\nrec_obj <- recipe(value ~ ., df_tbl)\n\nhai_data_impute(\n  .recipe_object = rec_obj,\n  value,\n  .type_of_imputation = \"roll\",\n  .roll_statistic = median\n)$impute_rec_obj %>%\n  get_juiced_data()\n\n# A tibble: 100 × 2\n   date_col     value\n   <date>       <dbl>\n 1 2013-01-01  0.325 \n 2 2013-02-01 -0.308 \n 3 2013-03-01  0.562 \n 4 2013-04-01  0.0805\n 5 2013-05-01 -0.528 \n 6 2013-06-01 -2.49  \n 7 2013-07-01 -0.964 \n 8 2013-08-01 -0.337 \n 9 2013-09-01 -1.08  \n10 2013-10-01 -0.322 \n# … with 90 more rows"
  },
  {
    "objectID": "posts/weekly-rtip-update-2022-11-18/index.html#healthyr.ts-1",
    "href": "posts/weekly-rtip-update-2022-11-18/index.html#healthyr.ts-1",
    "title": "Updates to {healthyverse} packages",
    "section": "healthyR.ts",
    "text": "healthyR.ts\nThis was a minor patch release fixing the function ts_lag_correlation() when the column that was the value was not explicitly called…value.\nThank you!"
  },
  {
    "objectID": "posts/weekly-tip-tidydensity-2022-11-04/index.html",
    "href": "posts/weekly-tip-tidydensity-2022-11-04/index.html",
    "title": "Bootstrapping and Plots with TidyDensity",
    "section": "",
    "text": "Many times in the real world we have a data set which is actually a sample as we typically do not know what the actual population is. This is where bootstrapping tends to come into play. It allows us to get a hold on what the possible parameter values are by taking repeated samples of the data that is available to us.\nAt it’s core it is a resampling method with replacement where it assigns measures of accuracy to the sample estimates. Here is the Wikipedia Article for bootstrapping.\nIn this post I am going to go over how to use the bootstrap function set with {TidyDensity}. You can find the pkgdown site with all function references here: TidyDensity"
  },
  {
    "objectID": "posts/weekly-tip-tidydensity-2022-11-04/index.html#cumulative-mean",
    "href": "posts/weekly-tip-tidydensity-2022-11-04/index.html#cumulative-mean",
    "title": "Bootstrapping and Plots with TidyDensity",
    "section": "Cumulative Mean",
    "text": "Cumulative Mean\n\ntb %>%\n  bootstrap_stat_plot(.value = y)\n\n\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .show_groups = TRUE)\n\n\n\ntb %>%\n  bootstrap_stat_plot(\n    .value = y,\n    .show_groups = TRUE,\n    .show_ci_labels = FALSE\n  )\n\n\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .interactive = TRUE)\n\n\n\n\n\nYou can see from this output that the statistic you choose is printed in the chart title and on the y axis, the caption will also tell you how many simulations are present. Lets look at skewness as another example."
  },
  {
    "objectID": "posts/weekly-tip-tidydensity-2022-11-04/index.html#cumulative-skewness",
    "href": "posts/weekly-tip-tidydensity-2022-11-04/index.html#cumulative-skewness",
    "title": "Bootstrapping and Plots with TidyDensity",
    "section": "Cumulative Skewness",
    "text": "Cumulative Skewness\n\nsc <- \"cskewness\"\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .stat = sc)\n\n\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .show_groups = TRUE,\n                      .stat = sc)\n\n\n\ntb %>%\n  bootstrap_stat_plot(\n    .value = y,\n    .stat = sc,\n    .show_groups = TRUE,\n    .show_ci_labels = FALSE\n  )\n\n\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .interactive = TRUE,\n                      .show_groups = TRUE,\n                      .stat = sc)\n\n\n\n\n\nVolia!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To Steve On Data",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\nI am using this as a site to host all of the tips and tricks for R/SQL and data that I will post to my LinkedIn, Twitter and Telegram channels."
  },
  {
    "objectID": "posts/rtip-2023-01-26/index.html",
    "href": "posts/rtip-2023-01-26/index.html",
    "title": "Transforming Your Data: A Guide to Popular Methods and How to Implement Them with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nTransforming data refers to the process of changing the scale or distribution of a variable in order to make it more suitable for analysis. There are many different methods for transforming data, and each has its own specific use case.\n\nBox-Cox: This is a method for transforming data that is positively skewed (i.e., has a long tail to the right) into a more normal distribution. It uses a power transformation to adjust the scale of the data.\nBasis Spline: This is a type of non-parametric regression that uses splines (piecewise polynomials) to model the relationship between a dependent variable and one or more independent variables.\nLog: This is a method for transforming data that is positively skewed (i.e., has a long tail to the right) into a more normal distribution. It uses the logarithm function to adjust the scale of the data.\nLogit: This is a method for transforming binary data (i.e., data with only two possible values) into a continuous scale. It uses the logistic function to adjust the scale of the data.\nNatural Spline: This is a type of non-parametric regression that uses splines (piecewise polynomials) to model the relationship between a dependent variable and one or more independent variables, where the splines are chosen to be as smooth as possible.\nRectified Linear Unit (ReLU): This is a type of activation function used in artificial neural networks. It is used to introduce non-linearity in the output of a neuron.\nSquare Root: This is a method for transforming data that is positively skewed (i.e., has a long tail to the right) into a more normal distribution. It uses the square root function to adjust the scale of the data.\nYeo-Johnson: This is a power transformation that works well for data that is positively or negatively skewed. It is a generalization of the Box-Cox transformation and handles zero and negative data.\n\nThe R library {healthyR.ai} provides a function called hai_data_transform() that allows users to easily apply any of these transforms to their data. The function takes in the data and the type of transformation as arguments, and returns the transformed data. This makes it easy for users to experiment with different transformations and see which one works best for their data.\n\n\nFunction\nLet’s take a look at the full function call.\n\nhai_data_transform(\n  .recipe_object = NULL,\n  ...,\n  .type_of_scale = \"log\",\n  .bc_limits = c(-5, 5),\n  .bc_num_unique = 5,\n  .bs_deg_free = NULL,\n  .bs_degree = 3,\n  .log_base = exp(1),\n  .log_offset = 0,\n  .logit_offset = 0,\n  .ns_deg_free = 2,\n  .rel_shift = 0,\n  .rel_reverse = FALSE,\n  .rel_smooth = FALSE,\n  .yj_limits = c(-5, 5),\n  .yj_num_unique = 5\n)\n\nNow let’s go over the arguments to the parameters.\n\n.recipe_object - The data that you want to process\n... - One or more selector functions to choose variables to be imputed. When used with imp_vars, these dots indicate which variables are used to predict the missing data in each variable. See selections() for more details\n.type_of_scale - This is a quoted argument and can be one of the following:\n\n“boxcox”\n“bs”\n“log”\n“logit”\n“ns”\n“relu”\n“sqrt”\n“yeojohnson\n\n.bc_limits - A length 2 numeric vector defining the range to compute the transformation parameter lambda.\n.bc_num_unique - An integer to specify minimum required unique values to evaluate for a transformation\n.bs_deg_free - The degrees of freedom for the spline. As the degrees of freedom for a spline increase, more flexible and complex curves can be generated. When a single degree of freedom is used, the result is a rescaled version of the original data.\n.bs_degree - Degree of polynomial spline (integer).\n.log_base - A numeric value for the base.\n.log_offset - An optional value to add to the data prior to logging (to avoid log(0))\n.logit_offset - A numeric value to modify values of the columns that are either one or zero. They are modifed to be x - offset or offset respectively.\n.ns_deg_free - The degrees of freedom for the natural spline. As the degrees of freedom for a natural spline increase, more flexible and complex curves can be generated. When a single degree of freedom is used, the result is a rescaled version of the original data.\n.rel_shift - A numeric value dictating a translation to apply to the data.\n.rel_reverse - A logical to indicate if the left hinge should be used as opposed to the right hinge.\n.rel_smooth - A logical indicating if hte softplus function, a smooth approximation to the rectified linear transformation, should be used.\n.yj_limits - A length 2 numeric vector defining the range to compute the transformation parameter lambda.\n.yj_num_unique - An integer where data that have less possible values will not be evaluated for a transformation.\n\n\n\nExamples\nLet’s look over some examples. For an example data set we are going to pick on the mtcars data set as the histogram will prove to be skewed which makes it a good candidate to test these transformations on.\n\ninstall.packages(\"healthyR.ai\")\n\nNow that we have {healthyR.ai} installed we can get to work. It does use the {recipes} package underneath so you will need to have that installed as well. Let’s look at the histogram of mtcars now.\n\nmpg_vec <- mtcars$mpg\n\nhist(mpg_vec)\n\n\n\nplot(density(mpg_vec))\n\n\n\n\nFirst up, Box-Cox\n\nlibrary(healthyR.ai)\nlibrary(recipes)\n\nro <- recipe(mpg ~ wt, data = mtcars)\n\nboxcox_vec <- hai_data_transform(\n  .recipe_object = ro,\n  mpg,\n  .type_of_scale = \"boxcox\"\n)$scale_rec_obj %>%\n  get_juiced_data() %>%\n  pull(mpg)\n\nplot(density(boxcox_vec))\n\n\n\n\nBasis Spline\n\nbs_vec <- hai_data_transform(\n  .recipe_object = ro,\n  mpg,\n  .type_of_scale = \"bs\"\n)$scale_rec_obj %>%\n  get_juiced_data()\n\nplot(density(bs_vec$mpg_bs_1))\n\n\n\nplot(density(bs_vec$mpg_bs_2))\n\n\n\nplot(density(bs_vec$mpg_bs_3))\n\n\n\n\nLog\n\nlog_vec <- hai_data_transform(\n  .recipe_object = ro,\n  mpg,\n  .type_of_scale = \"log\"\n)$scale_rec_obj %>%\n  get_juiced_data() %>%\n  pull(mpg)\n\nplot(density(log_vec))\n\n\n\n\nYeo-Johnson\n\nyj_vec <- hai_data_transform(\n  .recipe_object = ro,\n  mpg,\n  .type_of_scale = \"yeojohnson\"\n)$scale_rec_obj %>%\n  get_juiced_data() %>%\n  pull(mpg)\n\nplot(density(yj_vec))\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-healthyr-2023-01-27/index.html",
    "href": "posts/weekly-rtip-healthyr-2023-01-27/index.html",
    "title": "Service Line Grouping with {healthyR}",
    "section": "",
    "text": "Introduction\nHealthcare data analysis can be a complex and time-consuming task, but it doesn’t have to be. Meet {healthyR}, your new go-to R package for all things healthcare data analysis. With {healthyR}, you can easily and efficiently analyze your healthcare data, and make sense of the information it contains.\nOne of the key features of {healthyR} is the service_line_augment() function. This function is designed to help you quickly and easily append a vector to a data.frame or tibble that is passed to the .data parameter. In order to use this function, you will need a data.frame or tibble with a principal diagnosis column, a principal procedure column, and a column for the DRG number. These are needed so that the function can join the dx_cc_mapping and px_cc_mapping columns to provide the service line.\nThe service_line_augment() function is especially useful for analyzing healthcare data that is coded using ICD Version 10. This version of the ICD coding system is widely used in the healthcare industry, and the service_line_augment() function is specifically designed to work with it. With this function, you can quickly and easily append a vector to your data.frame or tibble that provides the service line for each visit.\nIn addition to the service_line_augment() function, {healthyR} also includes a wide range of other useful tools and functions for healthcare data analysis. Whether you’re looking to analyze claims data, clinical data, or any other type of healthcare data, {healthyR} has you covered.\nSo why wait? Download {healthyR} today and start making sense of your healthcare data! With {healthyR}, you can easily and efficiently analyze your healthcare data, and make sense of the information it contains.\n\n\nFunction\nLet’s take a look at the full function call.\n\nservice_line_augment(.data, .dx_col, .px_col, .drg_col)\n\nNow let’s look at the arguments to the parameters.\n\n.data - The data being passed that will be augmented by the function.\n.dx_col - The column containing the Principal Diagnosis for the discharge.\n.px_col - The column containing the Principal Coded Procedure for the discharge. It is possible that this could be blank.\n.drg_col - The DRG Number coded to the inpatient discharge.\n\nNow for some examples.\n\n\nExample\nFirst if you have not already, install {healthyR}\n\ninstall.packages(\"healthyR\")\n\nHere we go.\n\nlibrary(healthyR)\n\ndf <- data.frame(\n  dx_col = \"F10.10\",\n  px_col = NA,\n  drg_col = \"896\"\n)\n\nservice_line_augment(\n  .data = df,\n  .dx_col = dx_col,\n  .px_col = px_col,\n  .drg_col = drg_col\n)\n\n# A tibble: 1 × 4\n  dx_col px_col drg_col service_line \n  <chr>  <lgl>  <chr>   <chr>        \n1 F10.10 NA     896     alcohol_abuse\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-01-30/index.html",
    "href": "posts/rtip-2023-01-30/index.html",
    "title": "{healthyR.ts}: The New and Improved Library for Time Series Analysis",
    "section": "",
    "text": "Introduction\nAre you looking for a powerful and efficient library for time series analysis? Look no further than {healthyR.ts}! This library has recently been updated with new functions and improvements, making it easier for you to analyze and visualize your time series data.\nOne of the new functions in {healthyR.ts} is ts_geometric_brownian_motion(). This function allows you to generate multiple Brownian motion simulations at once, saving you time and effort. With this feature, you can easily generate multiple simulations to compare and analyze different scenarios.\nAnother new function, ts_brownian_motion_augment(), enables you to add a Brownian motion to a time series that you provide. This is a great tool for analyzing the impact of random variations on your data.\nThe ts_geometric_brownian_motion_augment() function generates a geometric Brownian motion, allowing you to study the effects of compounding growth or decay in your time series data. And, with the ts_brownian_motion_plot() function, you can easily plot both augmented and non-augmented Brownian motion plots, giving you a visual representation of your data.\nIn addition to the new functions, {healthyR.ts} has also made several minor fixes and improvements. For example, the ts_brownian_motion() function has been updated and optimized, resulting in a 49x speedup due to vectorization. Additionally, all Brownian motion functions now have an attribute of .motion_type, making it easier to track and identify your data.\nWith all of these new features and improvements, {healthyR.ts} is the ideal library for anyone looking to analyze and visualize time series data. So, if you want to take your time series analysis to the next level, install {healthyR.ts} today!\n\n\nFunction\nLet’s take a look at the new functions.\n\nts_geometric_brownian_motion(\n  .num_sims = 100,\n  .time = 25,\n  .mean = 0,\n  .sigma = 0.1,\n  .initial_value = 100,\n  .delta_time = 1/365,\n  .return_tibble = TRUE\n)\n\nIts arguments.\n\n.num_sims - Total number of simulations.\n.time - Total time of the simulation.\n.mean - Expected return\n.sigma - Volatility\n.initial_value - Integer representing the initial value.\n.delta_time - Time step size.\n.return_tibble - The default is TRUE. If set to FALSE then an object of class matrix will be returned.\n\n\nts_brownian_motion_augment(\n  .data,\n  .date_col,\n  .value_col,\n  .time = 100,\n  .num_sims = 10,\n  .delta_time = NULL\n)\n\nIts arguments.\n\n.data - The data.frame/tibble being augmented.\n.date_col - The column that holds the date.\n.value_col - The value that is going to get augmented. The last value of this column becomes the initial value internally.\n.time - How many time steps ahead.\n.num_sims - How many simulations should be run.\n.delta_time - Time step size.\n\n\nts_geometric_brownian_motion_augment(\n  .data,\n  .date_col,\n  .value_col,\n  .num_sims = 10,\n  .time = 25,\n  .mean = 0,\n  .sigma = 0.1,\n  .delta_time = 1/365\n)\n\nIts arguments.\n\n.data - The data you are going to pass to the function to augment.\n.date_col - The column that holds the date\n.value_col - The column that holds the value\n.num_sims - Total number of simulations.\n.time - Total time of the simulation.\n.mean - Expected return\n.sigma - Volatility\n.delta_time - Time step size.\n\n\nts_brownian_motion_plot(\n  .data, \n  .date_col, \n  .value_col, \n  .interactive = FALSE\n)\n\nIts arguments.\n\n.data - The data you are going to pass to the function to augment.\n.date_col - The column that holds the date\n.value_col - The column that holds the value\n.interactive - The default is FALSE, TRUE will produce an interactive plotly plot.\n\n\n\nExamples\nFirst make sure you install {healthyR.ts} if you do not yet already have it, otherwise update it to gain th enew functionality.\n\ninstall.packages(\"healthyR.ts\")\n\nNow let’s take a look at ts_geometric_brownian_motion().\n\nlibrary(healthyR.ts)\n\nts_geometric_brownian_motion()\n\n# A tibble: 2,600 × 3\n   sim_number        t     y\n   <fct>         <int> <dbl>\n 1 sim_number 1      1   100\n 2 sim_number 2      1   100\n 3 sim_number 3      1   100\n 4 sim_number 4      1   100\n 5 sim_number 5      1   100\n 6 sim_number 6      1   100\n 7 sim_number 7      1   100\n 8 sim_number 8      1   100\n 9 sim_number 9      1   100\n10 sim_number 10     1   100\n# … with 2,590 more rows\n\n\nNow let’s take a look at ts_brownian_motion_augment().\n\nrn <- rnorm(31)\ndf <- data.frame(\ndate_col = seq.Date(from = as.Date(\"2022-01-01\"),\n                      to = as.Date(\"2022-01-31\"),\n                      by = \"day\"),\n value = rn\n)\n\nts_brownian_motion_augment(\n  .data = df,\n  .date_col = date_col,\n  .value_col = value\n)\n\n# A tibble: 1,041 × 3\n   sim_number  date_col    value\n   <fct>       <date>      <dbl>\n 1 actual_data 2022-01-01 -0.303\n 2 actual_data 2022-01-02 -1.17 \n 3 actual_data 2022-01-03 -1.44 \n 4 actual_data 2022-01-04 -0.682\n 5 actual_data 2022-01-05 -2.31 \n 6 actual_data 2022-01-06 -1.19 \n 7 actual_data 2022-01-07 -0.454\n 8 actual_data 2022-01-08 -1.83 \n 9 actual_data 2022-01-09  0.659\n10 actual_data 2022-01-10 -0.150\n# … with 1,031 more rows\n\n\nNow ts_geometric_brownian_motion_augment().\n\nrn <- rnorm(31)\ndf <- data.frame(\ndate_col = seq.Date(from = as.Date(\"2022-01-01\"),\n                      to = as.Date(\"2022-01-31\"),\n                      by = \"day\"),\n value = rn\n)\n\nts_geometric_brownian_motion_augment(\n  .data = df,\n  .date_col = date_col,\n  .value_col = value\n)\n\n# A tibble: 291 × 3\n   sim_number  date_col    value\n   <fct>       <date>      <dbl>\n 1 actual_data 2022-01-01 -1.47 \n 2 actual_data 2022-01-02 -1.63 \n 3 actual_data 2022-01-03  1.01 \n 4 actual_data 2022-01-04  1.44 \n 5 actual_data 2022-01-05 -1.05 \n 6 actual_data 2022-01-06 -0.599\n 7 actual_data 2022-01-07 -0.393\n 8 actual_data 2022-01-08  1.06 \n 9 actual_data 2022-01-09 -0.121\n10 actual_data 2022-01-10 -0.349\n# … with 281 more rows\n\n\nNow for ts_brownian_motion_plot().\n\nts_geometric_brownian_motion() %>%\n  ts_brownian_motion_plot(.date_col = t, .value_col = y)\n\n\n\n\n\nts_brownian_motion() %>%\n  ts_brownian_motion_plot(t, y, .interactive = TRUE)\n\n\n\n\n\nAnd with the augmenting functions\n\nrn <- rnorm(31)\ndf <- data.frame(\ndate_col = seq.Date(from = as.Date(\"2022-01-01\"),\n                      to = as.Date(\"2022-01-31\"),\n                      by = \"day\"),\n value = rn\n)\n\nts_brownian_motion_augment(\n  .data = df,\n  .date_col = date_col,\n  .value_col = value,\n  .time = 30,\n  .num_sims = 30\n) %>%\n  ts_brownian_motion_plot(date_col, value, TRUE)\n\n\n\n\n\nAnd with a static ggplot2 plot.\n\nrn <- rnorm(31)\ndf <- data.frame(\ndate_col = seq.Date(from = as.Date(\"2022-01-01\"),\n                      to = as.Date(\"2022-01-31\"),\n                      by = \"day\"),\n value = rn\n)\n\nts_brownian_motion_augment(\n  .data = df,\n  .date_col = date_col,\n  .value_col = value,\n  .time = 30,\n  .num_sims = 30\n) %>%\n  ts_brownian_motion_plot(date_col, value)\n\n\n\n\nThank you for reading, and Voila!"
  },
  {
    "objectID": "posts/rtip-2023-01-31/index.html",
    "href": "posts/rtip-2023-01-31/index.html",
    "title": "Median: A Simple Way to Detect Excess Events Over Time with {healthyR}",
    "section": "",
    "text": "Introduction\nAs we collect data over time, it’s important to look for patterns and trends that can help us understand what’s happening. One common way to do this is to look at the median of the data. The median is the middle value of a set of numbers, and it can be a useful tool for detecting whether there is an excess of events, either positive or negative, occurring over time.\nBenefits of Looking at Median:\n\nShows the central tendency: The median gives us a good idea of the central tendency of the data. This can help us understand what’s typical and what’s not.\nResistant to outliers: Unlike the mean, the median is not affected by outliers. This means that if there are a few extreme values in the data, the median will not be skewed by them.\nEasy to understand: The median is easy to understand, even for people who are not familiar with statistics.\n\nUsing the R Library {healthyR} provides a convenient way to perform median analysis. The function ts_median_excess_plt() can be used to plot the median of an event over time and detect any excess events that may be occurring. This function is designed to be user-friendly, so even if you’re not an expert in statistics, you can still use it to gain valuable insights into your data.\nIn conclusion, looking at the median of an event over time can be a useful tool for detecting excess events, either positive or negative. The R library {healthyR} provides a convenient way to perform this analysis with the function ts_median_excess_plt(). Give it a try and see what insights you can uncover in your own data!\n\n\nFunction\nHere is the full function call.\n\nts_median_excess_plt(\n  .data,\n  .date_col,\n  .value_col,\n  .x_axis,\n  .ggplot_group_var,\n  .years_back\n)\n\nHere are its arguments.\n\n.data - The data that is being analyzed, data must be a tibble/data.frame.\n.date_col - The column of the tibble that holds the date.\n.value_col - The column that holds the value of interest.\n.x_axis - What is the be the x-axis, day, week, etc.\n.ggplot_group_var - The variable to group the ggplot on.\n.years_back - How many yeas back do you want to go in order to compute the median value.\n\n\n\nExample\nFirst make sure you have the package installed.\n\ninstall.packages(\"healthyR\")\n\nNow for an example. The data is required to be in a certain format, this function is dated, meaning it was one of the first ones I wrote so I will be taking time to improve it in the future. We are using data from my {healthyR.data]} package.\n\nlibrary(healthyR.data)\nlibrary(lubridate)\nlibrary(healthyR)\nlibrary(dplyr)\nlibrary(timetk)\nlibrary(ggplot2)\n\ndf <- healthyR_data %>%\n  filter_by_time(\n    .date_var = visit_start_date_time,\n    .start_date = \"2012\",\n    .end_date = \"2019\"\n  ) %>%\n  filter(ip_op_flag == \"I\") %>%\n  select(visit_id, visit_start_date_time) %>%\n  mutate(\n    visit_start_date_time = as.Date(visit_start_date_time, \"%Y%M%D\"),\n    record = 1\n    ) %>%\n  summarise_by_time(\n    .date_var = visit_start_date_time,\n    visits = sum(record)\n  ) %>%\n  ts_signature_tbl(\n    .date_col = visit_start_date_time\n  )\n\nOk now that we have our data, let’s take a look at it using glimpse()\n\nglimpse(df)\n\nRows: 2,922\nColumns: 30\n$ visit_start_date_time <date> 2012-01-01, 2012-01-02, 2012-01-03, 2012-01-04,…\n$ visits                <dbl> 34, 52, 53, 44, 46, 55, 42, 29, 50, 55, 50, 43, …\n$ index.num             <dbl> 1325376000, 1325462400, 1325548800, 1325635200, …\n$ diff                  <dbl> NA, 86400, 86400, 86400, 86400, 86400, 86400, 86…\n$ year                  <int> 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, …\n$ year.iso              <int> 2011, 2012, 2012, 2012, 2012, 2012, 2012, 2012, …\n$ half                  <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ quarter               <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ month                 <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ month.xts             <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ month.lbl             <ord> January, January, January, January, January, Jan…\n$ day                   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ hour                  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ minute                <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ second                <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ hour12                <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ am.pm                 <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ wday                  <int> 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, …\n$ wday.xts              <int> 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, …\n$ wday.lbl              <ord> Sunday, Monday, Tuesday, Wednesday, Thursday, Fr…\n$ mday                  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ qday                  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ yday                  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ mweek                 <int> 5, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, …\n$ week                  <int> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, …\n$ week.iso              <int> 52, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3,…\n$ week2                 <int> 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, …\n$ week3                 <int> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, …\n$ week4                 <int> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, …\n$ mday7                 <int> 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, …\n\n\nNow to visualize it.\n\ndf %>%\n  ts_median_excess_plt(\n    .date_col = visit_start_date_time,\n    .value_col = visits,\n    .x_axis = month.lbl,\n    .ggplot_group_var = year,\n    .years_back = 3\n  ) +\n  labs(\n    y = \"Excess Visits\",\n    title = \"Excess Visits by Month YoY\"\n  ) + \n  theme(axis.text.x=element_text(angle = -90, hjust = 0))\n\n\n\n\nSo from here what we can see is that looking back in time over the visits data that the current year (the max year in the data) shows that it is significantly under previous years median values by month.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-01/index.html",
    "href": "posts/rtip-2023-02-01/index.html",
    "title": "Attributes in R Functions: An Overview",
    "section": "",
    "text": "R is a powerful programming language that is widely used for data analysis, visualization, and machine learning. One of the features of R that makes it versatile and flexible is the ability to assign attributes to functions. Attributes are metadata associated with an object in R, and they can be used to store additional information about the function or to modify the behavior of the function.\nIn this blog post, we will discuss what attributes are, how they can be useful, and how they can be used inside R functions.\n\n\nAttributes are pieces of information that are stored alongside an object in R. Functions are objects in R, and they can have attributes associated with them. Some of the common attributes associated with functions in R include:\n\nformals: This attribute stores the arguments of the function and their default values.\nsrcref: This attribute stores the source code of the function, including the line numbers of the code.\nenvironment: This attribute stores the environment in which the function was defined.\n\n\n\n\nAttributes can be useful in R functions in several ways, including:\n\nDebugging: Attributes can be used to store information that can be used to debug functions. For example, the srcref attribute can be used to retrieve the source code of the function and the line numbers of the code, which can be useful when trying to identify the source of an error.\nMetadata: Attributes can be used to store metadata about the function, such as the author, version, and date of creation. This information can be used to keep track of the function and to provide information about its purpose and usage.\nModifying Function Behavior: Attributes can be used to modify the behavior of the function. For example, the environment attribute can be used to set the environment in which the function is executed. This can be useful when creating closures or when using functions in a specific context.\n\n\n\n\nTo access or modify the attributes of a function in R, you can use the attributes() function. For example, to retrieve the formals attribute of a function, you can use the following code:\n\nf <- function(x, y) { x + y }\nattributes(f)\n\n$srcref\nfunction(x, y) { x + y }\n\nformals(f)\n\n$x\n\n\n$y\n\n\nTo add an attribute to a function, you can use the attr() function. For example, to add a version attribute to a function, you can use the following code:\n\nf <- function(x, y) { x + y }\nattr(f, \"version\") <- \"1.0\"\nattributes(f)\n\n$srcref\nfunction(x, y) { x + y }\n\n$version\n[1] \"1.0\"\n\n\nTo remove an attribute from a function, you can use the attributes() function with the NULL value. For example, to remove the version attribute from a function, you can use the following code:\n\nf <- function(x, y) { x + y }\nattr(f, \"version\") <- \"1.0\"\nattributes(f)$version <- NULL\nattributes(f)\n\n$srcref\nfunction(x, y) { x + y }\n\n\nConclusion\nAttributes are a useful feature in R functions that can be used to store additional information about the function, to debug the function, and to modify its behavior. By using attributes, you can make your functions more versatile, flexible, and easier to work with."
  },
  {
    "objectID": "posts/rtip-2023-02-02/index.html",
    "href": "posts/rtip-2023-02-02/index.html",
    "title": "Diverging Lollipop Chart: A Visual Tool for Comparing Data with {healthyR}",
    "section": "",
    "text": "Introduction\nA diverging lollipop chart is a useful tool for comparing data that falls into two categories, usually indicated by different colors. This type of chart is particularly well-suited for comparing the differences between two data sets and for identifying which data points are contributing most to the differences.\nThe R package {healthyR} offers a function called diverging_lollipop_plt() that can be used to create a diverging lollipop chart. This function has several parameters that can be used to customize the chart to meet your specific needs.\nIn conclusion, the diverging lollipop chart is a useful tool for comparing data sets and can provide insights into the differences between two sets of data. The diverging_lollipop_plt() function from the {healthyR} package is a great option for creating this type of chart, as it offers a range of customization options to meet your specific needs. Whether you’re working with data related to business, finance, or any other field, a diverging lollipop chart can be a valuable tool in your visual analysis toolkit.\n\n\nFunction\nLet’s take a look at the full function call.\n\ndiverging_lollipop_plt(\n  .data,\n  .x_axis,\n  .y_axis,\n  .plot_title = NULL,\n  .plot_subtitle = NULL,\n  .plot_caption = NULL,\n  .interactive = FALSE\n)\n\nNow lets see the arguments that get provided to the parameters.\n\n.data - The data to pass to the function, must be a tibble/data.frame.\n.x_axis - The data that is passed to the x-axis. This will also be the x and xend parameters of the geom_segment\n.y_axis - The data that is passed to the y-axis. This will also equal the parameters of yend and label\n.plot_title - Default is NULL\n.plot_subtitle - Default is NULL\n.plot_caption - Default is NULL\n.interactive - Default is FALSE. TRUE returns a plotly plot\n\n\n\nExample\nLet’s see an example.\n\nlibrary(healthyR)\n\nsuppressPackageStartupMessages(library(ggplot2))\n\ndata(\"mtcars\")\nmtcars$car_name <- rownames(mtcars)\nmtcars$mpg_z <- round((mtcars$mpg - mean(mtcars$mpg))/sd(mtcars$mpg), 2)\nmtcars$mpg_type <- ifelse(mtcars$mpg_z < 0, \"below\", \"above\")\nmtcars <- mtcars[order(mtcars$mpg_z), ]  # sort\nmtcars$car_name <- factor(mtcars$car_name, levels = mtcars$car_name)\n\ndiverging_lollipop_plt(\n  .data = mtcars, \n  .x_axis = car_name,\n  .y_axis = mpg_z\n)\n\n\n\n\nNow let’s also see the interactive chart.\n\ndiverging_lollipop_plt(\n  .data = mtcars, \n  .x_axis = car_name,\n  .y_axis = mpg_z,\n  .interactive = TRUE\n)\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-03/index.html",
    "href": "posts/rtip-2023-02-03/index.html",
    "title": "The Argument Matcher: A Function for Selecting the Right Arguments {tidyAML}",
    "section": "",
    "text": "Introduction\nI am working on finishing up a few things with my new R package {tidyAML} before I release it to CRAN. One of those things is the ability of a user to build a model using a command that might be something like generate_model(). One of the things that is necessary to do is to match the function arguments from the generate_model() to the actual parsnip call.\nThis is where and argument matcher of sorts may come in handy. I am doing this because it will take one most step of abstraction away, and instead of say calling linear_reg() or mars() or something like that, you can just instead use generate_model() and type in your engine or the parsnip function call there.\nNow I am not one hundred percent certain that I’ll actually implement this or not, but the exercise was fun enough that I decided to share it. So let’s get into it.\n\n\nFunction\nHere is the current state of the function.\n\nargument_matcher <- function(.f = \"linear_reg\", .args = list()){\n  \n  # TidyEval ----\n  fns <- as.character(.f)\n  \n  fns_args <- formalArgs(fns)\n  fns_args_list <- as.list(fns_args)\n  names(fns_args_list) <- fns_args\n  \n  arg_list <- .args\n  arg_list_names <- unique(names(arg_list))\n  \n  l <- list(arg_list, fns_args_list)\n  \n  arg_idx <- which(arg_list_names %in% fns_args_list)\n  bad_arg_idx <- which(!arg_list_names %in% fns_args_list)\n  \n  bad_args <- arg_list[bad_arg_idx]\n  bad_arg_names <- unique(names(bad_args))\n  \n  final_args <- arg_list[arg_idx]\n  \n  # Return ----\n  if (length(bad_arg_names > 0)){\n    rlang::inform(\n      message = paste0(\"bad arguments passed: \", bad_arg_names),\n      use_cli_format = TRUE\n    )\n  }\n\n  return(final_args)\n}\n\nWhen working with R functions, it’s not uncommon to encounter a situation where you need to pass arguments to another function. This can be especially challenging when the arguments are not properly matched. Fortunately, the argument_matcher function provides an elegant solution to this problem.\nThe argument_matcher function takes two arguments: .f and .args. The .f argument is a string that specifies the name of the function you want to pass arguments to, while the .args argument is a list that contains the arguments you want to pass to the specified function.\nThe argument_matcher function first uses the formalArgs function to extract the formal arguments of the specified function and store them in fns_args. The names of the formal arguments are then used to create a list, fns_args_list.\nNext, the function extracts the names of the arguments in .args and stores them in arg_list_names. It then checks if the names of the arguments in .args match the names of the formal arguments of the specified function, and stores the matching arguments in final_args. Any arguments that don’t match the formal arguments are stored in bad_args, and a warning message is printed indicating that bad arguments were passed.\nThe final step is to return the final_args list, which contains only the arguments that match the formal arguments of the specified function.\nIn conclusion, the argument_matcher function is a useful tool for ensuring that arguments are properly matched when passed to another function. Whether you’re working with linear regression models or any other type of function, the argument_matcher function will help you select the right arguments and avoid common errors.\n\n\nExample\nLet’s see a simple example.\n\nsuppressPackageStartupMessages(library(tidymodels))\n\nargument_matcher(\n  .args = list(\n    mode = \"regression\", \n    engine = \"lm\",\n    cost = 0.5,\n    trees = 1, \n    mtry = 1\n    )\n  )\n\nbad arguments passed: cost\nbad arguments passed: trees\nbad arguments passed: mtry\n\n\n$mode\n[1] \"regression\"\n\n$engine\n[1] \"lm\"\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-06/inex.html",
    "href": "posts/rtip-2023-02-06/inex.html",
    "title": "Cumulative Measurement Functions with {TidyDensity}",
    "section": "",
    "text": "Introduction\nIf you’re looking for an easy-to-use package to calculate cumulative statistics in R, you may want to check out the TidyDensity package. This package offers several functions to calculate cumulative measurements, including mean, median, standard deviation, variance, skewness, kurtosis, harmonic mean, and geometric mean.\n\ncgmean(): Cumulative Geometric Mean\n\nThe cgmean() function calculates the cumulative geometric mean of a set of values. This is the nth root of the product of the first n elements of the set. It’s a useful measurement for sets of values that are multiplied together, such as growth rates.\n\nchmean(): Cumulative Harmonic Mean\n\nThe chmean() function calculates the cumulative harmonic mean of a set of values. This is the inverse of the arithmetic mean of the reciprocals of the values. It’s commonly used for sets of values that represent rates, such as speeds.\n\nckurtosis(): Cumulative Kurtosis\n\nThe ckurtosis() function calculates the cumulative kurtosis of a set of values. Kurtosis is a measure of the peakedness of a distribution, relative to a normal distribution. The cumulative kurtosis calculates the kurtosis of a set of values up to a specific point in the set.\n\ncmean(): Cumulative Mean\n\nThe cmean() function calculates the cumulative mean of a set of values. It’s a measure of the average of the values up to a specific point in the set.\n\ncmedian(): Cumulative Median\n\nThe cmedian() function calculates the cumulative median of a set of values. It’s the value that separates the lower half of the set from the upper half, up to a specific point in the set.\n\ncsd(): Cumulative Standard Deviation\n\nThe csd() function calculates the cumulative standard deviation of a set of values. Standard deviation is a measure of the spread of values in a set. The cumulative standard deviation calculates the standard deviation up to a specific point in the set.\n\ncskewness(): Cumulative Skewness\n\nThe cskewness() function calculates the cumulative skewness of a set of values. Skewness is a measure of the asymmetry of a distribution. The cumulative skewness calculates the skewness up to a specific point in the set.\n\ncvar(): Cumulative Variance\n\nThe cvar() function calculates the cumulative variance of a set of values. Variance is a measure of the spread of values in a set. The cumulative variance calculates the variance up to a specific point in the set.\nIn conclusion, the {TidyDensity} package offers several functions for calculating cumulative statistics, including mean, median, standard deviation, variance, skewness, kurtosis, harmonic mean, and geometric mean. These functions make it easy to calculate cumulative statistics for sets of values in R.\n\n\nFunctions\nAll of the functions perform work strictly on a vector. Because of this I will not go over the function calls separately because they all follow the vectorized for of fun(.x) where .x is the argument passed to the cumulative function.\n\n\nExamples\nHere I will go over some examples of each function use the AirPassengers data set.\n\nlibrary(TidyDensity)\n\nv <- AirPassengers\n\nLet’s start at the top.\nCumulative Geometric Mean:\n\nhead(cgmean(v))\n\n[1] 112.0000 114.9609 120.3810 122.4802 122.1827 124.2311\n\ntail(cgmean(v))\n\n[1] 249.6135 251.1999 252.4577 253.5305 254.2952 255.2328\n\nplot(cgmean(v), type = \"l\")\n\n\n\n\nCumulative Harmonic Mean:\n\nhead(chmean(v))\n\n[1] 112.00000  57.46087  40.03378  30.55222  24.39304  20.66000\n\ntail(chmean(v))\n\n[1] 1.636832 1.632423 1.627194 1.621471 1.614757 1.608744\n\nplot(chmean(v), type = \"l\")\n\n\n\n\nCumulative Kurtosis:\n\nhead(ckurtosis(v))\n\n[1]      NaN 1.000000 1.500000 1.315839 1.597316 1.597850\n\ntail(ckurtosis(v))\n\n[1] 2.668951 2.795314 2.733117 2.674195 2.649894 2.606228\n\nplot(ckurtosis(v), type = \"l\")\n\n\n\n\nCumulative Mean:\n\nhead(cmean(v))\n\n[1] 112.0000 115.0000 120.6667 122.7500 122.4000 124.5000\n\ntail(cmean(v))\n\n[1] 273.1367 275.5143 277.1631 278.4577 279.2378 280.2986\n\nplot(cmean(v), type = \"l\")\n\n\n\n\nCumulative Median:\n\nhead(cmedian(v))\n\n[1] 112.0 115.0 118.0 123.5 121.0 125.0\n\ntail(cmedian(v))\n\n[1] 259.0 261.5 264.0 264.0 264.0 265.5\n\nplot(cmedian(v), type = \"l\")\n\n\n\n\nCumulative Standard Deviation:\n\nhead(csd(v))\n\n[1]        NA  4.242641 10.263203  9.358597  8.142481  8.916277\n\ntail(csd(v))\n\n[1] 115.0074 117.9956 119.1924 119.7668 119.7083 119.9663\n\nplot(csd(v), type = \"l\")\n\n\n\n\nCumulative Skewness:\n\nhead(cskewness(v))\n\n[1]         NaN  0.00000000  0.44510927 -0.14739157 -0.02100016 -0.18544758\n\ntail(cskewness(v))\n\n[1] 0.5936970 0.6471651 0.6349071 0.6145579 0.5972102 0.5770682\n\nplot(cskewness(v), type = \"l\")\n\n\n\n\nCumulative Variance:\n\nhead(cvar(v))\n\n[1]        NA  18.00000 105.33333  87.58333  66.30000  79.50000\n\ntail(cvar(v))\n\n[1] 13226.70 13922.96 14206.84 14344.08 14330.07 14391.92\n\nplot(cvar(v), type = \"l\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-07/index.html",
    "href": "posts/rtip-2023-02-07/index.html",
    "title": "Subsetting Named Lists in R",
    "section": "",
    "text": "Introduction\nIn R, lists are a fundamental data structure that allows us to store multiple objects of different data types under a single name. Often times, we want to extract certain elements of a list based on their names, and this can be accomplished through the use of the subset function. In this blog post, we will take a look at how to use the grep function to subset named lists in R.\nFirst, we will create a list object as follows:\n\nasc_list <- list(\n  Facility = 1:10,\n  State = 11:20,\n  National = 21:30\n)\n\nWe now have a list with three elements, each with a different name. Next, we want to make sure that our list does not contain any 0 length items. This can be achieved by using the lapply function and the length function:\n\nasc_list <- asc_list[lapply(asc_list, length) > 0]\n\nThe lapply function applies the length function to each element of the list, and returns a logical vector indicating whether each element is of length greater than 0. By using the square bracket operator, we can extract only those elements for which the logical value is TRUE.\nNext, we create a character vector of possible items that we want to match on:\n\npatterns <- c(\"state\",\"faci\")\n\nWe can now pass this vector of patterns to the grep function, along with the names of our list and the ignore.case argument set to TRUE. The grep function returns the indices of the elements in our list that match the given pattern:\n\nasc_list[grep(\n  paste(patterns, collapse = \"|\"),\n  names(asc_list),\n  ignore.case = TRUE\n  )]\n\n$Facility\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$State\n [1] 11 12 13 14 15 16 17 18 19 20\n\n\nThe result of this code is a new list that contains only the elements of our original list whose names match either “state” or “faci”. The paste function is used to join the patterns in the vector into a single string, with the | character separating each pattern. This allows us to search for multiple patterns at once.\nIn conclusion, the grep function is a powerful tool for sub-setting named lists in R, especially when we have multiple patterns that we want to match on. By combining the grep function with other R functions such as lapply and length, we can extract specific elements from our lists with ease."
  },
  {
    "objectID": "posts/rtip-2023-02-08/index.html",
    "href": "posts/rtip-2023-02-08/index.html",
    "title": "Creating an R Project Directory",
    "section": "",
    "text": "Introduction\nWhen working in R I find it best to create a new project when working on something. This keeps all of the data and scripts in one location. This also means that if you are not careful the directory you have your project in can become quite messy. This used to happen to me with regularity, then I got smart and wrote a script that would standardize how projects are built for me.\nI find it important to have different fodlers for different parts of a project. This does not mean I will use them all for every project but that is fine, you can either comment that portion out or just delete the files that are created.\n\n\nFunction\nHere is what I do broken down into different steps. First, I see if the package {fs} is installed, and if not, then install it, and finally load it.\n\nif(!require(fs)) {\n  install.packages(\"fs\")\n}\nsuppressPackageStartupMessages(library(fs))\n\nNext we create a character vector of folder paths that will exist inside of the main project folder itself.\n\nfolders <- c(\n  \"00_Scripts\"\n  , \"00_Data\"\n  , \"01_Queries\"\n  , \"02_Data_Manipulation\"\n  , \"03_Viz\"\n  , \"04_TS_Modeling\"\n  , \"99_Automations\"\n)\n\nNow that the folders we want are spelt out, we can create them.\n\nfs::dir_create(\n  path = folders\n)\n\nNow that is done, it’s off to creating a few files that I personally almost always use. I do a lot of work out of a data warehouse so a connection file is needed. We also need a disconnection function.\n\n# DSS Connection \ndb_connect <- function() {\n  db_con <- LICHospitalR::db_connect()\n  \n  return(db_con)\n  \n}\n\n# Disconnect from Database\ndb_disconnect <- function(.connection) {\n  \n  DBI::dbDisconnect(\n    conn = db_connect()\n  )\n  \n}\n\nNow, let’s load in the typical libraries. You can modify this to suit your own needs.\n\n# Library Load\n\nlibrary_load <- function(){\n  \n  if(!require(pacman)){install.packages(\"pacman\")}\n  pacman::p_load(\n    \"DBI\"\n    , \"odbc\"\n    , \"janitor\"\n    , \"dplyr\"\n    , \"tibble\"\n    , \"tidyr\"\n    , \"LICHospitalR\"\n    , \"modeltime\"\n  )\n  \n}\n\nOk so now the functions have been created, let’s dump them!\n\ndb_funs <- c(\"db_connect\",\"db_disconnect\")\ndump(\n  list = db_funs,\n  file = \"00_Scripts/db_con_obj.R\"\n)\n\nlib_funs <- \"library_load\"\ndump(\n  list = lib_funs,\n  file = \"00_Scripts/library_load.R\"\n)\n\n\n\nExample\nHere is the full script!\n\nif(!require(fs)) {\n  install.packages(\"fs\")\n}\nsuppressPackageStartupMessages(library(fs))\n\nfolders <- c(\n  \"00_Scripts\"\n  , \"00_Data\"\n  , \"01_Queries\"\n  , \"02_Data_Manipulation\"\n  , \"03_Viz\"\n  , \"04_TS_Modeling\"\n  , \"99_Automations\"\n)\n\nfs::dir_create(\n  path = folders\n)\n\n\nfile_create(\"01_Queries/query_functions.R\")\nfile_create(\"02_Data_Manipulation/data_functions.R\")\nfile_create(\"03_Viz/viz_functions.R\")\nfile_create(\"04_TS_Modeling/ts_functions.R\")\n\n# DSS Connection \ndb_connect <- function() {\n  db_con <- LICHospitalR::db_connect()\n  \n  return(db_con)\n  \n}\n\n# Disconnect from Database\ndb_disconnect <- function(.connection) {\n  \n  DBI::dbDisconnect(\n    conn = db_connect()\n  )\n  \n}\n\n# Library Load\n\nlibrary_load <- function(){\n  \n  if(!require(pacman)){install.packages(\"pacman\")}\n  pacman::p_load(\n    \"DBI\"\n    , \"odbc\"\n    , \"janitor\"\n    , \"dplyr\"\n    , \"tibble\"\n    , \"tidyr\"\n    , \"LICHospitalR\"\n    , \"modeltime\"\n  )\n  \n}\n\ndb_funs <- c(\"db_connect\",\"db_disconnect\")\ndump(\n  list = db_funs,\n  file = \"00_Scripts/db_con_obj.R\"\n)\n\nlib_funs <- \"library_load\"\ndump(\n  list = lib_funs,\n  file = \"00_Scripts/library_load.R\"\n)\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-09/index.html",
    "href": "posts/rtip-2023-02-09/index.html",
    "title": "Creating and Predicting Fast Regression Parsnip Models with {tidyAML}",
    "section": "",
    "text": "Introduction\nI am almost ready for a first release of my R package {tidyAML}. The purpose of this is to act as a way of quickly generating models using the parsnip package and keeping things inside of the tidymodels framework allowing users to seamlessly create models in tidyAML but pluck and move them over to tidymodels should they prefer. This is because I believe that software should be interchangeable and work well with other libraries. Today I am going to showcase how the function fast_regression()\n\n\nFunction\nLet’s take a look at the function.\n\nfast_regression(\n  .data,\n  .rec_obj,\n  .parsnip_fns = \"all\",\n  .parsnip_eng = \"all\",\n  .split_type = \"initial_split\",\n  .split_args = NULL\n)\n\nHere are the arguments to the function:\n\n.data - The data being passed to the function for the regression problem\n.rec_obj - The recipe object being passed.\n.parsnip_fns - The default is ‘all’ which will create all possible regression model specifications supported.\n.parsnip_eng - The default is ‘all’ which will create all possible regression model specifications supported.\n.split_type - The default is ‘initial_split’, you can pass any type of split supported by rsample\n.split_args - The default is NULL, when NULL then the default parameters of the split type will be executed for the rsample split type.\n\n\n\nExample\nLet’s take a look at an example.\n\nlibrary(tidyAML)\nlibrary(dplyr)\nlibrary(recipes)\nlibrary(purrr)\n\nrec_obj <- recipe(mpg ~ ., data = mtcars)\nfast_reg_tbl <- fast_regression(\n  .data = mtcars,\n  .rec_obj = rec_obj,\n  .parsnip_eng = c(\"lm\",\"glm\"),\n  .parsnip_fns = \"linear_reg\"\n)\n\nglimpse(fast_reg_tbl)\n\nRows: 2\nColumns: 8\n$ .model_id       <int> 1, 2\n$ .parsnip_engine <chr> \"lm\", \"glm\"\n$ .parsnip_mode   <chr> \"regression\", \"regression\"\n$ .parsnip_fns    <chr> \"linear_reg\", \"linear_reg\"\n$ model_spec      <list> [~NULL, ~NULL, NULL, regression, TRUE, NULL, lm, TRUE]…\n$ wflw            <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ fitted_wflw     <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ pred_wflw       <list> [<tbl_df[24 x 1]>], [<tbl_df[24 x 1]>]\n\n\nLet’s take a look at the model spec.\n\nfast_reg_tbl %>% slice(1) %>% pull(model_spec) %>% pluck(1)\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nNow the wflw column.\n\nfast_reg_tbl %>% slice(1) %>% pull(wflw) %>% pluck(1)\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nThe Fitted workflow.\n\nfast_reg_tbl %>% slice(1) %>% pull(fitted_wflw) %>% pluck(1)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)          cyl         disp           hp         drat           wt  \n -15.077267     1.107474     0.001161    -0.001014     4.010199    -1.280324  \n       qsec           vs           am         gear         carb  \n   0.512318    -0.488014     2.430052     4.353568    -2.546043  \n\n\nAnd lastly tne predicted workflow column.\n\nfast_reg_tbl %>% slice(1) %>% pull(pred_wflw) %>% pluck(1)\n\n# A tibble: 24 × 1\n   .pred\n   <dbl>\n 1  24.7\n 2  28.2\n 3  18.9\n 4  12.0\n 5  14.8\n 6  15.4\n 7  14.7\n 8  20.0\n 9  11.2\n10  19.1\n# … with 14 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrdata-2023-02-10/index.html",
    "href": "posts/weekly-rtip-healthyrdata-2023-02-10/index.html",
    "title": "Get the Current Hospital Data Set from CMS with {healthyR.data}",
    "section": "",
    "text": "Introduction\nGetting data for health care in the US can sometimes be hard. With my R package {healthyR.data} I am hoping to alleviate some of that pain.\nRight now the package is bring actively developed from what was a simple yet sleepy simulated administrative data set is getting supercharged into a a full blow package that will retrieve data from outside sources. One such source is CMS.\nAt the start, and this is going to be a long road, I have started to build some functionality around getting the current hospital data from CMS. Let’s take a look at how it works.\n\n\nFunction\nHere is the function which has no parameters. This function will download the current and the official hospital data sets from the CMS.gov website.\nThe function makes use of a temporary directory and file to save and unzip the data. This will grab the current Hospital Data Files, unzip them and return a list of tibbles with each tibble named after the data file.\nThe function returns a list object with all of the current hospital data as a tibble. It does not save the data anywhere so if you want to save it you will have to do that manually.\nThis also means that you would have to store the data as a variable in order to access the data later on. It does have a given attributes and a class so that it can be piped into other functions.\n\ncurrent_hosp_data()\n\nNow let’s see it in action.\n\n\nExample\nWe will download the current hospital data sets and take a look.\n\nlibrary(healthyR.data)\nlibrary(dplyr)\n\ncurrent_hospital_dataset <- current_hosp_data()\n\nThis function downloads 70 files. Let’s see which ones have been downloaded.\n\nnames(current_hospital_dataset)\n\n [1] \"ASC_Facility.csv\"                                                \n [2] \"ASC_National.csv\"                                                \n [3] \"ASC_State.csv\"                                                   \n [4] \"ASCQR_OAS_CAHPS_BY_ASC.csv\"                                      \n [5] \"ASCQR_OAS_CAHPS_NATIONAL.csv\"                                    \n [6] \"ASCQR_OAS_CAHPS_STATE.csv\"                                       \n [7] \"CJR_PY6_Quality_Reporting_July_2022_Production_File.csv\"         \n [8] \"CMS_PSI_6_decimal_file.csv\"                                      \n [9] \"Complications_and_Deaths_Hospital.csv\"                           \n[10] \"Complications_and_Deaths_National.csv\"                           \n[11] \"Complications_and_Deaths_State.csv\"                              \n[12] \"Data_Updates_January_2023.csv\"                                   \n[13] \"Footnote_Crosswalk.csv\"                                          \n[14] \"FY_2023_HAC_Reduction_Program_Hospital.csv\"                      \n[15] \"FY_2023_Hospital_Readmissions_Reduction_Program_Hospital.csv\"    \n[16] \"FY2021_Distribution_of_Net_Change_in_Base_Op_DRG_Payment_Amt.csv\"\n[17] \"FY2021_Net_Change_in_Base_Op_DRG_Payment_Amt.csv\"                \n[18] \"FY2021_Percent_Change_in_Medicare_Payments.csv\"                  \n[19] \"FY2021_Value_Based_Incentive_Payment_Amount.csv\"                 \n[20] \"HCAHPS_Hospital.csv\"                                             \n[21] \"HCAHPS_National.csv\"                                             \n[22] \"HCAHPS_State.csv\"                                                \n[23] \"Healthcare_Associated_Infections_Hospital.csv\"                   \n[24] \"Healthcare_Associated_Infections_National.csv\"                   \n[25] \"Healthcare_Associated_Infections_State.csv\"                      \n[26] \"Hospital_General_Information.csv\"                                \n[27] \"HOSPITAL_QUARTERLY_MSPB_6_DECIMALS.csv\"                          \n[28] \"hvbp_clinical_outcomes.csv\"                                      \n[29] \"hvbp_efficiency_and_cost_reduction.csv\"                          \n[30] \"hvbp_person_and_community_engagement.csv\"                        \n[31] \"hvbp_safety.csv\"                                                 \n[32] \"hvbp_tps.csv\"                                                    \n[33] \"IPFQR_QualityMeasures_Facility.csv\"                              \n[34] \"IPFQR_QualityMeasures_National.csv\"                              \n[35] \"IPFQR_QualityMeasures_State.csv\"                                 \n[36] \"Maternal_Health_Hospital.csv\"                                    \n[37] \"Maternal_Health_National.csv\"                                    \n[38] \"Maternal_Health_State.csv\"                                       \n[39] \"Measure_Dates.csv\"                                               \n[40] \"Medicare_Hospital_Spending_by_Claim.csv\"                         \n[41] \"Medicare_Hospital_Spending_Per_Patient_Hospital.csv\"             \n[42] \"Medicare_Hospital_Spending_Per_Patient_National.csv\"             \n[43] \"Medicare_Hospital_Spending_Per_Patient_State.csv\"                \n[44] \"OAS_CAHPS_Footnotes.csv\"                                         \n[45] \"OQR_OAS_CAHPS_BY_HOSPITAL.csv\"                                   \n[46] \"OQR_OAS_CAHPS_NATIONAL.csv\"                                      \n[47] \"OQR_OAS_CAHPS_STATE.csv\"                                         \n[48] \"Outpatient_Imaging_Efficiency_Hospital.csv\"                      \n[49] \"Outpatient_Imaging_Efficiency_National.csv\"                      \n[50] \"Outpatient_Imaging_Efficiency_State.csv\"                         \n[51] \"Payment_National.csv\"                                            \n[52] \"Payment_State.csv\"                                               \n[53] \"Payment_and_Value_of_Care_Hospital.csv\"                          \n[54] \"PCH_HCAHPS_HOSPITAL.csv\"                                         \n[55] \"PCH_HCAHPS_NATIONAL.csv\"                                         \n[56] \"PCH_HCAHPS_STATE.csv\"                                            \n[57] \"PCH_HEALTHCARE_ASSOCIATED_INFECTIONS_HOSPITAL.csv\"               \n[58] \"PCH_ONCOLOGY_CARE_MEASURES_HOSPITAL.csv\"                         \n[59] \"PCH_OUTCOMES_HOSPITAL.csv\"                                       \n[60] \"PCH_OUTCOMES_NATIONAL.csv\"                                       \n[61] \"Timely_and_Effective_Care_Hospital.csv\"                          \n[62] \"Timely_and_Effective_Care_National.csv\"                          \n[63] \"Timely_and_Effective_Care_State.csv\"                             \n[64] \"Unplanned_Hospital_Visits_Hospital.csv\"                          \n[65] \"Unplanned_Hospital_Visits_National.csv\"                          \n[66] \"Unplanned_Hospital_Visits_State.csv\"                             \n[67] \"VA_IPF.csv\"                                                      \n[68] \"VA_TE.csv\"                                                       \n[69] \"Value_of_Care_National.csv\"                                      \n[70] \"Veterans_Health_Administration_Provider_Level_Data.csv\"          \n\n\nMore to come in the future!"
  },
  {
    "objectID": "posts/rtip-2023-02-13/index.html",
    "href": "posts/rtip-2023-02-13/index.html",
    "title": "Off to CRAN! {tidyAML}",
    "section": "",
    "text": "Introduction\nAre you tired of spending hours tuning and testing different machine learning models for your regression or classification problems? The new R package {tidyAML} is here to simplify the process for you! tidyAML is a simple interface for automatic machine learning that fits the tidymodels framework, making it easier for you to solve regression and classification problems.\nThe tidyAML package has been designed with the goal of providing a simple API that automates the entire machine learning pipeline, from data preparation to model selection, training, and prediction. This means that you no longer have to spend hours tuning and testing different models; tidyAML will do it all for you, saving you time and effort.\nIn this initial release (version 0.0.1), tidyAML introduces a number of new features and minor fixes to improve the overall user experience. Here are some of the updates in this release:\nNew Features:\n\nmake_regression_base_tbl() and make_classification_base_tbl() functions for creating base tables for regression and classification problems, respectively.\ninternal_make_spec_tbl() function for making the specification table for the machine learning pipeline.\ninternal_set_args_to_tune() function for setting arguments to tune the models. This has not yet been implemented in a true working fashion but might be useful for feedback in this initial release.\ncreate_workflow_set() function for creating a set of workflows to test different models.\nget_model(), extract_model_spec(), extract_wflw(), extract_wflw_fit(), and extract_wflw_pred() functions for extracting different parts of the machine learning pipeline.\nmatch_args() function for matching arguments between the base and specification tables.\n\nMinor Fixes and Improvements:\n\nUpdates to fast_classification_parsnip_spec_tbl() and fast_regression_parsnip_spec_tbl() to use the make_regression and make_classification functions and the internal_make_spec_tbl() function.\nAddition of a class for the base table functions and using that class in internal_make_spec_tbl().\nUpdate to the DESCRIPTION for R >= 3.4.0.\n\nIn conclusion, tidyAML is a game-changer for those looking to automate the machine learning pipeline. It provides a simple API that eliminates the need for manual tuning and testing of different models. With the updates in this initial release, the tidyAML package is sure to make your machine learning journey easier and more efficient.\n\n\nFunction\nThere are too many functions to go over in this post so you can find them all here\n\n\nExamples\nEven though there are many functions to go over, we can showcase some with a small useful example. So let’s get at it!\n\nlibrary(tidyAML)\nlibrary(recipes)\nlibrary(dplyr)\n\nrec_obj <- recipe(mpg ~ ., data = mtcars)\n\nfrt_tbl <- fast_regression(\n  .data = mtcars, \n  .rec_obj = rec_obj, \n  .parsnip_eng = c(\"lm\",\"glm\"),\n  .parsnip_fns = \"linear_reg\"\n  )\n\nglimpse(frt_tbl)\n\nRows: 2\nColumns: 8\n$ .model_id       <int> 1, 2\n$ .parsnip_engine <chr> \"lm\", \"glm\"\n$ .parsnip_mode   <chr> \"regression\", \"regression\"\n$ .parsnip_fns    <chr> \"linear_reg\", \"linear_reg\"\n$ model_spec      <list> [~NULL, ~NULL, NULL, regression, TRUE, NULL, lm, TRUE]…\n$ wflw            <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ fitted_wflw     <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ pred_wflw       <list> [<tbl_df[24 x 1]>], [<tbl_df[24 x 1]>]\n\n\nNow let’s go through the extractors.\nThe get_model() function.\n\nget_model(frt_tbl, 2) |>\n  glimpse()\n\nRows: 1\nColumns: 8\n$ .model_id       <int> 2\n$ .parsnip_engine <chr> \"glm\"\n$ .parsnip_mode   <chr> \"regression\"\n$ .parsnip_fns    <chr> \"linear_reg\"\n$ model_spec      <list> [~NULL, ~NULL, NULL, regression, TRUE, NULL, glm, TRUE…\n$ wflw            <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ fitted_wflw     <list> [cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb, mp…\n$ pred_wflw       <list> [<tbl_df[24 x 1]>]\n\n\nThe extract_model_spec() function.\n\nextract_model_spec(frt_tbl, 1)\n\n[[1]]\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nOr do multiples:\n\nextract_model_spec(frt_tbl, 1:2)\n\n[[1]]\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n[[2]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\nThe extract_wflw() function.\n\nextract_wflw(frt_tbl, 1)\n\n[[1]]\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nOr do multiples:\n\nextract_wflw(frt_tbl, c(1, 2))\n\n[[1]]\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n[[2]]\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\nThe extract_wflw_fit() function.\n\nextract_wflw_fit(frt_tbl, 1)\n\n[[1]]\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)          cyl         disp           hp         drat           wt  \n   28.21291     -1.60712      0.03458     -0.02189      0.56925     -5.69276  \n       qsec           vs           am         gear         carb  \n    0.69956      0.39398      1.50212     -0.35338      0.48289  \n\n\nOr do multiples:\n\nextract_wflw_fit(frt_tbl, 1:2)\n\n[[1]]\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)          cyl         disp           hp         drat           wt  \n   28.21291     -1.60712      0.03458     -0.02189      0.56925     -5.69276  \n       qsec           vs           am         gear         carb  \n    0.69956      0.39398      1.50212     -0.35338      0.48289  \n\n\n[[2]]\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::gaussian, data = data)\n\nCoefficients:\n(Intercept)          cyl         disp           hp         drat           wt  \n   28.21291     -1.60712      0.03458     -0.02189      0.56925     -5.69276  \n       qsec           vs           am         gear         carb  \n    0.69956      0.39398      1.50212     -0.35338      0.48289  \n\nDegrees of Freedom: 23 Total (i.e. Null);  13 Residual\nNull Deviance:      935.1 \nResidual Deviance: 121.5    AIC: 131\n\n\nFinally the extract_wflw_pred() function.\n\nextract_wflw_pred(frt_tbl, 2)\n\n[[1]]\n# A tibble: 24 × 1\n   .pred\n   <dbl>\n 1  24.8\n 2  26.5\n 3  18.5\n 4  13.9\n 5  24.6\n 6  29.1\n 7  14.0\n 8  17.9\n 9  10.0\n10  23.4\n# … with 14 more rows\n\n\nOr do multiples:\n\nextract_wflw_pred(frt_tbl, 1:2)\n\n[[1]]\n# A tibble: 24 × 1\n   .pred\n   <dbl>\n 1  24.8\n 2  26.5\n 3  18.5\n 4  13.9\n 5  24.6\n 6  29.1\n 7  14.0\n 8  17.9\n 9  10.0\n10  23.4\n# … with 14 more rows\n\n[[2]]\n# A tibble: 24 × 1\n   .pred\n   <dbl>\n 1  24.8\n 2  26.5\n 3  18.5\n 4  13.9\n 5  24.6\n 6  29.1\n 7  14.0\n 8  17.9\n 9  10.0\n10  23.4\n# … with 14 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-14/index.html",
    "href": "posts/rtip-2023-02-14/index.html",
    "title": "An example of using {box}",
    "section": "",
    "text": "Introduction\nToday I am going to make a short post on the R package {box} which was showcased to me quite nicely by Michael Miles. It was informative and I was able to immediately see the usefulness of the {box} library.\nSo what is ‘box’? Well here is the description straight from their site:\n\n‘box’ allows organising R code in a more modular way, via two mechanisms:\n\nIt enables writing modular code by treating files and folders of R code as independent (potentially nested) modules, without requiring the user to wrap reusable code into packages.\nIt provides a new syntax to import reusable code (both from packages and from modules) which is more powerful and less error-prone than library or require, by limiting the number of names that are made available.\n\n\nSo let’s see how it all works.\n\n\nFunction\nThe main portion of the script looks like this:\n\n# Main script\n\n# Script setup --------------------------------------\n\n# Load box modules\nbox::use(. / box / global_options / global_options)\nbox::use(. / box / io / imports)\nbox::use(. / box / io / exports)\nbox::use(. / box / mod / mod)\n\n# Load global options\nglobal_options$set_global_options() \n\n\n# Main script ---------------------------------------\n\n# Load data, process it, and export results\nall_data <- getOption('data_dir') |> \n  \n  # Load all data\n  imports$load_all() |> \n  \n  # Modify dataset\n  mod$modify_data() |> \n  \n  # Export data\n  exports$export_data()\n\nSo what does this do? Well it is grabbing data from a predefined location, modifying it and then re-exporting it. Now let’s look at all the code that is behind it, which allows us to do these things and then you will see the power of using box\n\n\nExample\nLet’s take a look at the global options settings.\n\n# Set global options\n#' @export\nset_global_options <- function() {\n  options(\n    look_ups = 'look-ups/',\n    data_dir = 'data/input/'\n  )\n}\n\nOk 6 lines, boxed down to one.\nNow the import function.\n\n# Function for importing data\n\n#' @export\nload_all <- function(file_path) {\n  \n  box::use(purrr)\n  box::use(vroom)\n  \n  file_path |> \n    \n    # Get all csv files from folder\n    list.files(full.names = TRUE) |> \n    \n    # Set list names\n    purrr$set_names(\\(file) basename(file)) |> \n    \n    # Load all csvs into list\n    purrr$map(\\(file) vroom$vroom(file))\n\n}\n\nNow the modify_data function.\n\n# Function for modifying data\n\n#' @export\nmodify_data <- function(df_list) {\n  \n  box::use(dplyr)\n  box::use(purrr)\n  \n  map_fun <- function(df) {\n    \n    df |> \n      dplyr$select(name:mass) |> \n      dplyr$mutate(lol = height * mass) |> \n      dplyr$filter(lol > 1500)\n  }\n  \n  # Apply mapping function to list\n  purrr$map(df_list, map_fun)\n  \n}\n\nOk again, a big savings here, instead of the above we simply call mod$modify_data() which makes things clearner and also modular in that we can go to a very specific spot in our proejct to fix an error or add/subtract functionality.\nLastly the export.\n\n# Function for exporting data\n\n#' @export\nexport_data <- function(df_list) {\n  \n  box::use(vroom)\n  box::use(purrr)\n  \n  # Export data\n  purrr$map2(.x = df_list,\n             .y = names(df_list),\n             ~vroom$vroom_write(x = .x,\n                               file = paste0('data/output/', \n                                             .y),\n                               delim = ','))\n  \n}\n\nVoila! I think to even a fresh user, the power of boxing your functions is fairly apparent and to the advanced user, eyes are most likely glowing!"
  },
  {
    "objectID": "posts/rtip-2023-02-15/index.html",
    "href": "posts/rtip-2023-02-15/index.html",
    "title": "Moving Average Plots with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nAre you interested in visualizing time series data in a clear and concise way? The R package {healthyR.ts} provides a variety of tools for time series analysis and visualization, including the ts_ma_plot() function.\nThe ts_ma_plot() function is designed to help you quickly and easily create moving average plots for time series data. This function takes several arguments, including the data you want to visualize, the date column from your data, the value column from your data, and the frequency of the aggregation.\nOne of the great features of ts_ma_plot() is that it can handle both weekly and monthly data frequencies, making it a flexible tool for analyzing a variety of time series data. If you pass in a frequency other than “weekly” or “monthly”, the function will default to weekly, so it’s important to ensure that your data is aggregated at the appropriate frequency.\nWith ts_ma_plot(), you can create a variety of plots to help you better understand your time series data. The function allows you to add up to three different titles to your plot, helping you to organize and communicate your findings effectively. The main_title argument sets the title for the main plot, while the secondary_title and tertiary_title arguments set the titles for the second and third plots, respectively.\nIf you’re interested in using ts_ma_plot() for your own time series data, you’ll first need to preprocess your data so that it’s in the appropriate format for this function. Once you’ve done that, though, ts_ma_plot() can help you to quickly identify trends and patterns in your data that might not be immediately apparent from a raw data set.\nIn summary, ts_ma_plot() is a powerful and flexible tool for visualizing time series data. Whether you’re working with weekly or monthly data, this function can help you to quickly and easily create moving average plots that can help you to better understand your data. If you’re interested in time series analysis, be sure to check out {healthyR.ts} and give ts_ma_plot() a try!\n\n\nFunction\nHere is the full function call.\n\nts_ma_plot(\n  .data,\n  .date_col,\n  .value_col,\n  .ts_frequency = \"monthly\",\n  .main_title = NULL,\n  .secondary_title = NULL,\n  .tertiary_title = NULL\n)\n\nNow for the arguments to the parameters.\n\n.data: the data you want to visualize, which should be pre-processed and the aggregation should match the .frequency argument.\n.date_col: the data column from the .data argument that contains the dates for your time series.\n.value_col: the data column from the .data argument that contains the values for your time series.\n.ts_frequency: the frequency of the aggregation, which should be quoted as “weekly” or “monthly”. If not specified, the function defaults to weekly.\n.main_title: the title of the main plot.\n.secondary_title: the title of the second plot.\n.tertiary_title: the title of the third plot.\n\n\n\nExample\nNow for an example.\n\nlibrary(healthyR.ts)\nlibrary(dplyr)\n\ndata_tbl <- ts_to_tbl(AirPassengers) |>\n  select(-index)\n\noutput <- ts_ma_plot(\n  .data = data_tbl,\n  .date_col = date_col,\n  .value_col = value\n)\n\nLet’s take a look at each piece of the output.\n\noutput$data_trans_xts |> head()\n\n           value ma12\n1949-01-01   112   NA\n1949-02-01   118   NA\n1949-03-01   132   NA\n1949-04-01   129   NA\n1949-05-01   121   NA\n1949-06-01   135   NA\n\n\n\noutput$data_diff_xts_a |> head()\n\n              diff_a\n1949-01-01        NA\n1949-02-01  5.357143\n1949-03-01 11.864407\n1949-04-01 -2.272727\n1949-05-01 -6.201550\n1949-06-01 11.570248\n\n\n\noutput$data_diff_xts_b |> head()\n\n           diff_b\n1949-01-01     NA\n1949-02-01     NA\n1949-03-01     NA\n1949-04-01     NA\n1949-05-01     NA\n1949-06-01     NA\n\n\n\noutput$data_summary_tbl\n\n# A tibble: 144 × 5\n   date_col   value  ma12 diff_a diff_b\n   <date>     <dbl> <dbl>  <dbl>  <dbl>\n 1 1949-01-01   112    NA   0         0\n 2 1949-02-01   118    NA   5.36      0\n 3 1949-03-01   132    NA  11.9       0\n 4 1949-04-01   129    NA  -2.27      0\n 5 1949-05-01   121    NA  -6.20      0\n 6 1949-06-01   135    NA  11.6       0\n 7 1949-07-01   148    NA   9.63      0\n 8 1949-08-01   148    NA   0         0\n 9 1949-09-01   136    NA  -8.11      0\n10 1949-10-01   119    NA -12.5       0\n# … with 134 more rows\n\n\n\noutput$pgrid\n\n\n\n\n\noutput$xts_plt"
  },
  {
    "objectID": "posts/rtip-2023-02-16/index.html",
    "href": "posts/rtip-2023-02-16/index.html",
    "title": "Officially on CRAN {tidyAML}",
    "section": "",
    "text": "Introduction\nI’m excited to announce that the R package {tidyAML} is now officially available on CRAN! This package is designed to make it easy for users to perform automated machine learning (AutoML) using the tidymodels ecosystem. With a simple and intuitive interface, tidyAML allows users to quickly generate high-quality machine learning models without worrying about the underlying details.\nOne of the key features of tidyAML is its ability to generate regression models on the fly, without the need to build a full specification or tune hyper-parameters. This makes it ideal for users who want to quickly build a machine learning model without spending a lot of time on the setup process.\ntidyAML is also designed to be easy to use, with a set of functions that are straightforward and can generate many models and predictions at once. And because it’s built on top of the tidymodels ecosystem, users don’t need to worry about setting up additional packages or dependencies.\nWe’re also happy to announce that tidyAML will be added to the R package {healthyverse} and pushed to CRAN this week. This means that users who install {healthyverse} will automatically get access to tidyAML, as well as other popular packages like ggplot2, dplyr, and tidyr.\nWhether you’re a beginner or an experienced machine learning practitioner, tidyAML is a powerful tool that can help you quickly generate high-quality models with minimal setup. We hope you’ll give it a try and let us know what you think!"
  },
  {
    "objectID": "posts/rtip-2023-02-17/index.html",
    "href": "posts/rtip-2023-02-17/index.html",
    "title": "Converting a {tidyAML} tibble to a {workflowsets}",
    "section": "",
    "text": "The {tidyAML} package is an R package that provides a set of tools for building regression/classification models on the fly with minimal input required. In this post we will discuss the create_workflow_set() function.\nThe create_workflow_set function is a function in the tidyAML package that is used to create a workflowset object from the workflowsets package. A workflow is a sequence of tasks that can be executed in a specific order, and is often used in data analysis and machine learning to automate data processing and model fitting. The create_workflow_set function takes as input a YAML specification of a set of workflows, and returns a list of workflow objects that can be executed using the tidymodels package and its associated packages.\nThe create_workflow_set function is particularly useful when working with the tidymodels package and the parsnip framework. The tidymodels package is a collection of packages for modeling and machine learning in R that provides a consistent interface for building, tuning, and evaluating machine learning models. The parsnip package is part of the tidymodels ecosystem and provides a way to specify a wide range of models in a consistent manner.\n\n\nTo use the create_workflow_set function with tidymodels andparsnip, you will need to provide a recipe or recipes as a list to the .recipe_list parameter and a model_spec tibble that you would get from something like fast_regression_parsnip_spec_tbl(), other classes will be supported in the future.\nThe reason this was done was because I did not want to force users to remain inside of tidyAML perhaps and most likely there are other packages out there that are more suited to an end users specific problem at hand."
  },
  {
    "objectID": "posts/rtip-2023-02-22/index.html",
    "href": "posts/rtip-2023-02-22/index.html",
    "title": "Calibrate and Plot a Time Series with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nIn time series analysis, it is common to split the data into training and testing sets to evaluate the accuracy of a model. However, it is important to ensure that the model is calibrated on the training set before evaluating its performance on the testing set. The {healthyR.ts} library provides a function called calibrate_and_plot() that simplifies this process.\n\n\nFunction\nHere is the full function call:\n\ncalibrate_and_plot(\n  ...,\n  .type = \"testing\",\n  .splits_obj,\n  .data,\n  .print_info = TRUE,\n  .interactive = FALSE\n)\n\nHere are the arguments to the parameters:\n\n... - The workflow(s) you want to add to the function.\n.type - Either the training(splits) or testing(splits) data.\n.splits_obj - The splits object.\n.data - The full data set.\n.print_info - The default is TRUE and will print out the calibration accuracy tibble and the resulting plotly plot.\n.interactive - The defaults is FALSE. This controls if a forecast plot is interactive or not via plotly.\n\n\n\nExample\nBy default, calibrate_and_plot() will print out a calibration accuracy tibble and a resulting plotly plot. This can be controlled with the print_info argument, which is set to TRUE by default. If you prefer a non-interactive forecast plot, you can set the interactive argument to FALSE.\nHere’s an example of how to use the calibrate_and_plot() function:\n\nlibrary(healthyR.ts)\nlibrary(dplyr)\nlibrary(timetk)\nlibrary(parsnip)\nlibrary(recipes)\nlibrary(workflows)\nlibrary(rsample)\n\n# Get the Data\ndata <- ts_to_tbl(AirPassengers) |>\n  select(-index)\n\n# Split the data into training and testing sets\nsplits <- time_series_split(\n   data\n  , date_col\n  , assess = 12\n  , skip = 3\n  , cumulative = TRUE\n)\n\n# Make the recipe object\nrec_obj <- recipe(value ~ ., data = training(splits))\n\n# Make the Model\nmodel_spec <- linear_reg(\n   mode = \"regression\"\n   , penalty = 0.5\n   , mixture = 0.5\n) |>\n   set_engine(\"lm\")\n\n# Make the workflow object\nwflw <- workflow() |>\n   add_recipe(rec_obj) |>\n   add_model(model_spec) |>\n   fit(training(splits))\n\n# Get our output\noutput <- calibrate_and_plot(\n  wflw\n  , .type = \"training\"\n  , .splits_obj = splits\n  , .data = data\n  , .print_info = FALSE\n  , .interactive = TRUE\n )\n\nThe resulting output will include a calibration accuracy tibble and a plotly plot showing the original time series data along with the fitted values for the training set.\nLet’s take a look at the output.\n\noutput$calibration_tbl\n\n# Modeltime Table\n# A tibble: 1 × 5\n  .model_id .model     .model_desc .type .calibration_data \n      <int> <list>     <chr>       <chr> <list>            \n1         1 <workflow> LM          Test  <tibble [132 × 4]>\n\n\n\noutput$model_accuracy\n\n# A tibble: 1 × 9\n  .model_id .model_desc .type   mae  mape  mase smape  rmse   rsq\n      <int> <chr>       <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1         1 LM          Test   31.4  12.0  1.31  11.9  41.7 0.846\n\n\nAnd…\n\noutput$plot\n\n\n\n\n\nOverall, the calibrate_and_plot() function is a useful tool for simplifying the process of calibrating time series models on a training set and evaluating their performance on a testing set.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-23/index.html",
    "href": "posts/rtip-2023-02-23/index.html",
    "title": "Data Preppers with {healthyR.ai}",
    "section": "",
    "text": "Introduction\nThere are many different methods that one can choose from in order to model their data. This brings with it a fundamental issue of how to prepare your data for the specified algorithm. With the [{healthyR.ai}] package there are many different functions in this family that will help solve this issue for some algorithms but of course not all, that would be utterly exhausting for me to do on my own.\nIn healthyR.ai I call these Data Preppers because they prep the data you supply to the format necessary for the algorithm to function properly.\nLet’s take a look at one.\n\n\nFunction\nHere we are going to use the hai_c50_data_prepper(.data, .recipe_formula) function.\n\nhai_c50_data_prepper(.data, .recipe_formula)\n\nHere are the simple arguments:\n\n.data - The data that you are passing to the function. Can be any type of data that is accepted by the data parameter of the recipes::recipe() function.\n.recipe_formula - The formula that is going to be passed. For example if you are using the iris data then the formula would most likely be something like Species ~ .\n\n\n\nExample\nHere is a small example:\n\nlibrary(healthyR.ai)\n\nhai_c50_data_prepper(.data = Titanic, .recipe_formula = Survived ~ .)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nOperations:\n\nFactor variables from tidyselect::vars_select_helpers$where(is.charac...\n\nrec_obj <- hai_c50_data_prepper(Titanic, Survived ~ .)\nget_juiced_data(rec_obj)\n\n# A tibble: 32 × 5\n   Class Sex    Age       n Survived\n   <fct> <fct>  <fct> <dbl> <fct>   \n 1 1st   Male   Child     0 No      \n 2 2nd   Male   Child     0 No      \n 3 3rd   Male   Child    35 No      \n 4 Crew  Male   Child     0 No      \n 5 1st   Female Child     0 No      \n 6 2nd   Female Child     0 No      \n 7 3rd   Female Child    17 No      \n 8 Crew  Female Child     0 No      \n 9 1st   Male   Adult   118 No      \n10 2nd   Male   Adult   154 No      \n# … with 22 more rows\n\n\nHere are the rest of the data-preppers at the time of writing this article:\n\nhai_c50_data_prepper()\nhai_cubist_data_prepper()\nhai_earth_data_prepper()\nhai_glmnet_data_prepper()\nhai_knn_data_prepper()\nhai_ranger_data_prepper()\nhai_svm_poly_data_prepper()\nhai_svm_rbf_data_prepper()\nhai_xgboost_data_prepper()\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-27/index.html",
    "href": "posts/rtip-2023-02-27/index.html",
    "title": "Quickly Generate Nested Time Series Models",
    "section": "",
    "text": "Introduction\nThere are many approaches to modeling time series data in R. One of the types of data that we might come across is a nested time series. This means the data is grouped simply by one or more keys. There are many methods in which to accomplish this task. This will be a quick post, but if you want a longer more detailed and quite frankly well written out one, then this is a really good article\n\n\nExampmle\nLet’s just get to it with a very simple example, the motivation here isn’t to be all encompassing, but rather to just showcase it is possible for those who may not know it is.\n\nlibrary(healthyR.data)\nlibrary(dplyr)\nlibrary(timetk)\n\nts_tbl <- healthyR_data |> \n  filter(ip_op_flag == \"I\") |> \n  select(visit_end_date_time, service_line, length_of_stay) |>\n  mutate(visit_end_date_time = as.Date(visit_end_date_time)) |>\n  group_by(service_line) |>\n  summarise_by_time(\n    .date_var = visit_end_date_time,\n    .by = \"month\",\n    los = mean(length_of_stay)\n  ) |>\n  ungroup()\n\nglimpse(ts_tbl)\n\nRows: 2,148\nColumns: 3\n$ service_line        <chr> \"Alcohol Abuse\", \"Alcohol Abuse\", \"Alcohol Abuse\",…\n$ visit_end_date_time <date> 2011-09-01, 2011-10-01, 2011-11-01, 2011-12-01, 2…\n$ los                 <dbl> 3.666667, 3.181818, 4.380952, 3.464286, 3.677419, …\n\n\n\nlibrary(forecast)\nlibrary(broom)\nlibrary(tidyr)\n\nglanced_models <- ts_tbl |> \n  nest_by(service_line) |> \n  mutate(AA = list(auto.arima(data$los))) |> \n  mutate(perf = list(glance(AA))) |> \n  unnest(cols = c(perf))\n\nglanced_models |>\n  select(-data)\n\n# A tibble: 23 × 7\n# Groups:   service_line [23]\n   service_line                  AA         sigma logLik   AIC   BIC  nobs\n   <chr>                         <list>     <dbl>  <dbl> <dbl> <dbl> <int>\n 1 Alcohol Abuse                 <fr_ARIMA> 2.22  -241.   493.  506.   109\n 2 Bariatric Surgery For Obesity <fr_ARIMA> 0.609  -80.1  168.  178.    88\n 3 CHF                           <fr_ARIMA> 0.963 -152.   309.  314.   110\n 4 COPD                          <fr_ARIMA> 0.987 -155.   315.  320.   110\n 5 CVA                           <fr_ARIMA> 1.50  -201.   407.  412.   110\n 6 Carotid Endarterectomy        <fr_ARIMA> 6.27  -166.   335.  339.    51\n 7 Cellulitis                    <fr_ARIMA> 1.07  -163.   329.  335.   110\n 8 Chest Pain                    <fr_ARIMA> 0.848 -139.   281.  287.   110\n 9 GI Hemorrhage                 <fr_ARIMA> 1.21  -179.   361.  366.   111\n10 Joint Replacement             <fr_ARIMA> 1.65  -196.   396.  401.   102\n# … with 13 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-02-28/index.html",
    "href": "posts/rtip-2023-02-28/index.html",
    "title": "Open a File Folder in R",
    "section": "",
    "text": "Inroduction\nWhen writing a function, it is possible that you may want to ask the user where they want the data stored and if they want to open the file folder after the download has taken place. Well we can do this in R by invoking the shell.exec() command where we use a variable like f_path that is the path to the folder. We are going to go over a super simple example.\n\n\nFunction\nHere is the function:\n\nshell.exec(file)\n\nHere are the arguments.\n\nfile - file, directory or URL to be opened.\n\nNow let’s go over a simple example\n\n\nExample\nHere we go.\n\n# Create a temporary file to store the zip file\nf_path <- utils::choose.dir()\n\n# Open file folder?\nif (.open_folder){\n    shell.exec(f_path)\n}\n\nIf in our function creation we make a variable .open_folder and set it equal to TRUE then the if statement will execute and shell.exec(f_path) will open the specified path set by utils::choose.dir()\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-01/index.html",
    "href": "posts/rtip-2023-03-01/index.html",
    "title": "Text Processing Made Easy with {healthyR}’s sql_left(), sql_mid(), and sql_right() Functions in R",
    "section": "",
    "text": "Introduction\nAre you tired of manually manipulating text data in R? Do you find yourself frequently needing to extract substrings from long strings or to grab just the first few characters of a string? If so, you’re in luck! The {healthyR} library has three functions that will make your text processing tasks much easier: sql_left(), sql_mid(), and sql_right().\n\n\nFunction\nHere are the function calls, I will also make the source avilable in the same cell so steal this code!!\n\n# LEFT\nsql_left(\"text\", 3)\n\nsql_left <- function(.text, .num_char) {\n    base::substr(.text, 1, .num_char)\n}\n\n# MID\nsql_mid(\"this is some text\", 6, 2)\n\nsql_mid <- function(.text, .start_num, .num_char) {\n    base::substr(.text, .start_num, .start_num + .num_char - 1)\n}\n\n# RIGHT\nsql_right(\"this is some more text\", 3)\n\nsql_right <- function(.text, .num_char) {\n    base::substr(.text, base::nchar(.text) - (.num_char-1), base::nchar(.text))\n}\n\n\n\nExample\nLet’s start with sql_left(). This function is similar to the LEFT() function in SQL and Excel, in that it returns the specified number of characters from the beginning of a string. For example, if we have the string “Hello, world!”, and we want to grab just the first three characters, we can use sql_left() like this:\n\nlibrary(healthyR)\nsql_left(\"Hello, world!\", 3)\n\n[1] \"Hel\"\n\n\nThis will return the string “Hel”.\nNext up is sql_mid(). This function is similar to the SUBSTRING() and MID() functions in SQL and Excel, in that it returns a specified portion of a string. The first argument is the string itself, the second argument is the starting position of the substring, and the third argument is the length of the substring. For example, if we have the string “This is some text”, and we want to grab the two characters starting at position six, we can use sql_mid() like this:\n\nsql_mid(\"This is some text\", 6, 2)\n\n[1] \"is\"\n\n\nThis will return the string “is”.\nFinally, we have sql_right(). This function is similar to the RIGHT() function in SQL and Excel, in that it returns the specified number of characters from the end of a string. For example, if we have the string “This is some more text”, and we want to grab just the last three characters, we can use sql_right() like this:\n\nsql_right(\"This is some more text\", 3)\n\n[1] \"ext\"\n\n\nThis will return the string “ext”.\nThese three functions can be extremely helpful when working with text data in R. They can save you time and effort, and make your code more concise and readable. So next time you find yourself needing to manipulate text data, remember to reach for sql_left(), sql_mid(), and sql_right()!\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-02/index.html",
    "href": "posts/rtip-2023-03-02/index.html",
    "title": "Forecasting Timeseries in a list with R",
    "section": "",
    "text": "Introduction\nIn this article, we will discuss how to perform an ARIMA forecast on nested data or data that is in a list using R programming language. This is a common scenario in which we have data stored in a list format, where each element of the list corresponds to a different time series. We will use the R programming language, specifically the “forecast” package, to perform the ARIMA forecast.\nFirst, we will need to load the required packages and data. For this example, we will use the “AirPassengers” dataset which is included in the “datasets” package. This dataset contains the number of international airline passengers per month from 1949 to 1960. We will then create a list containing subsets of this data for each year.\n\nlibrary(forecast)\n\nyearly_data <- split(AirPassengers, f = ceiling(seq_along(AirPassengers)/12))\n\nyearly_data\n\n$`1`\n [1] 112 118 132 129 121 135 148 148 136 119 104 118\n\n$`2`\n [1] 115 126 141 135 125 149 170 170 158 133 114 140\n\n$`3`\n [1] 145 150 178 163 172 178 199 199 184 162 146 166\n\n$`4`\n [1] 171 180 193 181 183 218 230 242 209 191 172 194\n\n$`5`\n [1] 196 196 236 235 229 243 264 272 237 211 180 201\n\n$`6`\n [1] 204 188 235 227 234 264 302 293 259 229 203 229\n\n$`7`\n [1] 242 233 267 269 270 315 364 347 312 274 237 278\n\n$`8`\n [1] 284 277 317 313 318 374 413 405 355 306 271 306\n\n$`9`\n [1] 315 301 356 348 355 422 465 467 404 347 305 336\n\n$`10`\n [1] 340 318 362 348 363 435 491 505 404 359 310 337\n\n$`11`\n [1] 360 342 406 396 420 472 548 559 463 407 362 405\n\n$`12`\n [1] 417 391 419 461 472 535 622 606 508 461 390 432\n\n\nIn the above code, we use the “split” function to split the data into yearly subsets. The “f” parameter is used to specify the grouping variable which, in this case, is the sequence of numbers from 1 to the length of the dataset divided by 12, rounded up to the nearest integer. This creates a list of 12 elements, one for each year.\n\n\nFunction\nNext, we will define a function that takes a single element of the list, fits an ARIMA model, and generates a forecast.\n\narima_forecast <- function(x){\n  fit <- auto.arima(x)\n  forecast(fit)\n}\n\nThis function takes a single argument “x” which is one of the elements of the list. We use the “auto.arima” function from the “forecast” package to fit an ARIMA model to the data. The “forecast” function is then used to generate a forecast based on this model.\n\n\nExample\nWe can now use the “lapply” function to apply this function to each element of the list.\n\nforecasts <- lapply(yearly_data, arima_forecast)\n\nThe “lapply” function applies the “arima_forecast” function to each element of the “yearly_data” list and returns a list of forecasts.\nFinally, we can extract and plot the forecasts for a specific year.\n\nplot(forecasts[[5]])\n\n\n\n\nNow lets take a look at them all.\n\npar(mfrow = c(2,1))\n\npurrr::map(forecasts, plot)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$`1`\n$`1`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 132.2237 126.4744 126.4744 126.4744 126.4744 126.4744 126.4744 126.4744\n [9] 126.4744 126.4744\n\n$`1`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 120.1608 113.7751\n14 110.0828 101.4056\n15 110.0828 101.4056\n16 110.0828 101.4056\n17 110.0828 101.4056\n18 110.0828 101.4056\n19 110.0828 101.4056\n20 110.0828 101.4056\n21 110.0828 101.4056\n22 110.0828 101.4056\n\n$`1`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 144.2865 150.6722\n14 142.8660 151.5432\n15 142.8660 151.5432\n16 142.8660 151.5432\n17 142.8660 151.5432\n18 142.8660 151.5432\n19 142.8660 151.5432\n20 142.8660 151.5432\n21 142.8660 151.5432\n22 142.8660 151.5432\n\n\n$`2`\n$`2`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 153.8708 139.5919 139.5919 139.5919 139.5919 139.5919 139.5919 139.5919\n [9] 139.5919 139.5919\n\n$`2`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 136.3778 127.1175\n14 115.8789 103.3260\n15 115.8789 103.3260\n16 115.8789 103.3260\n17 115.8789 103.3260\n18 115.8789 103.3260\n19 115.8789 103.3260\n20 115.8789 103.3260\n21 115.8789 103.3260\n22 115.8789 103.3260\n\n$`2`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 171.3638 180.6240\n14 163.3048 175.8577\n15 163.3048 175.8577\n16 163.3048 175.8577\n17 163.3048 175.8577\n18 163.3048 175.8577\n19 163.3048 175.8577\n20 163.3048 175.8577\n21 163.3048 175.8577\n22 163.3048 175.8577\n\n\n$`3`\n$`3`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 173.6413 170.0479 170.0479 170.0479 170.0479 170.0479 170.0479 170.0479\n [9] 170.0479 170.0479\n\n$`3`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 153.5404 142.8995\n14 146.6452 134.2565\n15 146.6452 134.2565\n16 146.6452 134.2565\n17 146.6452 134.2565\n18 146.6452 134.2565\n19 146.6452 134.2565\n20 146.6452 134.2565\n21 146.6452 134.2565\n22 146.6452 134.2565\n\n$`3`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 193.7423 204.3831\n14 193.4506 205.8393\n15 193.4506 205.8393\n16 193.4506 205.8393\n17 193.4506 205.8393\n18 193.4506 205.8393\n19 193.4506 205.8393\n20 193.4506 205.8393\n21 193.4506 205.8393\n22 193.4506 205.8393\n\n\n$`4`\n$`4`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 194.0074 194.0119 194.0147 194.0164 194.0174 194.0180 194.0184 194.0186\n [9] 194.0187 194.0188\n\n$`4`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 169.7973 156.9812\n14 165.6741 150.6730\n15 164.2944 148.5614\n16 163.8005 147.8051\n17 163.6201 147.5288\n18 163.5539 147.4272\n19 163.5296 147.3898\n20 163.5207 147.3761\n21 163.5175 147.3711\n22 163.5163 147.3692\n\n$`4`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 218.2176 231.0336\n14 222.3497 237.3509\n15 223.7350 239.4680\n16 224.2322 240.2276\n17 224.4146 240.5059\n18 224.4821 240.6088\n19 224.5071 240.6469\n20 224.5165 240.6611\n21 224.5200 240.6664\n22 224.5213 240.6684\n\n\n$`5`\n$`5`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 206.8929 210.7977 213.3851 215.0996 216.2356 216.9884 217.4872 217.8178\n [9] 218.0368 218.1819\n\n$`5`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 178.2600 163.1026\n14 176.4492 158.2662\n15 176.8082 157.4455\n16 177.5860 157.7275\n17 178.3181 158.2458\n18 178.8949 158.7294\n19 179.3167 159.1104\n20 179.6134 159.3893\n21 179.8176 159.5856\n22 179.9562 159.7208\n\n$`5`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 235.5258 250.6831\n14 245.1461 263.3291\n15 249.9620 269.3246\n16 252.6131 272.4716\n17 254.1531 274.2255\n18 255.0819 275.2475\n19 255.6578 275.8641\n20 256.0221 276.2462\n21 256.2559 276.4879\n22 256.4076 276.6430\n\n\n$`6`\n$`6`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 245.0709 240.0400 240.0400 240.0400 240.0400 240.0400 240.0400 240.0400\n [9] 240.0400 240.0400\n\n$`6`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 212.6687 195.5160\n14 196.9893 174.1996\n15 196.9893 174.1996\n16 196.9893 174.1996\n17 196.9893 174.1996\n18 196.9893 174.1996\n19 196.9893 174.1996\n20 196.9893 174.1996\n21 196.9893 174.1996\n22 196.9893 174.1996\n\n$`6`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 277.4731 294.6259\n14 283.0907 305.8803\n15 283.0907 305.8803\n16 283.0907 305.8803\n17 283.0907 305.8803\n18 283.0907 305.8803\n19 283.0907 305.8803\n20 283.0907 305.8803\n21 283.0907 305.8803\n22 283.0907 305.8803\n\n\n$`7`\n$`7`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 278.0001 278.0001 278.0002 278.0002 278.0002 278.0002 278.0002 278.0002\n [9] 278.0002 278.0002\n\n$`7`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 236.8903 215.1282\n14 228.5879 202.4307\n15 225.3145 197.4243\n16 223.9224 195.2953\n17 223.3147 194.3659\n18 223.0466 193.9559\n19 222.9278 193.7742\n20 222.8751 193.6936\n21 222.8516 193.6577\n22 222.8412 193.6418\n\n$`7`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 319.1098 340.8720\n14 327.4123 353.5695\n15 330.6859 358.5760\n16 332.0780 360.7051\n17 332.6857 361.6345\n18 332.9538 362.0445\n19 333.0726 362.2262\n20 333.1254 362.3069\n21 333.1488 362.3427\n22 333.1592 362.3587\n\n\n$`8`\n$`8`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 349.0540 373.2678 369.7906 348.0549 325.4487 315.1915 319.8599 332.7645\n [9] 344.2812 348.1670\n\n$`8`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 315.6225 297.9249\n14 322.1404 295.0752\n15 314.7795 285.6584\n16 292.8344 263.6024\n17 266.5768 235.4118\n18 252.9822 220.0505\n19 257.0954 223.8699\n20 269.7958 236.4622\n21 280.1875 246.2583\n22 283.2781 248.9280\n\n$`8`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 382.4855 400.1831\n14 424.3952 451.4604\n15 424.8018 453.9229\n16 403.2754 432.5074\n17 384.3206 415.4855\n18 377.4009 410.3325\n19 382.6243 415.8498\n20 395.7332 429.0668\n21 408.3750 442.3042\n22 413.0559 447.4061\n\n\n$`9`\n$`9`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 378.9729 406.5723 408.7509 392.6048 372.9147 361.5778 362.0569 370.2398\n [9] 379.1516 383.6927\n\n$`9`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 336.2126 313.5766\n14 342.0963 307.9648\n15 339.3660 302.6358\n16 323.1265 286.3469\n17 300.2319 261.7560\n18 285.7363 245.5882\n19 285.5516 245.0521\n20 293.6654 253.1294\n21 301.8675 260.9558\n22 305.8147 264.5885\n\n$`9`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 421.7333 444.3692\n14 471.0482 505.1797\n15 478.1359 514.8660\n16 462.0831 498.8627\n17 445.5975 484.0734\n18 437.4193 477.5674\n19 438.5622 479.0617\n20 446.8142 487.3503\n21 456.4356 497.3473\n22 461.5707 502.7968\n\n\n$`10`\n$`10`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 391.9249 381.5489 381.5489 381.5489 381.5489 381.5489 381.5489 381.5489\n [9] 381.5489 381.5489\n\n$`10`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 331.8921 300.1126\n14 304.6704 263.9734\n15 304.6704 263.9734\n16 304.6704 263.9734\n17 304.6704 263.9734\n18 304.6704 263.9734\n19 304.6704 263.9734\n20 304.6704 263.9734\n21 304.6704 263.9734\n22 304.6704 263.9734\n\n$`10`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 451.9577 483.7372\n14 458.4274 499.1244\n15 458.4274 499.1244\n16 458.4274 499.1244\n17 458.4274 499.1244\n18 458.4274 499.1244\n19 458.4274 499.1244\n20 458.4274 499.1244\n21 458.4274 499.1244\n22 458.4274 499.1244\n\n\n$`11`\n$`11`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 408.4203 410.7762 412.3990 413.5168 414.2868 414.8171 415.1824 415.4340\n [9] 415.6074 415.7268\n\n$`11`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 342.2241 307.1820\n14 330.3960 287.8452\n15 326.1006 280.4170\n16 324.5481 277.4509\n17 324.0788 276.3255\n18 324.0270 275.9656\n19 324.1175 275.9106\n20 324.2390 275.9632\n21 324.3506 276.0422\n22 324.4407 276.1168\n\n$`11`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 474.6165 509.6586\n14 491.1565 533.7072\n15 498.6974 544.3810\n16 502.4855 549.5827\n17 504.4948 552.2480\n18 505.6072 553.6686\n19 506.2474 554.4543\n20 506.6291 554.9049\n21 506.8641 555.1726\n22 507.0128 555.3367\n\n\n$`12`\n$`12`$mean\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n [1] 502.9998 476.0531 476.0531 476.0531 476.0531 476.0531 476.0531 476.0531\n [9] 476.0531 476.0531\n\n$`12`$lower\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 437.2687 402.4728\n14 387.1722 340.1214\n15 387.1722 340.1214\n16 387.1722 340.1214\n17 387.1722 340.1214\n18 387.1722 340.1214\n19 387.1722 340.1214\n20 387.1722 340.1214\n21 387.1722 340.1214\n22 387.1722 340.1214\n\n$`12`$upper\nTime Series:\nStart = 13 \nEnd = 22 \nFrequency = 1 \n        80%      95%\n13 568.7308 603.5267\n14 564.9341 611.9848\n15 564.9341 611.9848\n16 564.9341 611.9848\n17 564.9341 611.9848\n18 564.9341 611.9848\n19 564.9341 611.9848\n20 564.9341 611.9848\n21 564.9341 611.9848\n22 564.9341 611.9848\n\ndev.off()\n\nnull device \n          1 \n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-03/index.html",
    "href": "posts/rtip-2023-03-03/index.html",
    "title": "Simple examples of pmap() from {purrr}",
    "section": "",
    "text": "Introduction\nThe pmap() function in R is part of the purrr library, which is a package designed to make it easier to work with functions that operate on vectors, lists, and other types of data structures.\nThe pmap() function is used to apply a function to a list of arguments, where each element in the list contains the arguments for a single function call. The function is applied in parallel, meaning that each call is executed concurrently, which can help speed up computations when working with large datasets.\nHere is the basic syntax of the pmap() function:\n\npmap(.l, .f, ...)\n\nwhere:\n\n.l - is a list of arguments, where each element of the list contains the arguments for a single function call.\n.f - is the function to apply to the arguments in .l.\n... - is used to pass additional arguments to .f.\n\nThe pmap() function returns a list, where each element of the list contains the output of a single function call.\nLet’s define a function for an example.\n\n\nFunction\n\nmy_function <- function(a, b, c) {\n  # do something with a, b, and c\n  return(a + b + c)\n}\n\nA very simple function that just adds up the elements passed.\nNow let’s go over a couple simple examples.\n\n\nExample\n\nlibrary(purrr)\nlibrary(TidyDensity)\n\n\n# create a list of vectors with your arguments\nmy_args <- list(\n  c(1, 2, 3),\n  c(4, 5, 6),\n  c(7, 8, 9)\n)\n\n# apply your function to each combination of arguments in parallel\nresults <- pmap(my_args, my_function)\n\n# print the results\nprint(results)\n\n[[1]]\n[1] 12\n\n[[2]]\n[1] 15\n\n[[3]]\n[1] 18\n\n\nNow lets see a couple more examples.\n\nargsl <- list(\n  c(100, 100, 100, 100), # this is .n\n  c(0,1,2,3),            # this is .mean\n  c(4,3,2,1),            # this is .sd\n  c(10,10,10,10)         # this is .num_sims\n)\n\npmap(argsl, tidy_normal)\n\n[[1]]\n# A tibble: 1,000 × 7\n   sim_number     x      y    dx        dy     p      q\n   <fct>      <int>  <dbl> <dbl>     <dbl> <dbl>  <dbl>\n 1 1              1  3.56  -15.0 0.0000353 0.814  3.56 \n 2 1              2 -0.433 -14.6 0.0000679 0.457 -0.433\n 3 1              3 -1.93  -14.3 0.000125  0.315 -1.93 \n 4 1              4  1.68  -14.0 0.000219  0.663  1.68 \n 5 1              5  4.18  -13.7 0.000369  0.852  4.18 \n 6 1              6  0.805 -13.4 0.000596  0.580  0.805\n 7 1              7  7.99  -13.1 0.000922  0.977  7.99 \n 8 1              8 -1.61  -12.8 0.00137   0.344 -1.61 \n 9 1              9  1.83  -12.5 0.00195   0.676  1.83 \n10 1             10  6.66  -12.1 0.00267   0.952  6.66 \n# … with 990 more rows\n\n[[2]]\n# A tibble: 1,000 × 7\n   sim_number     x      y    dx        dy      p      q\n   <fct>      <int>  <dbl> <dbl>     <dbl>  <dbl>  <dbl>\n 1 1              1 -0.335 -9.02 0.0000814 0.328  -0.335\n 2 1              2  2.00  -8.82 0.000162  0.630   2.00 \n 3 1              3 -0.238 -8.62 0.000304  0.340  -0.238\n 4 1              4  1.17  -8.41 0.000544  0.523   1.17 \n 5 1              5  1.50  -8.21 0.000921  0.567   1.50 \n 6 1              6  4.68  -8.01 0.00148   0.890   4.68 \n 7 1              7  4.59  -7.81 0.00227   0.884   4.59 \n 8 1              8 -1.18  -7.61 0.00331   0.233  -1.18 \n 9 1              9  2.35  -7.40 0.00460   0.673   2.35 \n10 1             10 -3.73  -7.20 0.00610   0.0574 -3.73 \n# … with 990 more rows\n\n[[3]]\n# A tibble: 1,000 × 7\n   sim_number     x      y    dx       dy     p      q\n   <fct>      <int>  <dbl> <dbl>    <dbl> <dbl>  <dbl>\n 1 1              1  4.42  -3.98 0.000118 0.886  4.42 \n 2 1              2  2.24  -3.86 0.000211 0.547  2.24 \n 3 1              3 -0.207 -3.73 0.000369 0.135 -0.207\n 4 1              4  3.32  -3.61 0.000622 0.745  3.32 \n 5 1              5  0.999 -3.48 0.00101  0.308  0.999\n 6 1              6  4.08  -3.36 0.00160  0.851  4.08 \n 7 1              7  5.81  -3.23 0.00244  0.972  5.81 \n 8 1              8  6.11  -3.11 0.00362  0.980  6.11 \n 9 1              9  2.30  -2.98 0.00518  0.560  2.30 \n10 1             10  0.231 -2.86 0.00718  0.188  0.231\n# … with 990 more rows\n\n[[4]]\n# A tibble: 1,000 × 7\n   sim_number     x     y      dx       dy       p     q\n   <fct>      <int> <dbl>   <dbl>    <dbl>   <dbl> <dbl>\n 1 1              1 3.41  -0.635  0.000128 0.658   3.41 \n 2 1              2 0.415 -0.557  0.000243 0.00487 0.415\n 3 1              3 3.24  -0.479  0.000440 0.593   3.24 \n 4 1              4 3.73  -0.401  0.000758 0.768   3.73 \n 5 1              5 4.22  -0.324  0.00124  0.889   4.22 \n 6 1              6 3.70  -0.246  0.00193  0.757   3.70 \n 7 1              7 4.35  -0.168  0.00288  0.911   4.35 \n 8 1              8 1.50  -0.0899 0.00408  0.0672  1.50 \n 9 1              9 2.58  -0.0120 0.00551  0.336   2.58 \n10 1             10 3.41   0.0658 0.00713  0.661   3.41 \n# … with 990 more rows\n\npmap(argsl, tidy_normal) |>\n  map(tidy_autoplot)\n\n[[1]]\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-06/index.html",
    "href": "posts/rtip-2023-03-06/index.html",
    "title": "Simple examples of imap() from {purrr}",
    "section": "",
    "text": "Introduction\nThe imap() function is a powerful tool for iterating over a list or a vector while also keeping track of the index or names of the elements. This function applies a given function to each element of a list, along with the name or index of that element, and returns a new list with the results.\nThe imap() function takes two main arguments: x and .f. x is the list or vector to iterate over, and .f is the function to apply to each element. The .f function takes two arguments: x and i, where x is the value of the element and i is the index or name of the element.\n\n\nFunction\nHere is the imap() function.\n\nimap(.x, .f, ...)\n\nHere is the documentation from the function page:\n\n.x - A list or atomic vector.\n.f - A function, specified in one of the following ways:\n\nA named function, e.g. paste.\nAn anonymous function, e.g. (x, idx) x + idx or function(x, idx) x + idx.\nA formula, e.g. ~ .x + .y. You must use .x to refer to the current element and .y to refer to the current index. Only recommended if you require backward compatibility with older versions of R.\n\n... - Additional arguments passed on to the mapped function. We now generally recommend against using … to pass additional (constant) arguments to .f. Instead use a shorthand anonymous function:\n\n\n# Instead of\nx |> map(f, 1, 2, collapse = \",\")\n# do:\nx |> map(\\(x) f(x, 1, 2, collapse = \",\"))\n\nThis makes it easier to understand which arguments belong to which function and will tend to yield better error messages.\n\n\nExample\nHere’s an example of using imap() with a simple list of integers:\n\nlibrary(purrr)\n\n# create a list of integers\nmy_list <- list(1, 2, 3, 4, 5)\n\n# define a function to apply to each element of the list\nmy_function <- function(x, i) {\n  paste(\"The element at index\", i, \"is\", x)\n}\n\n# apply the function to each element of the list using imap()\nresult <- imap(my_list, my_function)\n\n# print the result\nprint(result)\n\n[[1]]\n[1] \"The element at index 1 is 1\"\n\n[[2]]\n[1] \"The element at index 2 is 2\"\n\n[[3]]\n[1] \"The element at index 3 is 3\"\n\n[[4]]\n[1] \"The element at index 4 is 4\"\n\n[[5]]\n[1] \"The element at index 5 is 5\"\n\n\nIn this example, we create a list of integers called my_list. We define a function called my_function that takes two arguments: x, which is the value of each element in the list, and i, which is the index of that element. We then use imap() to apply my_function to each element of my_list, passing both the value and the index of the element as arguments. The result is a new list where each element contains the output of my_function applied to the corresponding element of my_list.\nNow let’s take a look at a slightly more complex example. In this case, we will use imap() to iterate over a list of data frames, apply a function to each data frame that subsets the data to include only certain columns, and return a new list of data frames with the subsetted data.\n\n# create a list of data frames\nmy_list <- list(\n  data.frame(x = 1:5, y = c(\"a\", \"b\", \"c\", \"d\", \"e\")),\n  data.frame(x = 6:10, y = c(\"f\", \"g\", \"h\", \"i\", \"j\")),\n  data.frame(x = 11:15, y = c(\"k\", \"l\", \"m\", \"n\", \"o\"))\n)\n\n# define a function to apply to each element of the list\nmy_function <- function(df, i) {\n  # subset the data to include only the x column\n  df_subset <- df[, \"x\", drop = FALSE]\n  # rename the column to include the index of the element\n  colnames(df_subset) <- paste(\"x_\", i, sep = \"\")\n  # return the subsetted data frame\n  return(df_subset)\n}\n\n# apply the function to each element of the list using imap\nresult <- imap(my_list, my_function)\n\n# print the result\nprint(result)\n\n[[1]]\n  x_1\n1   1\n2   2\n3   3\n4   4\n5   5\n\n[[2]]\n  x_2\n1   6\n2   7\n3   8\n4   9\n5  10\n\n[[3]]\n  x_3\n1  11\n2  12\n3  13\n4  14\n5  15\n\n\nIn this example, we create a list of three data frames called my_list. We define a function called my_function that takes two arguments: df, which is the value of each element in the list (a data frame), and i, which is the index of that element. The function subsets the data frame to include only the x column, renames the column to include the index of the element, and returns the subsetted data frame.\nWe use imap() to apply my_function to each element of my_list, passing both the data frame and the index of the element as arguments. The result is a new list of data frames, where each data frame contains only the x column from the original data frame, with a new name that includes the index of the element.\nAs you can see, the output is a list of three data frames, each containing only the x column from the corresponding original data frame, with a new name that includes the index of the element.\nIn summary, the imap() function from the R library purrr is a useful tool for iterating over a list or a vector while also keeping track of the index or names of the elements. The function takes a list or a vector as its first argument, and a function as its second argument, which takes two arguments: the value of each element, and the index or name of that element. The function returns a new list or vector with the results of applying the function to each element of the original list or vector. This function is particularly useful for complex data structures, where the index or name of each element is important for further data analysis or processing.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-07/index.html",
    "href": "posts/rtip-2023-03-07/index.html",
    "title": "tidy_bernoulli() with {data.table}",
    "section": "",
    "text": "Introduction\nSo I was challanged by Adrian Antico to learn data.table, so yesterday I started with a single function from my package {TidyDensity} called tidy_bernoulli().\nSo let’s see how I did (hint, works but needs a lot of improvement, so I’ll learn it.)\n\n\nFunction\nLet’s see the function in data.table\n\nlibrary(data.table)\nlibrary(tidyr)\nlibrary(stats)\nlibrary(purrr)\n\nnew_func <- function(num_sims, n, pr) {\n\n  # Create a data.table with one row per simulation\n  sim_data <- data.table(sim_number = factor(seq(1, num_sims, 1)))\n  \n  # Group the data by sim_number and add columns for x and y\n  sim_data[, `:=` (\n    x = list(1:n),\n    y = list(stats::rbinom(n = n, size = 1, prob = pr))\n  ), by = sim_number]\n  \n  # Compute the density of the y values and add columns for dx and dy\n  sim_data[, `:=` (\n    d = list(density(unlist(y), n = n)[c(\"x\", \"y\")] |>\n      set_names(\"dx\", \"dy\") |>\n      as_tibble())\n  ), by = sim_number]\n  \n  # Compute the p-values for the y values and add a column for p\n  sim_data[, `:=` (\n    p = list(stats::pbinom(unlist(y), size = 1, prob = pr))\n  ), by = sim_number]\n  \n  # Compute the q-values for the p-values and add a column for q\n  sim_data[, `:=` (\n    q = list(stats::qbinom(unlist(p), size = 1, prob = pr))\n  ), by = sim_number]\n  \n  # Unnest the columns for x, y, d, p, and q\n  sim_data <- sim_data[, \n                       unnest(\n                         .SD, \n                         cols = c(\"x\", \"y\", \"d\", \"p\", \"q\")\n                         ), \n                       by = sim_number]\n  \n  # Remove the grouping\n  sim_data[, sim_number := as.factor(sim_number)]\n  \n  return(sim_data)\n}\n\n\n\nExample\nNow, let’s see the output of the original function tidy_bernoulli() and new_func().\n\nlibrary(TidyDensity)\nn <- 50\npr <- 0.1\nsims <- 5\n\nset.seed(123)\ntb <- tidy_bernoulli(.n = n, .prob = pr, .num_sims = sims)\n\nset.seed(123)\nnf <- new_func(n = n, num_sims = sims, pr = pr)\n\nprint(tb)\n\n# A tibble: 250 × 7\n   sim_number     x     y      dx     dy     p     q\n   <fct>      <int> <int>   <dbl>  <dbl> <dbl> <dbl>\n 1 1              1     0 -0.405  0.0292   0.9     0\n 2 1              2     0 -0.368  0.0637   0.9     0\n 3 1              3     0 -0.331  0.129    0.9     0\n 4 1              4     0 -0.294  0.243    0.9     0\n 5 1              5     1 -0.258  0.424    1       1\n 6 1              6     0 -0.221  0.688    0.9     0\n 7 1              7     0 -0.184  1.03     0.9     0\n 8 1              8     0 -0.147  1.44     0.9     0\n 9 1              9     0 -0.110  1.87     0.9     0\n10 1             10     0 -0.0727 2.25     0.9     0\n# … with 240 more rows\n\nprint(nf)\n\n     sim_number  x y         dx          dy   p q\n  1:          1  1 0 -0.4053113 0.029196114 0.9 0\n  2:          1  2 0 -0.3683598 0.063683226 0.9 0\n  3:          1  3 0 -0.3314083 0.129227066 0.9 0\n  4:          1  4 0 -0.2944568 0.242967496 0.9 0\n  5:          1  5 1 -0.2575054 0.424395426 1.0 1\n ---                                             \n246:          5 46 0  1.2575054 0.057872104 0.9 0\n247:          5 47 0  1.2944568 0.033131931 0.9 0\n248:          5 48 1  1.3314083 0.017621873 1.0 1\n249:          5 49 1  1.3683598 0.008684076 1.0 1\n250:          5 50 0  1.4053113 0.003981288 0.9 0\n\n\nOk so at least the output is identical which is a good sign. Now let’s benchmark the two solutions.\n\nlibrary(rbenchmark)\nlibrary(dplyr)\n\nbenchmark(\n  \"original\" = {\n    tidy_bernoulli(.n = n, .prob = pr, .num_sims = sims)\n  },\n  \"data.table\" = {\n    new_func(n = n, pr = pr, num_sims = sims)\n  },\n  replications = 100,\n  columns = c(\"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\"  )\n) |>\n  arrange(relative)\n\n        test replications elapsed relative user.self sys.self\n1   original          100    3.29     1.00      2.51     0.08\n2 data.table          100    4.64     1.41      3.34     0.04\n\n\nYeah, needs some work but it’s a start."
  },
  {
    "objectID": "posts/rtip-2023-03-08/index.html",
    "href": "posts/rtip-2023-03-08/index.html",
    "title": "Getting NYS Home Heating Oil Prices with {rvest}",
    "section": "",
    "text": "Introduction\nIf you live in New York and rely on heating oil to keep your home warm during the colder months, you know how important it is to keep track of heating oil prices. Fortunately, with a bit of R code, you can easily access the latest heating oil prices in New York.\nThe code uses the {dplyr} package to clean and manipulate the data, as well as the {timetk} package to plot the time series. Here’s a breakdown of what the code does:\n\nFirst, it loads the necessary packages and sets the URL for the data source.\nNext, it reads the HTML from the URL using the read_html function from the xml2 package.\nIt then uses the html_node function from the rvest package to extract the HTML node that contains the data table.\n\nThe resulting data table is then cleaned and transformed using dplyr functions such as html_table, as_tibble, set_names, select, mutate, and arrange.\nFinally, the resulting time series data is plotted using plot_time_series from the timetk package.\nTo run this code, you will need to have these packages installed on your machine. You can install them using the install.packages function in R. Here’s how you can install the packages:\n\ninstall.packages(\"dplyr\")\ninstall.packages(\"xml2\")\ninstall.packages(\"rvest\")\ninstall.packages(\"tibble\")\ninstall.packages(\"purrr\")\ninstall.packages(\"lubridate\")\ninstall.packages(\"timetk\")\n\nOnce you have installed the packages, you can copy and paste the code into your R console or RStudio and run it to get the latest heating oil prices in New York.\nIn conclusion, the code above provides a simple and efficient way to access and visualize heating oil prices in New York using R. By keeping track of these prices, you can make informed decisions about when to buy heating oil and how much to purchase, ultimately saving you money on your heating bills.\n\n\nExample\nNow let’s run it!\n\nurl  <- \"https://www.eia.gov/opendata/qb.php?sdid=PET.W_EPD2F_PRS_SNY_DPG.W\"\npage <- xml2::read_html(url)\nnode <- rvest::html_node(\n    x = page\n    , xpath = \"/html/body/div[1]/section/div/div/div[2]/div[1]/table\"\n)\nny_tbl <- node |>\n    rvest::html_table() |>\n    tibble::as_tibble() |>\n    purrr::set_names('series_name','period','frequency','value','units') |>\n    dplyr::select(period, frequency, value, units, series_name) |>\n    dplyr::mutate(period = lubridate::ymd(period)) |>\n    dplyr::arrange(period)\n\nny_tbl |>\n    timetk::plot_time_series(.date_var = period, .value = value)\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-09/index.html",
    "href": "posts/rtip-2023-03-09/index.html",
    "title": "Multiple Solutions to speedup tidy_bernoulli() with {data.table}",
    "section": "",
    "text": "Introduction\nI had just recently posted on making an attempt to speedup computations with my package {TidyDensity} using a purely data.table solution, yes of course I can use {dtplyr} or {tidytable} but that not the challenge put to me.\nMy original attempt was worse than the original solution of tidy_bernoulli(). After I posted on Mastadon, LinkedIn and Reddit, I recieved potential solutions from each site by users. Let’s check them out below.\n\n\nFunction\nFirst let’s load in the necessary libraries.\n\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(rbenchmark)\nlibrary(TidyDensity)\n\nNow let’s look at the different solutions.\n\n# My original new function\nnew_func <- function(num_sims, n, pr) {\n\n  # Create a data.table with one row per simulation\n  sim_data <- data.table(sim_number = factor(seq(1, num_sims, 1)))\n\n  # Group the data by sim_number and add columns for x and y\n  sim_data[, `:=` (\n    x = list(1:n),\n    y = list(stats::rbinom(n = n, size = 1, prob = pr))\n  ), by = sim_number]\n\n  # Compute the density of the y values and add columns for dx and dy\n  sim_data[, `:=` (\n    d = list(density(unlist(y), n = n)[c(\"x\", \"y\")] |>\n               set_names(\"dx\", \"dy\") |>\n               as_tibble())\n  ), by = sim_number]\n\n  # Compute the p-values for the y values and add a column for p\n  sim_data[, `:=` (\n    p = list(stats::pbinom(unlist(y), size = 1, prob = pr))\n  ), by = sim_number]\n\n  # Compute the q-values for the p-values and add a column for q\n  sim_data[, `:=` (\n    q = list(stats::qbinom(unlist(p), size = 1, prob = pr))\n  ), by = sim_number]\n\n  # Unnest the columns for x, y, d, p, and q\n  sim_data <- sim_data[,\n                       unnest(\n                         .SD,\n                         cols = c(\"x\", \"y\", \"d\", \"p\", \"q\")\n                       ),\n                       by = sim_number]\n\n  # Remove the grouping\n  sim_data[, sim_number := as.factor(sim_number)]\n\n  return(sim_data)\n}\n\nreddit_func <- function(num_sims, n, pr) {\n  sim_dat <- data.table(sim_number = rep(1:num_sims,each=n),\n                        x          = rep(1:n,num_sims))\n\n  sim_dat[, y := stats::rbinom(n = n, size = 1, prob = pr), by=sim_number]\n  sim_dat[, c(\"dx\",\"dy\") := density(y,n=n)[c(\"x\",\"y\")]    , by=sim_number]\n  sim_dat[, p := stats::pbinom(y, size = 1, prob = pr)    , by=sim_number]\n  sim_dat[, q := stats::qbinom(p, size = 1, prob = pr)    , by=sim_number]\n  \n  return(sim_dat)\n}\n\nmastadon_func <- function(num_sims, n, pr){\n  sim_data <- data.table(sim_number = 1:num_sims\n  )[, `:=`( x = .(1:n), y= .(rbinom(n = n, size = 1, prob = pr))), sim_number\n  ][, `:=`( d = .(density(unlist(y), n = n)[c('x','y')] |> \n                    as.data.table() |> \n                    setnames(c('dx','dy'))\n                  )\n            ), sim_number\n  ][, `:=`( p = .(pbinom(unlist(y), size = 1, prob = pr))), sim_number\n  ][, `:=`( q = .(qbinom(unlist(p), size = 1, prob = pr))), sim_number]\n\n    cbind(\n      sim_data[, lapply(.SD, unlist), by = sim_number, .SDcol = c('x','y','p','q')],\n      rbindlist(sim_data$d)\n    ) |>\n    setcolorder(c('sim_number','x','y','dx','dy'))\n    \n    return(sim_data)\n}\n\nlinkedin_func <- function(num_sims, n, pr) {\n\n  # Create a data.table with one row per simulation\n  sim_data <- CJ(sim_number = factor(1:num_sims), x = 1:n)\n\n  # Group the data by sim_number and add columns for x and y\n  sim_data[, y := stats::rbinom(n = .N, size = 1, prob = pr)]\n\n\n  # Compute the density of the y values and add columns for dx and dy\n  sim_data[, c(\"dx\", \"dy\") := density(y, n = n)[c(\"x\", \"y\")], by = sim_number]\n\n  # Compute the p-values for the y values and add a column for p\n  sim_data[, p := stats::pbinom(y, size = 1, prob = pr)]\n\n  # Compute the q-values for the p-values and add a column for q\n  sim_data[, q := stats::qbinom(p, size = 1, prob = pr)]\n  setkey(sim_data, NULL) # needed only to compare with new_func\n  return(sim_data)\n}\n\nAll of the functions work in the same set of three arguments as input: * num_sims: an integer value that specifies the number of simulations to run * n: an integer value that specifies the sample size * pr: a numeric value that specifies the probability of success\nThe functions use the data.table package to create a data table named sim_dat/sim_data. The data table has two columns: sim_number and x. The sim_number column represents the simulation number, and x column represents the observation number.\nThe functions then generate random binary data using the rbinom function from the stats package. The function generates n binary data points for each simulation number (sim_number) using the input parameter pr as the probability of success. The resulting binary data points are stored in the y column of sim_dat/data.\nNext, the function calculates the density of y using the density function from the stats package. The function calculates the density separately for each simulation number (sim_number) and stores the resulting values in the dx and dy columns of sim_dat/data.\nThe functions then calculate the cumulative probability (p) of each binary data point using the pbinom function from the stats package. The function calculates the cumulative probability separately for each simulation number (sim_number) and stores the resulting values in the p column of sim_dat.\nFinally, the functions calculate the inverse of the cumulative probability (q) using the qbinom function from the stats package. The function calculates the inverse of the cumulative probability separately for each simulation number (sim_number) and stores the resulting values in the q column of sim_dat.\nThe functions then return the data table containing the results of the simulations.\n\n\nExample\nHow do they stack up to each other? Lets see!\n\nn <- 50\npr <- 0.1\nnum_sims <- sims <- 5\n\nbenchmark(\n  \"tidy_bernoulli()\" = {\n    tidy_bernoulli(.n = n, .prob = pr, .num_sims = sims)\n  },\n  \"my.first.attempt\" = {\n    new_func(n = n, pr = pr, num_sims = sims)\n  },\n  \"linkedin.attempt\" = {\n    linkedin_func(n = n, pr = pr, num_sims = sims)\n  },\n  \"mastadon.attempt\" = {\n    mastadon_func(n = n, pr = pr, num_sims = sims)\n  },\n  \"reddit.attempt\" = {\n    reddit_func(n = n, pr = pr, num_sims = sims)\n  },\n  replications = 200,\n  columns = c(\"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\"  )\n) |>\n  arrange(relative)\n\n              test replications elapsed relative user.self sys.self\n1 linkedin.attempt          200    0.84    1.000      0.64     0.00\n2   reddit.attempt          200    0.86    1.024      0.69     0.03\n3 mastadon.attempt          200    1.57    1.869      1.17     0.08\n4 tidy_bernoulli()          200    6.47    7.702      4.68     0.11\n5 my.first.attempt          200    8.82   10.500      6.67     0.01\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2023-03-10/index.html",
    "href": "posts/rtip-2023-03-10/index.html",
    "title": "Use of the apply family of functions",
    "section": "",
    "text": "In this post I will talk about the use of the R functions apply(), lapply(), sapply(), tapply(), and vapply() with examples.\nThese functions are all designed to help users apply a function to a set of data in R, but they differ in their input and output types, as well as in the way they handle missing values and other complexities. By using the right function for your particular problem, you can make your code more efficient and easier to read.\nLet’s start with the basics.\n\n\nBefore we dive into the details of each function, let’s define some terms:\n\nA vector is a one-dimensional array of data, like a list of numbers or strings.\nA matrix is a two-dimensional array of data, like a table of numbers.\nA data frame is a two-dimensional object that can hold different types of data, like a spreadsheet.\nA list is a collection of objects, which can be of different types, like a shopping bag full of different items.\n\nEach of the five functions we’ll discuss here takes a list as input (although some can also take vectors or matrices). Let’s create a list object to use in our examples:\n\nmy_list <- list(\n  a = c(1, 2, 3),\n  b = matrix(1:6, nrow = 2),\n  c = data.frame(x = 1:3, y = c(\"a\", \"b\", \"c\")),\n  d = c(4, NA, 6),\n  e = list(\"foo\", \"bar\", \"baz\")\n)\n\nThis list contains five elements:\n\nA vector of numbers (a)\nA matrix of numbers (b)\nA data frame with two columns (c)\nA vector of numbers with a missing value (d)\nA list of character strings (e)\n\nNow that we have our data, let’s look at each of the functions in turn.\n\n\n\napply()\nThe apply() function applies a function to the rows or columns of a matrix or array. It is most commonly used with matrices, but can also be used with higher-dimensional arrays. The function takes three arguments:\n\nThe matrix or array to apply the function to\nThe margin (1 for rows, 2 for columns, or a vector of dimensions)\nThe function to apply\n\nLet’s apply the mean() function to the columns of our matrix in my_list$b:\n\napply(my_list$b, 2, mean)\n\n[1] 1.5 3.5 5.5\n\n\nThis will return a vector of means for each column of the matrix\nlapply()\nThe lapply() function applies a function to each element of a list and returns a list of the results. It takes two arguments:\n\nThe list to apply the function to\nThe function to apply\n\nLet’s apply the class() function to each element of our list:\n\nlapply(my_list, class)\n\n$a\n[1] \"numeric\"\n\n$b\n[1] \"matrix\" \"array\" \n\n$c\n[1] \"data.frame\"\n\n$d\n[1] \"numeric\"\n\n$e\n[1] \"list\"\n\n\nThis will return a list of the classes of each element.\nsapply()\nThe sapply() function is similar to lapply(), but it simplifies the output to a vector or matrix if possible. It takes the same two arguments as lapply():\n\nThe list to apply the function\nThe function to apply\n\nLet’s apply the length() function to each element of our list using sapply():\n\nsapply(my_list, length)\n\na b c d e \n3 6 2 3 3 \n\n\nThis will return a vector of lengths for each element.\ntapply()\nThe tapply() function applies a function to subsets of a vector or data frame, grouped by one or more factors. It takes three arguments:\n\nThe vector or data frame to apply the function to\nThe factor(s) to group the data by\nThe function to apply\n\nLet’s apply the mean() function to the elements of our vector my_list$d, grouped by whether they are missing or not:\n\ntapply(my_list$d, !is.na(my_list$d), mean)\n\nFALSE  TRUE \n   NA     5 \n\n\nThis will return a vector of means for each group where they are NOT NA.\nvapply()\nThe vapply() function is similar to sapply(), but allows the user to specify the output type and length, making it more efficient and less prone to errors. It takes four arguments:\n\nThe list to apply the function to\nThe function to apply\nThe output type of the function\nThe length of the output vector or matrix\n\nLet’s apply the length() function to each element of our list, specifying that the output type is an integer and the length is 1:\n\nvapply(my_list, length, integer(1))\n\na b c d e \n3 6 2 3 3 \n\n\nThis will return a matrix of lengths for each element, with 1 row:"
  },
  {
    "objectID": "posts/rtip-2023-03-17/index.html",
    "href": "posts/rtip-2023-03-17/index.html",
    "title": "Use of the apply family of functions",
    "section": "",
    "text": "I am thrilled to announce that the R universe of packages {healthyverse} has surpassed 60,000 downloads! Thank you to everyone who has downloaded and used these packages, your support is greatly appreciated.\nFor those who are not familiar, the {healthyverse} package is a collection of R packages focused on data science and analysis with an emphasis on healthcare. These packages are ever evolving and for the most part still in an experimental stage, but are maturing. These packages are:\n\n{healthyR}\n{healthyR.ts}\n{healthyR.ai}\n{healthyR.data}\n{TidyDensity}\n{tidyAML}\n\nI am continuously working on updates and improvements to the {healthyverse} package, and I hope to release them soon. Some of the updates include bug fixes, new functionality, and enhancements to existing functions.\nAdditionally, I want to encourage users who are interested in contributing to the {healthyverse} package to submit pull requests. Contributions can be in the form of bug fixes, new functions, or enhancements to existing ones. I am always open to feedback and suggestions on how to improve these packages.\nOnce again, thank you to everyone who has downloaded and used the {healthyverse} package. Your support motivates me to continue working on this project and making it the best it can be.\n\n\n\n60k"
  },
  {
    "objectID": "posts/rtip-2023-03-21/index.html",
    "href": "posts/rtip-2023-03-21/index.html",
    "title": "Getting the CCI30 Index Current Makeup",
    "section": "",
    "text": "Introduction\nThe CCI30 Crypto Index is a cryptocurrency index that tracks the performance of the top 30 cryptocurrencies by market capitalization. It was created in 2017 by a team of researchers and analysts from the CryptoCompare and MVIS indices.\nThe CCI30 Crypto Index is designed to provide a broad-based and representative measure of the cryptocurrency market’s overall performance. It includes a diverse range of cryptocurrencies, such as Bitcoin, Ethereum, Litecoin, Ripple, and many others. The index is weighted by market capitalization, with each cryptocurrency’s weight determined by its market capitalization relative to the total market capitalization of all 30 cryptocurrencies.\nThe CCI30 Crypto Index has become a popular benchmark for the cryptocurrency market, as it offers a comprehensive view of the market’s performance, rather than just focusing on one particular cryptocurrency. It is often used by investors, traders, and researchers to analyze trends and make investment decisions.\nOne notable feature of the CCI30 Crypto Index is that it is rebalanced every quarter. This means that the composition of the index is adjusted to reflect changes in the market capitalization of the constituent cryptocurrencies. This helps to ensure that the index remains representative of the overall cryptocurrency market.\nOverall, the CCI30 Crypto Index provides a useful tool for tracking the performance of the cryptocurrency market. It is a valuable resource for investors, traders, and researchers who are interested in this exciting and rapidly evolving field.\n\n\nCode Explanation\nLet’s break it down step by step:\n\nThe first line of the code loads the “dplyr” package, which provides a set of functions for data manipulation.\nThe second line of the code reads the HTML code from the website “https://cci30.com/” using the “read_html” function from the “xml2” package.\nThe next two blocks of code extract two tables from the HTML document using the “html_node” function from the “rvest” package. The tables are located at two different XPaths in the HTML document.\nThe extracted tables are then converted into tibbles using the “as_tibble” function from the “tibble” package. The tibbles are further transformed by selecting only the columns from the second to the fifth column using the “select” function from the “dplyr” package.\nThe column names of the tibbles are then set using the “set_names” function from the “purrr” package.\nFinally, the two tibbles are combined using the “union” function from the “dplyr” package, and the resulting tibble is printed to the console.\n\nIn summary, the code is extracting two tables from a website, transforming them into tibbles, selecting a subset of columns, renaming the columns, and combining them into a single tibble.\n\n\nExample\n\ncci30 <- xml2::read_html(\"https://cci30.com/\")\n\ntbl1 <- cci30 |>\n    rvest::html_node(xpath = \"/html/body/div[2]/div/div/div/div[2]/div[1]/table\") |>\n    rvest::html_table(header = 1) |>\n    tibble::as_tibble() |>\n    dplyr::select(2:5) |>\n    purrr::set_names(\n        \"Coin\",\"Price\",\"Mkt Cap\",\"Daily Change\"\n    )\n\ntbl2 <- cci30 |>\n    rvest::html_node(xpath = \"/html/body/div[2]/div/div/div/div[2]/div[2]/table\") |>\n    rvest::html_table(header = 1) |>\n    tibble::as_tibble() |>\n    dplyr::select(2:5) |>\n    purrr::set_names(\n        \"Coin\",\"Price\",\"Mkt Cap\",\"Daily Change\"\n    )\n\ntbl <- tbl1 |>\n  dplyr::union(tbl2) |>\n  knitr::kable()\n\ntbl\n\n\n\n\nCoin\nPrice\nMkt Cap\nDaily Change\n\n\n\n\nBitcoin\n$27,767.24\n$536,553,055,078\n0.17%\n\n\nEthereum\n$1,735.32\n$212,357,972,798\n0.04%\n\n\nBNB\n$332.92\n$52,565,516,823\n0.13%\n\n\nXRP\n$0.37\n$19,087,613,742\n0.13%\n\n\nCardano\n$0.33\n$11,547,419,916\n0.05%\n\n\nPolygon\n$1.10\n$9,643,536,324\n0.17%\n\n\nDogecoin\n$0.07\n$9,484,198,878\n0.34%\n\n\nSolana\n$22.18\n$8,507,167,040\n0.12%\n\n\nPolkadot\n$6.10\n$7,119,808,610\n0.08%\n\n\nShiba Inu\n$0.00\n$6,169,390,592\n0.24%\n\n\nTRON\n$0.07\n$5,936,468,687\n0.14%\n\n\nLitecoin\n$78.42\n$5,685,409,727\n0.40%\n\n\nAvalanche\n$16.64\n$5,418,625,875\n0.06%\n\n\nUniswap\n$6.19\n$4,716,487,304\n0.19%\n\n\nChainlink\n$7.06\n$3,649,558,739\n0.06%\n\n\nCosmos\n$11.56\n$3,309,216,299\n0.03%\n\n\nUNUS SED LEO\n$3.35\n$3,195,413,769\n-0.55%\n\n\nToncoin\n$2.38\n$2,907,590,168\n-0.62%\n\n\nMonero\n$151.58\n$2,767,118,876\n-0.01%\n\n\nEthereum Classic\n$19.58\n$2,741,016,944\n-6.84%\n\n\nOKB\n$44.34\n$2,660,455,327\n0.56%\n\n\nBitcoin Cash\n$130.60\n$2,526,173,508\n-0.48%\n\n\nStellar\n$0.09\n$2,293,156,708\n-0.04%\n\n\nCronos\n$0.07\n$1,787,408,658\n1.05%\n\n\nNEAR Protocol\n$2.00\n$1,728,135,015\n0.26%\n\n\nVeChain\n$0.02\n$1,665,251,562\n0.33%\n\n\nQuant\n$126.33\n$1,525,183,149\n0.31%\n\n\nInternet Computer\n$5.11\n$1,516,076,264\n0.19%\n\n\nAlgorand\n$0.21\n$1,498,361,340\n0.41%\n\n\nApeCoin\n$4.06\n$1,496,070,125\n0.12%"
  },
  {
    "objectID": "posts/rtip-2023-03-22/index.html",
    "href": "posts/rtip-2023-03-22/index.html",
    "title": "Some Examples of Cumulative Mean with {TidyDensity}",
    "section": "",
    "text": "Introduction\nCumulative mean is a statistical measure that calculates the mean of a set of numbers up to a certain point in time or after a certain number of observations. It is also known as a running average or moving average.\nCumulative mean can be useful in a variety of contexts. For example:\n\nTracking progress: Cumulative mean can be used to track progress over time. For instance, a teacher might use it to track the average test scores of her students throughout the school year.\nAnalyzing trends: Cumulative mean can help identify trends in data. For example, a business might use it to track the average revenue generated by a new product over the course of several months.\nSmoothing data: Cumulative mean can be used to smooth out fluctuations in data. For instance, a meteorologist might use it to calculate the average temperature over the course of a year, which would help to smooth out the effects of daily temperature fluctuations.\n\nIn summary, cumulative mean is a useful statistical measure that can help track progress, analyze trends, and smooth out fluctuations in data.\n\n\nFunction\nThe function we will review is cmean() from the {TidyDensity} R package. Let’s take a look at it.\n\ncmean()\n\nThe only argument is .x which is a numeric vector as this is a vectorized function. Let’s see it in use.\n\n\nExample\nFirst let’s load in TidyDensity\n\nlibrary(TidyDensity)\n\nOk now let’s make some data. For this we are going to use the simple rnorm() function.\n\nx <- rnorm(100)\n\nhead(x)\n\n[1] -0.8293250 -1.2983499  2.2782337 -0.1521549  0.6859169  0.3809020\n\n\nOk, now that we have our vector, let’s run it through the function and see what it outputs and then we will graph it.\n\ncmx <- cmean(x)\nhead(cmx)\n\n[1] -0.8293249774 -1.0638374319  0.0501862766 -0.0003990095  0.1368641726\n[6]  0.1775371452\n\n\nNow let’s graph it.\n\nplot(cmx, type = \"l\")\n\n\n\n\nOk nice, so can we do this on grouped data or lists of data? Of course! First let’s use a for loop to generate a list of rnorm() values.\n\n# Initialize an empty list to store the generated values\nmy_list <- list()\n\n# Generate values using rnorm(5) in a for loop and store them in the list\nfor (i in 1:5) {\n  my_list[[i]] <- rnorm(100)\n}\n\n# Print the generated list\npurrr::map(my_list, head)\n\n[[1]]\n[1] -0.8054353 -0.4596541 -0.2362475  1.1486398 -0.7242154  0.5184610\n\n[[2]]\n[1]  0.3243327  0.7170802 -0.5963424 -1.0307104  0.3388504  0.5717486\n\n[[3]]\n[1]  1.7360816 -1.0359467 -0.3206138 -1.2157684 -0.8841356  0.1856481\n\n[[4]]\n[1] -1.1401642 -0.4437817 -0.2555245 -0.1809040 -0.2131763 -0.1251750\n\n[[5]]\n[1]  0.08835903 -1.79153379 -2.15010900  0.67344844  1.06125849  0.99848796\n\n\nNow that we have our list object let’s go ahead and plot the values out after we pass the data through cmean().\n\nlibrary(purrr)\n\nmy_list |>\n  map(\\(x) x |> cmean() |> plot(type = \"l\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n[[5]]\nNULL\n\n\nFrom here I think it is easy to see how one could do this on gruoped data as well with dplyr’s group_by()."
  }
]