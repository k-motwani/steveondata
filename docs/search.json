[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Steve On Data",
    "section": "",
    "text": "Event Analysis with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGartner Magic Chart and its usefulness in healthcare analytics with {healthyR}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyr\n\n\n\n\n\n\n\n\n\n\n\nDec 29, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nSimulating Time Series Model Forecasts with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nweeklyrtip\n\n\nhealthyrts\n\n\ntimeseries\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nDec 23, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nListing Functions and Parameters\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ndplyr\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution Statistics with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Walks with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nrandomwalk\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nViewing Different Versions of the Same Statistical Distribution with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ndistributions\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Scedacity Plots with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Moving Average Plots with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution Summaries with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydesntiy\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMixture Distributions with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydesntiy\n\n\nmixturemodels\n\n\n\n\n\n\n\n\n\n\n\nDec 13, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate QQ Plots for Time Series Models with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a Faceted Historgram Plot with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhistogram\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate Multiple {parsnip} Model Specs with {purrr}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nparsnip\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZ-Score Scaling Step Recipe with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNaming Items in a List with {purrr}, {dplyr}, or {healthyR}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\nhealthyR\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuto KNN with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nweeklyrtip\n\n\nknn\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtract Boilerplate Workflow Metrics with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate Random Walk Data with {healthyR.ts}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntimeseries\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Lists\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nlists\n\n\nlapply\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefault Metric Sets with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary Statistics with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\ntidydensity\n\n\ndatatable\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Preprocessing Scale/Normalize with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrap Modeling with Base R\n\n\n\n\n\n\n\ncode\n\n\nbootstrap\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nUpdates to {healthyverse} packages\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\nhealthyverse\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrap Modeling with {purrr} and {modler}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\nmodelr\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCumulative Harmonic Mean with {TidyDensity}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuto Prep data for XGBoost with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nxgboost\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFind Skewed Features with {healthyR.ai}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nskew\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nTime Series Lag Correlation Plots\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrts\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReading Multiple Files with {purrr}\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\npurrr\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nMapping K-Means with healthyR.ai\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nkmeans\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHyperbolic Transform with healthyR.ai\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscrete Fourier Vec with healthyR.ai\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nBootstrapping and Plots with TidyDensity\n\n\n\n\n\n\n\ncode\n\n\ntidydensity\n\n\nbootstrap\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCumulative Skewness\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nPCA with healthyR.ai\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nControl Charts in healthyR.ai\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCumulative Variance\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\ncumulative\n\n\nsapply\n\n\nlapply\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nTime Series Clustering with healthyR.ts\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\nhealthyrts\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nhealthyR.ai Primer\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\nhealthyrai\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n  \n\n\n\n\nTidyDensity Primer\n\n\n\n\n\n\n\ncode\n\n\nweeklytip\n\n\ntidydensity\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple lapply()\n\n\n\n\n\n\n\ncode\n\n\nrtip\n\n\nweeklytip\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To Steve On Data\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\nSteven P. Sanderson II, MPH\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/healthyrai-20221013/index.html",
    "href": "posts/healthyrai-20221013/index.html",
    "title": "healthyR.ai Primer",
    "section": "",
    "text": "This particular article is going to focus on k-means clustering with umap projection and visualization.\nFirst things first, lets load in the library:\n\nlibrary(healthyR.ai)\n\n\n== Welcome to healthyR.ai ===========================================================================\nIf you find this package useful, please leave a star: \n   https://github.com/spsanderson/healthyR.ai'\n\nIf you encounter a bug or want to request an enhancement please file an issue at:\n   https://github.com/spsanderson/healthyR.ai/issues\n\nThank you for using healthyR.ai\n\n\n\nInformation\nK-Means is a partition algorithm initially designed for signal processing. The goal is to partition n observations into k clusters where each n is in k. The unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters.\nThe aim of this post is to showcase the use of the healthyR.ai wrapper for the kmeans function along with the wrapper and plot for the uwot::umap projection function. We will go through the entire workflow from getting the data to getting the final UMAP plot.\n\n\nGenerate some data\n\nsuppressPackageStartupMessages(library(healthyR.data))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(broom))\nsuppressPackageStartupMessages(library(ggplot2))\n\ndata_tbl <- healthyR_data %>%\n    filter(ip_op_flag == \"I\") %>%\n    filter(payer_grouping != \"Medicare B\") %>%\n    filter(payer_grouping != \"?\") %>%\n    select(service_line, payer_grouping) %>%\n    mutate(record = 1) %>%\n    as_tibble()\n\ndata_tbl %>%\n  glimpse()\n\nRows: 116,823\nColumns: 3\n$ service_line   <chr> \"Medical\", \"Schizophrenia\", \"Syncope\", \"Pneumonia\", \"Ch…\n$ payer_grouping <chr> \"Blue Cross\", \"Medicare A\", \"Medicare A\", \"Medicare A\",…\n$ record         <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\n\nNow that we have our data we need to generate what is called a user item table. To do this we use the function hai_kmeans_user_item_tbl which takes in just a few arguments. The purpose of the user item table is to aggregate and normalize the data between the users and the items.\nThe data that we have generated is going to look for clustering amongst the service_lines (the user) and the payer_grouping (item) columns.\nLets now create the user item table.\n\n\nUser Item Tibble\n\nuit_tbl <- hai_kmeans_user_item_tbl(\n  data_tbl, \n  service_line, \n  payer_grouping, \n  record\n)\n\nuit_tbl\n\n# A tibble: 23 × 12\n   service_line   Blue …¹ Comme…² Compe…³ Excha…⁴    HMO Medic…⁵ Medic…⁶ Medic…⁷\n   <chr>            <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>\n 1 Alcohol Abuse   0.0941 0.0321  5.25e-4 0.0116  0.0788 0.158    0.367   0.173 \n 2 Bariatric Sur…  0.317  0.0583  0       0.0518  0.168  0.00324  0.343   0.0485\n 3 Carotid Endar…  0.0845 0.0282  0       0       0.0141 0        0.0282  0.648 \n 4 Cellulitis      0.110  0.0339  1.18e-2 0.00847 0.0805 0.0869   0.192   0.355 \n 5 Chest Pain      0.144  0.0391  2.90e-3 0.00543 0.112  0.0522   0.159   0.324 \n 6 CHF             0.0295 0.00958 5.18e-4 0.00414 0.0205 0.0197   0.0596  0.657 \n 7 COPD            0.0493 0.0228  2.28e-4 0.00548 0.0342 0.0461   0.172   0.520 \n 8 CVA             0.0647 0.0246  1.07e-3 0.0107  0.0524 0.0289   0.0764  0.555 \n 9 GI Hemorrhage   0.0542 0.0175  1.25e-3 0.00834 0.0480 0.0350   0.0855  0.588 \n10 Joint Replace…  0.139  0.0179  3.36e-2 0.00673 0.0516 0        0.0874  0.5   \n# … with 13 more rows, 3 more variables: `Medicare HMO` <dbl>,\n#   `No Fault` <dbl>, `Self Pay` <dbl>, and abbreviated variable names\n#   ¹​`Blue Cross`, ²​Commercial, ³​Compensation, ⁴​`Exchange Plans`, ⁵​Medicaid,\n#   ⁶​`Medicaid HMO`, ⁷​`Medicare A`\n\n\nThe table is aggregated by item for the various users to which the algorithm will be applied.\nNow that we have this data we need to find what will be out optimal k (clusters). To do this we need to generate a table of data that will have a column of k and for that k apply the k-means function to the data with that k and return the total within sum of squares.\nTo do this there is a convienent function called hai_kmeans_mapped_tbl that takes as its sole argument the output from the hai_kmeans_user_item_tbl. There is an argument .centers where the default is set to 15.\n\n\nK-Means Mapped Tibble\n\nkmm_tbl <- hai_kmeans_mapped_tbl(uit_tbl)\nkmm_tbl\n\n# A tibble: 15 × 3\n   centers k_means  glance          \n     <int> <list>   <list>          \n 1       1 <kmeans> <tibble [1 × 4]>\n 2       2 <kmeans> <tibble [1 × 4]>\n 3       3 <kmeans> <tibble [1 × 4]>\n 4       4 <kmeans> <tibble [1 × 4]>\n 5       5 <kmeans> <tibble [1 × 4]>\n 6       6 <kmeans> <tibble [1 × 4]>\n 7       7 <kmeans> <tibble [1 × 4]>\n 8       8 <kmeans> <tibble [1 × 4]>\n 9       9 <kmeans> <tibble [1 × 4]>\n10      10 <kmeans> <tibble [1 × 4]>\n11      11 <kmeans> <tibble [1 × 4]>\n12      12 <kmeans> <tibble [1 × 4]>\n13      13 <kmeans> <tibble [1 × 4]>\n14      14 <kmeans> <tibble [1 × 4]>\n15      15 <kmeans> <tibble [1 × 4]>\n\n\nAs we see there are three columns, centers, k_means and glance. The k_means column is the k_means list object and glance is the tibble returned by the broom::glance function.\n\nkmm_tbl %>%\n  tidyr::unnest(glance)\n\n# A tibble: 15 × 6\n   centers k_means  totss tot.withinss betweenss  iter\n     <int> <list>   <dbl>        <dbl>     <dbl> <int>\n 1       1 <kmeans>  1.41       1.41    1.33e-15     1\n 2       2 <kmeans>  1.41       0.592   8.17e- 1     1\n 3       3 <kmeans>  1.41       0.372   1.04e+ 0     2\n 4       4 <kmeans>  1.41       0.276   1.13e+ 0     2\n 5       5 <kmeans>  1.41       0.202   1.21e+ 0     2\n 6       6 <kmeans>  1.41       0.159   1.25e+ 0     3\n 7       7 <kmeans>  1.41       0.124   1.28e+ 0     3\n 8       8 <kmeans>  1.41       0.0884  1.32e+ 0     2\n 9       9 <kmeans>  1.41       0.0745  1.33e+ 0     3\n10      10 <kmeans>  1.41       0.0576  1.35e+ 0     2\n11      11 <kmeans>  1.41       0.0460  1.36e+ 0     2\n12      12 <kmeans>  1.41       0.0363  1.37e+ 0     3\n13      13 <kmeans>  1.41       0.0293  1.38e+ 0     3\n14      14 <kmeans>  1.41       0.0202  1.39e+ 0     2\n15      15 <kmeans>  1.41       0.0161  1.39e+ 0     2\n\n\nAs stated we use the tot.withinss to decide what will become our k, an easy way to do this is to visualize the Scree Plot, also known as the elbow plot. This is done by ploting the x-axis as the centers and the y-axis as the tot.withinss.\n\n\nScree Plot and Data\n\nhai_kmeans_scree_plt(.data = kmm_tbl)\n\n\n\n\nIf we want to see the scree plot data that creates the plot then we can use another function hai_kmeans_scree_data_tbl.\n\nhai_kmeans_scree_data_tbl(kmm_tbl)\n\n# A tibble: 15 × 2\n   centers tot.withinss\n     <int>        <dbl>\n 1       1       1.41  \n 2       2       0.592 \n 3       3       0.372 \n 4       4       0.276 \n 5       5       0.202 \n 6       6       0.159 \n 7       7       0.124 \n 8       8       0.0884\n 9       9       0.0745\n10      10       0.0576\n11      11       0.0460\n12      12       0.0363\n13      13       0.0293\n14      14       0.0202\n15      15       0.0161\n\n\nWith the above pieces of information we can decide upon a value for k, in this instance we are going to use 3. Now that we have that we can go ahead with creating the umap list object where we can take a look at a great many things associated with the data.\n\n\nUMAP List Object\nNow lets go ahead and create our UMAP list object.\n\nump_lst <- hai_umap_list(.data = uit_tbl, kmm_tbl, 3)\n\nNow that it is created, lets take a look at each item in the list. The umap_list function returns a list of 5 items.\n\numap_obj\numap_results_tbl\nkmeans_obj\nkmeans_cluster_tbl\numap_kmeans_cluster_results_tbl\n\nSince we have the list object we can now inspect the kmeans_obj, first thing we will do is use the hai_kmeans_tidy_tbl function to inspect things.\n\nkm_obj <- ump_lst$kmeans_obj\nhai_kmeans_tidy_tbl(.kmeans_obj = km_obj, .data = uit_tbl, .tidy_type = \"glance\")\n\n# A tibble: 1 × 4\n  totss tot.withinss betweenss  iter\n  <dbl>        <dbl>     <dbl> <int>\n1  1.41        0.372      1.04     2\n\nhai_kmeans_tidy_tbl(km_obj, uit_tbl, \"augment\")\n\n# A tibble: 23 × 2\n   service_line                  cluster\n   <chr>                         <fct>  \n 1 Alcohol Abuse                 1      \n 2 Bariatric Surgery For Obesity 1      \n 3 Carotid Endarterectomy        2      \n 4 Cellulitis                    3      \n 5 Chest Pain                    3      \n 6 CHF                           2      \n 7 COPD                          2      \n 8 CVA                           2      \n 9 GI Hemorrhage                 2      \n10 Joint Replacement             2      \n# … with 13 more rows\n\nhai_kmeans_tidy_tbl(km_obj, uit_tbl, \"tidy\")\n\n# A tibble: 3 × 14\n  Blue …¹ Comme…² Compe…³ Excha…⁴    HMO Medic…⁵ Medic…⁶ Medic…⁷ Medic…⁸ No Fa…⁹\n    <dbl>   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1  0.150   0.0368 3.07e-4 0.0207  0.163   0.131   0.314    0.132  0.0319 0.00136\n2  0.0784  0.0218 4.32e-3 0.00620 0.0449  0.0368  0.0800   0.563  0.152  0.00348\n3  0.117   0.0314 1.02e-2 0.0139  0.0982  0.0856  0.147    0.354  0.105  0.00707\n# … with 4 more variables: `Self Pay` <dbl>, size <int>, withinss <dbl>,\n#   cluster <fct>, and abbreviated variable names ¹​`Blue Cross`, ²​Commercial,\n#   ³​Compensation, ⁴​`Exchange Plans`, ⁵​Medicaid, ⁶​`Medicaid HMO`,\n#   ⁷​`Medicare A`, ⁸​`Medicare HMO`, ⁹​`No Fault`\n\n\n\n\nUMAP Plot\nNow that we have all of the above data we can visualize our clusters that are colored by their cluster number.\n\nhai_umap_plot(.data = ump_lst, .point_size = 3, TRUE)"
  },
  {
    "objectID": "posts/healthyrts-20221021/index.html",
    "href": "posts/healthyrts-20221021/index.html",
    "title": "Time Series Clustering with healthyR.ts",
    "section": "",
    "text": "There are two components to time-series clustering with {healthyR.ts}. There is the function that will create the clustering data along with a slew of other information and then there is a plotting function that will plot out the data in a time-series fashion colored by cluster.\nThe first function as mentioned is the function ts_feature_cluster(), and the next is ts_feature_cluster_plot()\nFunction Reference:\n\nts_feature_cluster()\nts_feature_cluster_plot()`\n\nWe are going to use the built-in AirPassengers data set for this example so let’s get right to it!"
  },
  {
    "objectID": "posts/healthyrts-20221021/index.html#ts_feature_cluster",
    "href": "posts/healthyrts-20221021/index.html#ts_feature_cluster",
    "title": "Time Series Clustering with healthyR.ts",
    "section": "ts_feature_cluster()",
    "text": "ts_feature_cluster()\nAs mentioned there are several outputs from the ts_feature_cluster(). Those are as follows:\nData Section\n\nts_feature_tbl\nuser_item_matrix_tbl\nmapped_tbl\nscree_data_tbl\ninput_data_tbl (the original data)\n\nPlots\n\nstatic_plot\nplotly_plot\n\nNow that we have our output, let’s take a look at each individual component of the output.\nts_feature_tbl\n\noutput$data$ts_feature_tbl |> glimpse()\n\nRows: 12\nColumns: 8\n$ group_id       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ ts_x_acf1      <dbl> 0.7405485, 0.7301127, 0.7664718, 0.7151592, 0.7296932, …\n$ ts_x_acf10     <dbl> 1.552193, 1.499645, 1.622249, 1.457441, 1.479361, 1.609…\n$ ts_diff1_acf1  <dbl> -0.09954220, -0.01546920, -0.47063892, -0.25269676, -0.…\n$ ts_diff1_acf10 <dbl> 0.4742052, 0.6538733, 0.5623693, 0.4570952, 0.4168840, …\n$ ts_diff2_acf1  <dbl> -0.18227012, -0.14657790, -0.62033635, -0.55453480, -0.…\n$ ts_seas_acf1   <dbl> 0.2829677, 0.2792932, 0.2947631, 0.2931678, 0.2888097, …\n$ ts_entropy     <dbl> 0.4945771, 0.4818227, 0.6426700, 0.6798957, 0.8772866, …\n\n\nuser_item_matrix_tbl\n\noutput$data$user_item_matrix_tbl |> glimpse()\n\nRows: 12\nColumns: 8\n$ group_id       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ ts_x_acf1      <dbl> 0.7405485, 0.7301127, 0.7664718, 0.7151592, 0.7296932, …\n$ ts_x_acf10     <dbl> 1.552193, 1.499645, 1.622249, 1.457441, 1.479361, 1.609…\n$ ts_diff1_acf1  <dbl> -0.09954220, -0.01546920, -0.47063892, -0.25269676, -0.…\n$ ts_diff1_acf10 <dbl> 0.4742052, 0.6538733, 0.5623693, 0.4570952, 0.4168840, …\n$ ts_diff2_acf1  <dbl> -0.18227012, -0.14657790, -0.62033635, -0.55453480, -0.…\n$ ts_seas_acf1   <dbl> 0.2829677, 0.2792932, 0.2947631, 0.2931678, 0.2888097, …\n$ ts_entropy     <dbl> 0.4945771, 0.4818227, 0.6426700, 0.6798957, 0.8772866, …\n\n\nmapped_tbl\n\noutput$data$mapped_tbl |> glimpse()\n\nRows: 3\nColumns: 3\n$ centers <int> 1, 2, 3\n$ k_means <list> [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.7416450, 1.5450677, -0.…\n$ glance  <list> [<tbl_df[1 x 4]>], [<tbl_df[1 x 4]>], [<tbl_df[1 x 4]>]\n\n\nscree_data_tbl\n\noutput$data$scree_data_tbl |> glimpse()\n\nRows: 3\nColumns: 2\n$ centers      <int> 1, 2, 3\n$ tot.withinss <dbl> 1.8324477, 0.7364934, 0.4571258\n\n\ninput_data_tbl\n\noutput$data$input_data_tbl |> glimpse()\n\nRows: 144\nColumns: 3\n$ date_col <date> 1949-01-01, 1949-02-01, 1949-03-01, 1949-04-01, 1949-05-01, …\n$ value    <dbl> 112, 118, 132, 129, 121, 135, 148, 148, 136, 119, 104, 118, 1…\n$ group_id <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8…\n\n\nNow the plots.\nstatic_plot\n\noutput$plots$static_plot\n\n\n\n\nplotly_plot\n\noutput$plots$plotly_plot\n\n\n\n\n\nNow that we have seen the output of the ts_feature_cluster() function, let’s take a look at the output of the ts_feature_cluster_plot() function."
  },
  {
    "objectID": "posts/healthyrts-20221021/index.html#ts_feature_cluster_plot",
    "href": "posts/healthyrts-20221021/index.html#ts_feature_cluster_plot",
    "title": "Time Series Clustering with healthyR.ts",
    "section": "ts_feature_cluster_plot()",
    "text": "ts_feature_cluster_plot()\nThis function itself returns a list object of a multitude of data. First before we get into that lets look at the function call itself:\n\nts_feature_cluster_plot(\n  .data,\n  .date_col,\n  .value_col,\n  ...,\n  .center = 3,\n  .facet_ncol = 3,\n  .smooth = FALSE\n)\n\nThe data that comes back from this function is:\nData Section\n\noriginal_data\nkmm_data_tbl\nuser_item_tbl\ncluster_tbl\n\nPlots\n\nstatic_plot\nplotly_plot\n\nK-Means Object\n\nk-means object\n\nWe will go through the same exercise and show the output of all the sections. First we have to create the output. The static plot will automatically print out.\n\nplot_out <- ts_feature_cluster_plot(\n  .data = output,\n  .date_col = date_col,\n  .value_col = value,\n  .center = 2,\n  group_id\n)\n\nJoining, by = \"group_id\"\n\n\n\n\n\n\nThe Data Section:\noriginal_data\n\nplot_out$data$original_data |> glimpse()\n\nRows: 144\nColumns: 3\n$ date_col <date> 1949-01-01, 1949-02-01, 1949-03-01, 1949-04-01, 1949-05-01, …\n$ value    <dbl> 112, 118, 132, 129, 121, 135, 148, 148, 136, 119, 104, 118, 1…\n$ group_id <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8…\n\n\nkmm_data_tbl\n\nplot_out$data$kmm_data_tbl |> glimpse()\n\nRows: 3\nColumns: 3\n$ centers <int> 1, 2, 3\n$ k_means <list> [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.7416450, 1.5450677, -0.…\n$ glance  <list> [<tbl_df[1 x 4]>], [<tbl_df[1 x 4]>], [<tbl_df[1 x 4]>]\n\n\nuser_item_data\n\nplot_out$data$user_item_data |> glimpse()\n\n NULL\n\n\ncluster_tbl\n\nplot_out$data$cluster_tbl |> glimpse()\n\nRows: 12\nColumns: 9\n$ cluster        <int> 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2\n$ group_id       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ ts_x_acf1      <dbl> 0.7405485, 0.7301127, 0.7664718, 0.7151592, 0.7296932, …\n$ ts_x_acf10     <dbl> 1.552193, 1.499645, 1.622249, 1.457441, 1.479361, 1.609…\n$ ts_diff1_acf1  <dbl> -0.09954220, -0.01546920, -0.47063892, -0.25269676, -0.…\n$ ts_diff1_acf10 <dbl> 0.4742052, 0.6538733, 0.5623693, 0.4570952, 0.4168840, …\n$ ts_diff2_acf1  <dbl> -0.18227012, -0.14657790, -0.62033635, -0.55453480, -0.…\n$ ts_seas_acf1   <dbl> 0.2829677, 0.2792932, 0.2947631, 0.2931678, 0.2888097, …\n$ ts_entropy     <dbl> 0.4945771, 0.4818227, 0.6426700, 0.6798957, 0.8772866, …\n\n\n\n\nThe plot data.\nstatic_plot\n\nplot_out$plot$static_plot\n\n\n\n\nplotly_plot\n\nplot_out$plot$plotly_plot\n\n\n\n\n\n\n\nThe K-Means Object\nkmeans_object\n\nplot_out$kmeans_object\n\n[[1]]\nK-means clustering with 2 clusters of sizes 5, 7\n\nCluster means:\n  ts_x_acf1 ts_x_acf10 ts_diff1_acf1 ts_diff1_acf10 ts_diff2_acf1 ts_seas_acf1\n1 0.7456468   1.568532     0.1172685      0.4858013    -0.1799728    0.2876449\n2 0.7387865   1.528308    -0.2909349      0.3638392    -0.5916245    0.2930543\n  ts_entropy\n1  0.4918321\n2  0.6438176\n\nClustering vector:\n [1] 1 1 2 2 2 1 1 1 2 2 2 2\n\nWithin cluster sum of squares by cluster:\n[1] 0.3704304 0.3660630\n (between_SS / total_SS =  59.8 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nThanks for reading!"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Simple lapply()",
    "section": "",
    "text": "# Let l be some list of lists, where all elements of lists are numbers\nl <- list(\n  a = 1:10,\n  b = 11:20,\n  c = 21:30\n)\n\nNow let’s take a look at our list l and see it’s structure.\n\nl\n\n$a\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$b\n [1] 11 12 13 14 15 16 17 18 19 20\n\n$c\n [1] 21 22 23 24 25 26 27 28 29 30\n\n\nNow that we see the structure, we can use the lapply function to get the sum of each list element, the mean, etc.\n\nlapply(l, sum)\n\n$a\n[1] 55\n\n$b\n[1] 155\n\n$c\n[1] 255\n\nlapply(l, mean)\n\n$a\n[1] 5.5\n\n$b\n[1] 15.5\n\n$c\n[1] 25.5\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-10-24/index.html",
    "href": "posts/rtip-2022-10-24/index.html",
    "title": "Cumulative Variance",
    "section": "",
    "text": "Generate Data\nThe first thing we need to do in order to showcase this function is to generate some data. Lets do that below:\n\nl <- list(\n  a = rnorm(50),\n  b = rnorm(50, 1),\n  c = rnorm(50, 2)\n)\n\nl\n\n$a\n [1] -0.96548479  0.49276394  0.14030455  1.11786377 -1.47239834 -0.06906506\n [7] -1.51133985  1.48910665  0.09444727 -0.01216806  0.35365683 -1.13562871\n[13] -1.27899694  0.10963391 -0.00708945 -1.26718573  0.92143855  0.09716551\n[19] -0.28025814 -0.18046616 -1.75919633  0.01686201 -0.10204673  0.91791398\n[25] -1.70503761  1.50856724 -1.29433294  0.42665133 -0.78176459  0.16141529\n[31]  1.42536506 -0.42168041 -0.30222269  0.05129043 -0.73717680 -1.60823604\n[37] -0.11921815  0.08357566  0.23250949  0.50846618 -0.02674088  0.12101223\n[43]  0.10390867 -1.11476987 -1.42201791 -1.35493159  0.35703193 -1.08176152\n[49] -0.08189606  0.46341303\n\n$b\n [1]  1.67636555  1.22588224  0.44445597  2.06992723  1.82473269 -0.03321279\n [7]  1.29568923 -0.29542080  1.46614555  0.51617492  2.03383464  0.13835453\n[13]  3.18982479 -0.38493278  0.67450796  1.69715532  1.19963387  1.17294403\n[19]  0.83585415  1.49308994  0.53831112  1.76345465  1.80154859  0.47358491\n[25]  1.40422472  2.50254552 -0.07376997  0.38077031  1.13606122 -0.26052567\n[31]  0.88624336  1.89232197  1.37488657  2.53211686  1.77919794  3.42367520\n[37] -0.59175356 -0.04816522  2.08963807  1.40124074 -0.73135934  0.65282741\n[43]  0.87359580  0.14540086  1.52502012  0.52190806  2.29922084  0.62462975\n[49]  2.94462210  1.06173482\n\n$c\n [1]  1.01194711  1.36267530  1.37423091  1.14980487  1.39304340  3.26911528\n [7]  1.71184232  1.88096194  2.90461886  1.39510407  1.86157191  1.14906542\n[13]  1.90072693  1.78998258  1.61307934  0.76604158  2.92366827  2.32424523\n[19]  2.94645235  2.73102591  0.87949048  3.31239943  1.05720691  1.42571354\n[25]  1.79266828  1.84627335  0.81364549  0.25976918  1.48698512  1.10254109\n[31]  1.60219278  1.84545465  1.93508206  2.13570750  2.32733075  2.53404107\n[37]  1.25864169  3.28238628  1.98998276  1.44299079  2.26296491  3.86667748\n[43]  1.84651988  3.24765507  0.18464631 -0.01404234  2.78432762 -0.05193538\n[49]  0.35160392  2.58212054\n\n\n\n\nMake Function\nNow that we have our data, lets make the function:\n\ncvar <- function(.x){\n  sapply(seq_along(.x), function(k, z) stats::var(z[1:k]), z = .x)\n}\n\nOk, now that we have our function, lets take a look at it in use.\n\n\nUse Function\n\nsapply(l, cvar)\n\n              a         b          c\n [1,]        NA        NA         NA\n [2,] 1.0632447 0.1014676 0.06150513\n [3,] 0.5789145 0.3885272 0.04239889\n [4,] 0.7633500 0.4867186 0.03075658\n [5,] 1.1294646 0.4093271 0.02873772\n [6,] 0.9043498 0.6932616 0.69685950\n [7,] 1.0277904 0.5789892 0.58271798\n [8,] 1.2918409 0.7813852 0.50862439\n [9,] 1.1344452 0.7052323 0.62156290\n[10,] 1.0088029 0.6580963 0.56764373\n[11,] 0.9242084 0.6858993 0.51210764\n[12,] 0.9418512 0.7024341 0.49623990\n[13,] 0.9661294 1.0026509 0.45782344\n[14,] 0.8992042 1.1041314 0.42295247\n[15,] 0.8371837 1.0364119 0.39358167\n[16,] 0.8556585 0.9929979 0.42396425\n[17,] 0.8822275 0.9315646 0.49164277\n[18,] 0.8344918 0.8770439 0.48215682\n[19,] 0.7888762 0.8321667 0.52875406\n[20,] 0.7473648 0.7964123 0.54171586\n[21,] 0.8305355 0.7722667 0.56162921\n[22,] 0.7940780 0.7564316 0.63535849\n[23,] 0.7587189 0.7425071 0.63686713\n[24,] 0.7802954 0.7290301 0.61692337\n[25,] 0.8409644 0.7019443 0.59130379\n[26,] 0.9248957 0.7464413 0.56765490\n[27,] 0.9359290 0.7761117 0.58464117\n[28,] 0.9159283 0.7676951 0.64765859\n[29,] 0.8952420 0.7403040 0.62681485\n[30,] 0.8690093 0.7773175 0.61856072\n[31,] 0.9251735 0.7524213 0.59834912\n[32,] 0.8976913 0.7499102 0.57961326\n[33,] 0.8702922 0.7290409 0.56296663\n[34,] 0.8452302 0.7678835 0.55094641\n[35,] 0.8301013 0.7571526 0.54480209\n[36,] 0.8638223 0.8786798 0.54627250\n[37,] 0.8400510 0.9426489 0.53823917\n[38,] 0.8195803 0.9560736 0.58478212\n[39,] 0.8028106 0.9542482 0.57032971\n[40,] 0.7943867 0.9312335 0.55895977\n[41,] 0.7750385 0.9957722 0.55033290\n[42,] 0.7581242 0.9766789 0.63799874\n[43,] 0.7417073 0.9547108 0.62281006\n[44,] 0.7453949 0.9533619 0.65240410\n[45,] 0.7629119 0.9360654 0.70195202\n[46,] 0.7747320 0.9223139 0.76179573\n[47,] 0.7652088 0.9339432 0.76550157\n[48,] 0.7645076 0.9188787 0.82293002\n[49,] 0.7490587 0.9695572 0.84800554\n[50,] 0.7434405 0.9498710 0.84419808\n\nlapply(l, cvar)\n\n$a\n [1]        NA 1.0632447 0.5789145 0.7633500 1.1294646 0.9043498 1.0277904\n [8] 1.2918409 1.1344452 1.0088029 0.9242084 0.9418512 0.9661294 0.8992042\n[15] 0.8371837 0.8556585 0.8822275 0.8344918 0.7888762 0.7473648 0.8305355\n[22] 0.7940780 0.7587189 0.7802954 0.8409644 0.9248957 0.9359290 0.9159283\n[29] 0.8952420 0.8690093 0.9251735 0.8976913 0.8702922 0.8452302 0.8301013\n[36] 0.8638223 0.8400510 0.8195803 0.8028106 0.7943867 0.7750385 0.7581242\n[43] 0.7417073 0.7453949 0.7629119 0.7747320 0.7652088 0.7645076 0.7490587\n[50] 0.7434405\n\n$b\n [1]        NA 0.1014676 0.3885272 0.4867186 0.4093271 0.6932616 0.5789892\n [8] 0.7813852 0.7052323 0.6580963 0.6858993 0.7024341 1.0026509 1.1041314\n[15] 1.0364119 0.9929979 0.9315646 0.8770439 0.8321667 0.7964123 0.7722667\n[22] 0.7564316 0.7425071 0.7290301 0.7019443 0.7464413 0.7761117 0.7676951\n[29] 0.7403040 0.7773175 0.7524213 0.7499102 0.7290409 0.7678835 0.7571526\n[36] 0.8786798 0.9426489 0.9560736 0.9542482 0.9312335 0.9957722 0.9766789\n[43] 0.9547108 0.9533619 0.9360654 0.9223139 0.9339432 0.9188787 0.9695572\n[50] 0.9498710\n\n$c\n [1]         NA 0.06150513 0.04239889 0.03075658 0.02873772 0.69685950\n [7] 0.58271798 0.50862439 0.62156290 0.56764373 0.51210764 0.49623990\n[13] 0.45782344 0.42295247 0.39358167 0.42396425 0.49164277 0.48215682\n[19] 0.52875406 0.54171586 0.56162921 0.63535849 0.63686713 0.61692337\n[25] 0.59130379 0.56765490 0.58464117 0.64765859 0.62681485 0.61856072\n[31] 0.59834912 0.57961326 0.56296663 0.55094641 0.54480209 0.54627250\n[37] 0.53823917 0.58478212 0.57032971 0.55895977 0.55033290 0.63799874\n[43] 0.62281006 0.65240410 0.70195202 0.76179573 0.76550157 0.82293002\n[49] 0.84800554 0.84419808\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-10-26/index.html",
    "href": "posts/rtip-2022-10-26/index.html",
    "title": "Control Charts in healthyR.ai",
    "section": "",
    "text": "Library Load\nCan’t do anyting without loading the library into our current session.\n\nlibrary(healthyR.ai)\n\n\n== Welcome to healthyR.ai ===========================================================================\nIf you find this package useful, please leave a star: \n   https://github.com/spsanderson/healthyR.ai'\n\nIf you encounter a bug or want to request an enhancement please file an issue at:\n   https://github.com/spsanderson/healthyR.ai/issues\n\nThank you for using healthyR.ai\n\n\n\n\nGenerate Data\nLets generate some fake data below:\n\ndata_tbl <- tibble::tibble(\n  day = sample(\n    c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"),\n    100, TRUE\n  ),\n  person = sample(c(\"Tom\", \"Jane\", \"Alex\"), 100, TRUE),\n  count = rbinom(100, 20, ifelse(day == \"Friday\", .5, .2)),\n  date = Sys.Date() - sample.int(100)\n)\n\nLets take a look at the data_tbl, to do so we will use the glimpse() function from dplyr\n\ndplyr::glimpse(data_tbl)\n\nRows: 100\nColumns: 4\n$ day    <chr> \"Tuesday\", \"Monday\", \"Monday\", \"Monday\", \"Wednesday\", \"Wednesda…\n$ person <chr> \"Jane\", \"Jane\", \"Tom\", \"Tom\", \"Tom\", \"Jane\", \"Alex\", \"Alex\", \"A…\n$ count  <int> 2, 3, 2, 4, 3, 3, 1, 2, 0, 3, 1, 4, 4, 10, 6, 3, 6, 3, 3, 4, 4,…\n$ date   <date> 2022-08-02, 2022-09-23, 2022-09-07, 2022-10-14, 2022-08-01, 20…\n\n\n\n\nControl Chart\nThe {healthyR.ai} package comes with a control chart function. The reference for this function can be found here.\nLet’s take a look at the full function call and it’s defaults.\n\nhai_control_chart(\n  .data,\n  .value_col,\n  .x_col,\n  .center_line = mean,\n  .std_dev = 3,\n  .plt_title = NULL,\n  .plt_catpion = NULL,\n  .plt_font_size = 11,\n  .print_plot = TRUE\n)\n\nSo we see that there are several arguments to the function, with only three that are required from the user.\n\n\nDetails\nControl charts, also known as Shewhart charts (after Walter A. Shewhart) or process-behavior charts, are a statistical process control tool used to determine if a manufacturing or business process is in a state of control. It is more appropriate to say that the control charts are the graphical device for Statistical Process Monitoring (SPM). Traditional control charts are mostly designed to monitor process parameters when underlying form of the process distributions are known. However, more advanced techniques are available in the 21st century where incoming data streaming can-be monitored even without any knowledge of the underlying process distributions. Distribution-free control charts are becoming increasingly popular.\nNow let’s see an example or a couple.\n\n\nVisual\nLet’s use the mean as is the default\n\nhai_control_chart(.data = data_tbl, .value_col = count, .x_col = date)\n\n\n\n\nNow let’s use the median and change the standard deviation argument to 1.\n\nhai_control_chart(data_tbl, count, date, median, 1)\n\n\n\n\nFor a more advanced set of charts from a dedicated package you may want to check out the following from John MacKintosh:\n\nruncharter\ncusumcharter\nspccharter\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-10-31/index.html",
    "href": "posts/rtip-2022-10-31/index.html",
    "title": "Cumulative Skewness",
    "section": "",
    "text": "Data\nWe are going to use the mtcars data set and use the mpg column for this example. Let’s set x equal to mtcars$mpg\n\nx <- mtcars$mpg\n\n\n\nExample\nNow let’s see the function in use.\n\ncum_skewness(x)\n\n [1]  0.000000e+00  0.000000e+00  8.249747e-06  5.049149e-06 -1.113787e-05\n [6] -8.569220e-06 -1.134377e-04 -8.440629e-05 -8.280585e-05 -5.457236e-05\n[11] -3.209937e-05 -1.758922e-05 -5.567456e-06  1.436318e-07 -6.299325e-05\n[16] -8.605705e-05 -5.869380e-05  1.594511e-04  1.675837e-04  2.221143e-04\n[21]  1.855217e-04  1.936299e-04  1.998527e-04  2.082240e-04  1.897575e-04\n[26]  1.505425e-04  1.180971e-04  9.974055e-05  1.048461e-04  9.801797e-05\n[31]  1.024713e-04  9.107160e-05\n\n\nLet’s plot it out.\n\nplot(cum_skewness(x), type = \"l\")"
  },
  {
    "objectID": "posts/rtip-2022-11-07/index.html",
    "href": "posts/rtip-2022-11-07/index.html",
    "title": "Discrete Fourier Vec with healthyR.ai",
    "section": "",
    "text": "Function\nHere is the full function call:\n\nhai_fourier_discrete_vec(\n  .x,\n  .period,\n  .order,\n  .scale_type = c(\"sin\", \"cos\", \"sincos\")\n)\n\nHere are the parameters to the function and what they expect:\n\n.x - A numeric vector\n.period - The number of observations that complete a cycle\n.order - The fourier term order\n.scale_type - A character of one of the following: sin,cos,sincos\n\nThe internal caluclation is straightforward:\n\nsin = sin(2 * pi * h * x), where h = .order/.period\ncos = cos(2 * pi * h * x), where h = .order/.period\nsincos = sin(2 * pi * h * x) * cos(2 * pi * h * x) where h = .order/.period\n\n\n\nExample\nLet’s work throught a quick and simple example.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(healthyR.ai)\nlibrary(tidyr)\n\nlen_out <- 24\nby_unit <- \"month\"\nstart_date <- as.Date(\"2021-01-01\")\n\ndata_tbl <- tibble(\n  date_col = seq.Date(\n    from = start_date, \n    length.out = len_out, \n    by = by_unit\n  ),\n  a = rnorm(len_out, sd = 2),\n  fv_sin = hai_fourier_discrete_vec(a, 12, 1, \"sin\"),\n  fv_cos = hai_fourier_discrete_vec(a, 12, 1, \"cos\"),\n  fv_sc  = hai_fourier_discrete_vec(a, 12, 1, \"sincos\")\n)\n\ndata_tbl\n\n# A tibble: 24 × 5\n   date_col         a fv_sin fv_cos fv_sc\n   <date>       <dbl>  <dbl>  <dbl> <dbl>\n 1 2021-01-01 -0.486       0      1     0\n 2 2021-02-01 -0.708       0      1     0\n 3 2021-03-01 -0.119       0      1     0\n 4 2021-04-01  0.0405      1      1     1\n 5 2021-05-01  1.19        1      1     1\n 6 2021-06-01  1.88        1      1     1\n 7 2021-07-01 -1.32        0      1     0\n 8 2021-08-01 -0.0214      0      1     0\n 9 2021-09-01  2.80        1      1     1\n10 2021-10-01  1.67        1      1     1\n# … with 14 more rows\n\n\n\n\nVisual\nLet’s visualize.\n\ndata_tbl %>% \n  pivot_longer(cols = -date_col) %>% \n  ggplot(aes(x = date_col, y = value, color = name)) + \n  geom_line() + \n  facet_wrap(~ name, scales = \"free\") +\n  theme_minimal() +\n  labs(color = \"\")\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-08/index.html",
    "href": "posts/rtip-2022-11-08/index.html",
    "title": "Hyperbolic Transform with healthyR.ai",
    "section": "",
    "text": "Function\nThe function takes in a numeric vector as it’s argument and will transform the data with one of the following:\n\nsin\ncos\ntan\nsincos This will do: value = sin(x) * cos(x)\n\nThe full function call is:\n\nhai_hyperbolic_vec(.x, .scale_type = c(\"sin\", \"cos\", \"tan\", \"sincos\"))\n\n\n\nExample\n\nlibrary(dplyr)\nlibrary(healthyR.ai)\nlibrary(tidyr)\nlibrary(ggplot2)\n\nlen_out <- 25\nby_unit <- \"month\"\nstart_date <- as.Date(\"2021-01-01\")\n\ndata_tbl <- tibble(\n  date_col = seq.Date(\n    from = start_date, \n    length.out = len_out, \n    by = by_unit\n    ),\n  b = runif(len_out),\n  fv_sin = hai_hyperbolic_vec(b, .scale_type = \"sin\"),\n  fv_cos = hai_hyperbolic_vec(b, .scale_type = \"cos\"),\n  fv_sc  = hai_hyperbolic_vec(b, .scale_type = \"sincos\")\n)\n\ndata_tbl\n\n# A tibble: 25 × 5\n   date_col        b fv_sin fv_cos  fv_sc\n   <date>      <dbl>  <dbl>  <dbl>  <dbl>\n 1 2021-01-01 0.961  0.820   0.573 0.470 \n 2 2021-02-01 0.418  0.406   0.914 0.371 \n 3 2021-03-01 0.0729 0.0728  0.997 0.0726\n 4 2021-04-01 0.426  0.413   0.911 0.376 \n 5 2021-05-01 0.851  0.752   0.659 0.496 \n 6 2021-06-01 0.824  0.734   0.679 0.499 \n 7 2021-07-01 0.659  0.612   0.791 0.484 \n 8 2021-08-01 0.683  0.631   0.776 0.490 \n 9 2021-09-01 0.173  0.172   0.985 0.169 \n10 2021-10-01 0.345  0.338   0.941 0.318 \n# … with 15 more rows\n\n\n\n\nVisual\n\ndata_tbl %>% \n  pivot_longer(cols = -date_col) %>% \n  ggplot(aes(x = date_col, y = value, color = name)) + \n  geom_line() + \n  facet_wrap(~ name, scales = \"free\") +\n  theme_minimal() +\n  labs(color = \"\")"
  },
  {
    "objectID": "posts/rtip-2022-11-09/index.html",
    "href": "posts/rtip-2022-11-09/index.html",
    "title": "Mapping K-Means with healthyR.ai",
    "section": "",
    "text": "K-Means is a clustering algorithm that can be used to find potential clusters in your data.\nThe algorithm does require that you look at different values of K in order to assess which is the optimal value.\nIn the R package {healthyR.ai} there is a utility to do this."
  },
  {
    "objectID": "posts/rtip-2022-11-09/index.html#parameters",
    "href": "posts/rtip-2022-11-09/index.html#parameters",
    "title": "Mapping K-Means with healthyR.ai",
    "section": "Parameters",
    "text": "Parameters\nThe parameters take the following arguments:\n\n.data - This is the data that should be an output of the hai_user_item_tbl() or it’s synonym, or should at least be in the user item matrix format.\n.centers - The maximum amount of centers you want to map to the k-means function. The default is 15."
  },
  {
    "objectID": "posts/rtip-2022-11-10/index.html",
    "href": "posts/rtip-2022-11-10/index.html",
    "title": "Reading Multiple Files with {purrr}",
    "section": "",
    "text": "There may be times when you have multiple structured files in the same folder, maybe they are .csv files. For this short tip, we will say that they are.\nI will show the short script and then discuss it.\n\n# Library Load ----\nlibrary(dplyr)\nlibrary(purrr)\n\n# Set file path ----\nfolder    <- \"FileFolder\"\npath      <- \"C:/Some/Root/Path/\"\nfull_path <- paste0(path,folder,\"/\")\n\n# File List ----\nfile_list <- dir(full_path\n                 , pattern = \"\\\\.csv$\"\n                 , full.names = T)\n\n# Read Files ----\nfiles <- file_list %>%\n  map(read.csv) %>%\n  map(as_tibble)\n\n# Clean File Names ----\nfile_names <- file_list %>%\n  str_remove(full_path) %>%\n  str_replace(\n    pattern = \"_OldStuff.csv\", \n    replacement = \"_NewStuff.csv\"\n  )\n\nnames(files) <- file_names\n\nWe load in {dplyr} for the pipe and the as_tibble function. After this we set out to create the file path. I have chosen to do this in two separate pieces as I have had experience with needing to go through different folders in the same root directory. While this could further be scripted I leave it as is.\nfolder is the folder that has the files of interest, in this case the .csv files. We then get the root path to that folder but not including it, this is defined as path in the above. After we have both folder and path we can create the full_path by using paste0\nNow after this we use the base R function of dir to list out all of the files that fit the specific format of .csv with a regex pattern. I always want the name of the file as it allows me to go back to the file later and lets me name the files in the upcoming list later on.\nSince these are .csv files I use purrr::map and then read.csv to read in all of the .csv files in the list that was created, we then used map again and this time used as_tibble to make sure that each file is a tibble and not something else like data.frame\nSince I provided the argument of T to dir, full.names I can then get a character vector of the names of the files which then is applied to the file list.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-14/index.html",
    "href": "posts/rtip-2022-11-14/index.html",
    "title": "Find Skewed Features with {healthyR.ai}",
    "section": "",
    "text": "Function\nLet’s first take a look at the function call.\n\nhai_skewed_features(\n  .data, \n  .threshold = 0.6, \n  .drop_keys = NULL\n  )\n\nNow let’s take a look at the arguments that go to the parameters of the function.\n\n.data - The data.frame/tibble you are passing in.\n.threshold - A level of skewness that indicates where you feel a column should be considered skewed.\n.drop_keys - A c() character vector of columns you do not want passed to the function.\n\n\n\nExample\nHere are a couple of examples.\n\nlibrary(healthyR.ai)\n\nhai_skewed_features(mtcars)\n\n[1] \"mpg\"  \"hp\"   \"carb\"\n\nhai_skewed_features(mtcars, .drop_keys = \"hp\")\n\n[1] \"mpg\"  \"carb\""
  },
  {
    "objectID": "posts/rtip-2022-11-15/index.html",
    "href": "posts/rtip-2022-11-15/index.html",
    "title": "Auto Prep data for XGBoost with {healthyR.ai}",
    "section": "",
    "text": "Function\nLet’s take a look at the function call.\n\nhai_xgboost_data_prepper(.data, .recipe_formula)\n\nNow let’s go over the arguments that are passed to the function.\n\n.data - The data that you are passing to the function. Can be any type of data that is accepted by the data parameter of the recipes::reciep() function.\n.recipe_formula - The formula that is going to be passed. For example if you are using the diamonds data then the formula would most likely be something like price ~ .\n\n\n\nExample\nLet’s go over some examples.\n\nlibrary(ggplot2)\nlibrary(healthyR.ai)\n\n# Regression\nhai_xgboost_data_prepper(.data = diamonds, .recipe_formula = price ~ .)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          9\n\nOperations:\n\nFactor variables from tidyselect::vars_select_helpers$where(is.charac...\nNovel factor level assignment for recipes::all_nominal_predictors()\nDummy variables from recipes::all_nominal_predictors()\nZero variance filter on recipes::all_predictors()\n\nreg_obj <- hai_xgboost_data_prepper(diamonds, price ~ .)\nget_juiced_data(reg_obj)\n\n# A tibble: 53,940 × 27\n   carat depth table     x     y     z price  cut_1  cut_2  cut_3  cut_4   cut_5\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <int>  <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n 1  0.23  61.5    55  3.95  3.98  2.43   326  0.359 -0.109 -0.522 -0.567 -0.315 \n 2  0.21  59.8    61  3.89  3.84  2.31   326  0.120 -0.436 -0.298  0.378  0.630 \n 3  0.23  56.9    65  4.05  4.07  2.31   327 -0.359 -0.109  0.522 -0.567  0.315 \n 4  0.29  62.4    58  4.2   4.23  2.63   334  0.120 -0.436 -0.298  0.378  0.630 \n 5  0.31  63.3    58  4.34  4.35  2.75   335 -0.359 -0.109  0.522 -0.567  0.315 \n 6  0.24  62.8    57  3.94  3.96  2.48   336 -0.120 -0.436  0.298  0.378 -0.630 \n 7  0.24  62.3    57  3.95  3.98  2.47   336 -0.120 -0.436  0.298  0.378 -0.630 \n 8  0.26  61.9    55  4.07  4.11  2.53   337 -0.120 -0.436  0.298  0.378 -0.630 \n 9  0.22  65.1    61  3.87  3.78  2.49   337 -0.598  0.546 -0.373  0.189 -0.0630\n10  0.23  59.4    61  4     4.05  2.39   338 -0.120 -0.436  0.298  0.378 -0.630 \n# … with 53,930 more rows, and 15 more variables: color_1 <dbl>, color_2 <dbl>,\n#   color_3 <dbl>, color_4 <dbl>, color_5 <dbl>, color_6 <dbl>, color_7 <dbl>,\n#   clarity_1 <dbl>, clarity_2 <dbl>, clarity_3 <dbl>, clarity_4 <dbl>,\n#   clarity_5 <dbl>, clarity_6 <dbl>, clarity_7 <dbl>, clarity_8 <dbl>\n\n# Classification\nhai_xgboost_data_prepper(Titanic, Survived ~ .)\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nOperations:\n\nFactor variables from tidyselect::vars_select_helpers$where(is.charac...\nNovel factor level assignment for recipes::all_nominal_predictors()\nDummy variables from recipes::all_nominal_predictors()\nZero variance filter on recipes::all_predictors()\n\ncla_obj <- hai_xgboost_data_prepper(Titanic, Survived ~ .)\nget_juiced_data(cla_obj)\n\n# A tibble: 32 × 7\n       n Survived Class_X2nd Class_X3rd Class_Crew Sex_Male Age_Child\n   <dbl> <fct>         <dbl>      <dbl>      <dbl>    <dbl>     <dbl>\n 1     0 No                0          0          0        1         1\n 2     0 No                1          0          0        1         1\n 3    35 No                0          1          0        1         1\n 4     0 No                0          0          1        1         1\n 5     0 No                0          0          0        0         1\n 6     0 No                1          0          0        0         1\n 7    17 No                0          1          0        0         1\n 8     0 No                0          0          1        0         1\n 9   118 No                0          0          0        1         0\n10   154 No                1          0          0        1         0\n# … with 22 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-16/index.html",
    "href": "posts/rtip-2022-11-16/index.html",
    "title": "Cumulative Harmonic Mean with {TidyDensity}",
    "section": "",
    "text": "Function\nHere is the function call, it is very simple as it is a vectorized function.\n\nchmean(.x)\n\nThe only argument you provide to this function is a numeric vector. Let’s take a quick look at the construction of the function.\n\nchmean <- function(.x) {\n  1 / (cumsum(1 / .x))\n}\n\n\n\nExamples\nLet’s take a look at an example.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\nx <- mtcars$mpg\n\nchmean(x)\n\n [1] 21.0000000 10.5000000  7.1891892  5.3813575  4.1788087  3.3949947\n [7]  2.7436247  2.4663044  2.2255626  1.9943841  1.7934398  1.6166494\n[13]  1.4784877  1.3474251  1.1928760  1.0701322  0.9975150  0.9677213\n[19]  0.9378663  0.9126181  0.8754572  0.8286539  0.7858140  0.7419753\n[25]  0.7143688  0.6961523  0.6779989  0.6632076  0.6364908  0.6165699\n[31]  0.5922267  0.5762786\n\nmtcars %>%\n  select(mpg) %>%\n  mutate(cum_har_mean = chmean(mpg)) %>%\n  head(10)\n\n                   mpg cum_har_mean\nMazda RX4         21.0    21.000000\nMazda RX4 Wag     21.0    10.500000\nDatsun 710        22.8     7.189189\nHornet 4 Drive    21.4     5.381358\nHornet Sportabout 18.7     4.178809\nValiant           18.1     3.394995\nDuster 360        14.3     2.743625\nMerc 240D         24.4     2.466304\nMerc 230          22.8     2.225563\nMerc 280          19.2     1.994384\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-17/index.html",
    "href": "posts/rtip-2022-11-17/index.html",
    "title": "Bootstrap Modeling with {purrr} and {modler}",
    "section": "",
    "text": "Functions\nThe main functions that we are going to showcase are purrr::map() and modelr::bootstrap()\n\n\nExamples\nLet’s get right into it.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\ndf <- mtcars\n\nfit_boots <- df %>% \n  modelr::bootstrap(n = 200, id = 'boot_num') %>%\n  group_by(boot_num) %>%\n  mutate(fit = map(strap, ~lm(mpg ~ ., data = data.frame(.))))\n\nfit_boots\n\n# A tibble: 200 × 3\n# Groups:   boot_num [200]\n   strap                boot_num fit   \n   <list>               <chr>    <list>\n 1 <resample [32 x 11]> 001      <lm>  \n 2 <resample [32 x 11]> 002      <lm>  \n 3 <resample [32 x 11]> 003      <lm>  \n 4 <resample [32 x 11]> 004      <lm>  \n 5 <resample [32 x 11]> 005      <lm>  \n 6 <resample [32 x 11]> 006      <lm>  \n 7 <resample [32 x 11]> 007      <lm>  \n 8 <resample [32 x 11]> 008      <lm>  \n 9 <resample [32 x 11]> 009      <lm>  \n10 <resample [32 x 11]> 010      <lm>  \n# … with 190 more rows\n\n\nNow lets get our parameter estimates.\n\n# get parameters ####\nparams_boot <- fit_boots %>%\n  mutate(tidy_fit = map(fit, tidy)) %>%\n  unnest(cols = tidy_fit) %>%\n  ungroup()\n\n# get predictions\npreds_boot <- fit_boots %>%\n  mutate(augment_fit = map(fit, augment)) %>%\n  unnest(cols = augment_fit) %>%\n  ungroup()\n\nTime to visualize.\n\nlibrary(patchwork)\n\n# plot distribution of estimated parameters\np1 <- ggplot(params_boot, aes(estimate)) +\n  geom_histogram(col = 'black', fill = 'white') +\n  facet_wrap(~ term, scales = 'free') +\n  theme_minimal()\n\n# plot points with predictions\np2 <- ggplot() +\n  geom_line(aes(mpg, .fitted, group = boot_num), preds_boot, alpha = .03) +\n  geom_point(aes(mpg, .fitted), preds_boot, col = 'steelblue', alpha = 0.05) +\n  theme_minimal()\n  \n# plot both\np1 + p2\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-21/index.html",
    "href": "posts/rtip-2022-11-21/index.html",
    "title": "Bootstrap Modeling with Base R",
    "section": "",
    "text": "Example\nWe will be using a very simple for loop to accomplish this. You will find an excellent post on this on Stats StackExchange from Francisco Jos Goerlich Gisbert\n\nn    <- 2000\ndf   <- mtcars\npred <- numeric(0)\n\nlibrary(tictoc) # for timing\n\ntic()\nset.seed(123)\nfor (i in 1:n){\n  boot    <- sample(nrow(df), n, replace = TRUE)\n  fit     <- lm(mpg ~ wt, data = df[boot,])\n  pred[i] <- predict(fit, newdata = df[boot,]) +\n    sample(resid(fit), size = 1)\n}\ntoc()\n\n6.8 sec elapsed\n\n\nSo we can see that the process ran pretty quickly and the loop itself is not a very difficult one. Let’s explain a little.\nSo the boot object is a sampling of df which in this case is the mtcars data set. We took a sample with replacement from this data set. We took 2000 samples and did this 2000 times.\nNext we made the fit object by fitting a simple linear model to the data where mpg is a function of wt. Once this is done, we made out predictions.\nThat’s it!"
  },
  {
    "objectID": "posts/rtip-2022-11-22/index.html",
    "href": "posts/rtip-2022-11-22/index.html",
    "title": "Data Preprocessing Scale/Normalize with {healthyR.ai}",
    "section": "",
    "text": "Function\nLet’s take a look at the function call.\n\nhai_data_scale(\n  .recipe_object = NULL,\n  ...,\n  .type_of_scale = \"center\",\n  .range_min = 0,\n  .range_max = 1,\n  .scale_factor = 1\n)\n\nNow let’s go over the arguments that get supplied to the parameters of this function.\n\n.recipe_object - The data that you want to process\n... - One or more selector functions to choose variables to be imputed. When used with imp_vars, these dots indicate which variables are used to predict the missing data in each variable. See selections() for more details\n.type_of_scale - This is a quoted argument and can be one of the following:\n\n“center”\n“normalize”\n“range”\n“scale”\n\nrange_min - A single numeric value for the smallest value in the range. This defaults to 0.\n.range_max - A single numeric value for the largeest value in the range. This defaults to 1.\n.scale_factor - A numeric value of either 1 or 2 that scales the numeric inputs by one or two standard deviations. By dividing by two standard deviations, the coefficients attached to continuous predictors can be interpreted the same way as with binary inputs. Defaults to 1.\n\n\n\nExample\nNow let’s see it in action!\n\nlibrary(healthyR.ai)\nlibrary(dplyr)\nlibrary(recipes)\n\ndate_seq <- seq.Date(\n  from = as.Date(\"2013-01-01\"), \n  length.out = 100, \n  by = \"month\"\n)\n\nval_seq <- rep(rnorm(10, mean = 6, sd = 2), times = 10)\n\ndf_tbl <- tibble(\n  date_col = date_seq,\n  value    = val_seq\n)\n\ndf_tbl\n\n# A tibble: 100 × 2\n   date_col   value\n   <date>     <dbl>\n 1 2013-01-01  6.66\n 2 2013-02-01  6.66\n 3 2013-03-01  5.09\n 4 2013-04-01  6.94\n 5 2013-05-01  5.96\n 6 2013-06-01  6.18\n 7 2013-07-01  3.62\n 8 2013-08-01  7.31\n 9 2013-09-01  4.58\n10 2013-10-01  7.29\n# … with 90 more rows\n\nrec_obj <- recipe(value ~ ., df_tbl)\n\nnew_rec_obj <- hai_data_scale(\n  .recipe_object = rec_obj,\n  value,\n  .type_of_scale = \"center\"\n)$scale_rec_obj\n\nnew_rec_obj %>% \n  get_juiced_data()\n\n# A tibble: 100 × 2\n   date_col     value\n   <date>       <dbl>\n 1 2013-01-01  0.633 \n 2 2013-02-01  0.630 \n 3 2013-03-01 -0.935 \n 4 2013-04-01  0.909 \n 5 2013-05-01 -0.0676\n 6 2013-06-01  0.149 \n 7 2013-07-01 -2.41  \n 8 2013-08-01  1.28  \n 9 2013-09-01 -1.45  \n10 2013-10-01  1.26  \n# … with 90 more rows\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-28/index.html",
    "href": "posts/rtip-2022-11-28/index.html",
    "title": "Default Metric Sets with {healthyR.ai}",
    "section": "",
    "text": "Function\nThe functions themselves are {yardstick} metric set functions. Let’s take a look at them.\n\nlibrary(healthyR.ai)\n\nhai_default_classification_metric_set()\n\n# A tibble: 11 × 3\n   metric       class        direction\n   <chr>        <chr>        <chr>    \n 1 sensitivity  class_metric maximize \n 2 specificity  class_metric maximize \n 3 recall       class_metric maximize \n 4 precision    class_metric maximize \n 5 mcc          class_metric maximize \n 6 accuracy     class_metric maximize \n 7 f_meas       class_metric maximize \n 8 kap          class_metric maximize \n 9 ppv          class_metric maximize \n10 npv          class_metric maximize \n11 bal_accuracy class_metric maximize \n\nhai_default_regression_metric_set()\n\n# A tibble: 6 × 3\n  metric class          direction\n  <chr>  <chr>          <chr>    \n1 mae    numeric_metric minimize \n2 mape   numeric_metric minimize \n3 mase   numeric_metric minimize \n4 smape  numeric_metric minimize \n5 rmse   numeric_metric minimize \n6 rsq    numeric_metric maximize \n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-29/index.html",
    "href": "posts/rtip-2022-11-29/index.html",
    "title": "Working with Lists",
    "section": "",
    "text": "Example\nFirst let’s make a list.\n\nl <- list(\n  letters,\n  1:26,\n  rnorm(26)\n)\n\nl\n\n[[1]]\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n[[2]]\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26\n\n[[3]]\n [1] -1.5647537840 -1.3080486753  1.3331315389 -0.5490502644 -0.4467608750\n [6] -1.5876952894  0.2292049732 -0.2885449316  1.4614499298 -0.0864987690\n[11]  0.5686850031 -0.3897819578  0.1776603862 -1.1326372302 -1.8651290164\n[16]  1.2676006036  0.2405115523 -1.0506728047  1.4069277686 -1.0125778892\n[21] -0.7687818102 -0.1325350681  0.3639485041  0.0005700058 -1.0698214370\n[26]  1.1972767040\n\n\nNow let’s look at somethings we can do with lists. First, let’s see if we can get the class of each item in the list. We are going to use lapply() for this.\n\nlapply(l, class)\n\n[[1]]\n[1] \"character\"\n\n[[2]]\n[1] \"integer\"\n\n[[3]]\n[1] \"numeric\"\n\n\nNow, let’s perform some simple operations on each item of the list.\n\nlapply(l, length)\n\n[[1]]\n[1] 26\n\n[[2]]\n[1] 26\n\n[[3]]\n[1] 26\n\ntry(lapply(l, sum))\n\nError in FUN(X[[i]], ...) : invalid 'type' (character) of argument\n\n\nOk so we see taking the sum of the first element of the list in lapply() did not work because of a class type mismatch. Let’s see how we can get around this an only apply the sum function to a numeric type. To do this we can rely on {purrr} by using a function map_if()\n\nlibrary(purrr)\n\nmap_if(l, is.numeric, sum)\n\n[[1]]\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n[[2]]\n[1] 351\n\n[[3]]\n[1] -5.006323\n\n\n\nmap_if(l, is.numeric, mean)\n\n[[1]]\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n[[2]]\n[1] 13.5\n\n[[3]]\n[1] -0.1925509\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-11-30/index.html",
    "href": "posts/rtip-2022-11-30/index.html",
    "title": "Generate Random Walk Data with {healthyR.ts}",
    "section": "",
    "text": "Function\nHere is the full function call.\n\nts_random_walk(\n  .mean = 0,\n  .sd = 0.1,\n  .num_walks = 100,\n  .periods = 100,\n  .initial_value = 1000\n)\n\nNow let’s look at the arguments to the parameters.\n\n.mean - The desired mean of the random walks\n.sd - The standard deviation of the random walks\n.num_walks - The number of random walks you want generated\n.periods - The length of the random walk(s) you want generated\n.initial_value - The initial value where the random walks should start\n\nThe underlying data of this function is generated by rnorm()\n\n\nExample\nLet’s take a look at an example and see some visuals.\n\nlibrary(healthyR.ts)\nlibrary(ggplot2)\n\ndf <- ts_random_walk(.num_walks = 100)\n\ndf\n\n# A tibble: 10,000 × 4\n     run     x        y cum_y\n   <dbl> <dbl>    <dbl> <dbl>\n 1     1     1 -0.144    856.\n 2     1     2  0.00648  862.\n 3     1     3  0.0726   924.\n 4     1     4 -0.152    784.\n 5     1     5  0.0228   802.\n 6     1     6 -0.0455   765.\n 7     1     7  0.0972   840.\n 8     1     8 -0.234    643.\n 9     1     9 -0.0501   611.\n10     1    10 -0.0358   589.\n# … with 9,990 more rows\n\n\nThere are attributes attached to the output of this function, let’s see what they are.\n\natb <- attributes(df)\n\nnames_to_print <- names(atb)[which(names(atb) != \"row.names\")]\n\natb[names_to_print]\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$names\n[1] \"run\"   \"x\"     \"y\"     \"cum_y\"\n\n$.mean\n[1] 0\n\n$.sd\n[1] 0.1\n\n$.num_walks\n[1] 100\n\n$.periods\n[1] 100\n\n$.initial_value\n[1] 1000\n\n\nNow lets visualize.\n\ndf %>%\n   ggplot(\n       mapping = aes(\n           x = x\n           , y = cum_y\n           , color = factor(run)\n           , group = factor(run)\n        )\n    ) +\n    geom_line(alpha = 0.8) +\n    ts_random_walk_ggplot_layers(df)\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-01/index.html",
    "href": "posts/rtip-2022-12-01/index.html",
    "title": "Extract Boilerplate Workflow Metrics with {healthyR.ai}",
    "section": "",
    "text": "Function\nLet’s take a look at the function call.\n\nhai_auto_wflw_metrics(.data)\n\nThe only parameter is .data and this is strictly the output object of one of the hai_auto_ boiler plate functions\n\n\nExample\nSince this function requires the input from an hai_auto function, we will walk through an example with the iris data set. We are going to use the hai_auto_knn() to classify the Species.\n\nlibrary(healthyR.ai)\n\ndata <- iris\n\nrec_obj <- hai_knn_data_prepper(data, Species ~ .)\n\nauto_knn <- hai_auto_knn(\n  .data = data,\n  .rec_obj = rec_obj,\n  .best_metric = \"f_meas\",\n  .model_type = \"classification\",\n  .grid_size = 2,\n  .num_cores = 4\n)\n\nhai_auto_wflw_metrics(auto_knn)\n\n# A tibble: 22 × 9\n   neighbors weight_func dist_power .metric  .esti…¹  mean     n std_err .config\n       <int> <chr>            <dbl> <chr>    <chr>   <dbl> <int>   <dbl> <chr>  \n 1         8 rank             0.888 accuracy multic… 0.95     25 0.00652 Prepro…\n 2         8 rank             0.888 bal_acc… macro   0.962    25 0.00471 Prepro…\n 3         8 rank             0.888 f_meas   macro   0.947    25 0.00649 Prepro…\n 4         8 rank             0.888 kap      multic… 0.922    25 0.0102  Prepro…\n 5         8 rank             0.888 mcc      multic… 0.925    25 0.00964 Prepro…\n 6         8 rank             0.888 npv      macro   0.975    25 0.00351 Prepro…\n 7         8 rank             0.888 ppv      macro   0.949    25 0.00663 Prepro…\n 8         8 rank             0.888 precisi… macro   0.949    25 0.00663 Prepro…\n 9         8 rank             0.888 recall   macro   0.949    25 0.00633 Prepro…\n10         8 rank             0.888 sensiti… macro   0.949    25 0.00633 Prepro…\n# … with 12 more rows, and abbreviated variable name ¹​.estimator\n\n\nAs we see this pulls out the full metric table from the workflow.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-05/index.html",
    "href": "posts/rtip-2022-12-05/index.html",
    "title": "Naming Items in a List with {purrr}, {dplyr}, or {healthyR}",
    "section": "",
    "text": "Function\nFull function call.\n\nnamed_item_list(.data, .group_col)\n\nThere are only two arguments to supply.\n\n.data - The data.frame/tibble.\n.group_col - The column that contains the groupings.\n\nThat’s it.\n\n\nExamples\nLet’s jump into it.\n\nlibrary(purrr)\nlibrary(dplyr)\n\ndata_tbl <- iris\n\ndata_tbl_list <- data_tbl %>%\n  group_split(Species)\n\ndata_tbl_list\n\n<list_of<\n  tbl_df<\n    Sepal.Length: double\n    Sepal.Width : double\n    Petal.Length: double\n    Petal.Width : double\n    Species     : factor<fb977>\n  >\n>[3]>\n[[1]]\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 40 more rows\n\n[[2]]\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n 1          7           3.2          4.7         1.4 versicolor\n 2          6.4         3.2          4.5         1.5 versicolor\n 3          6.9         3.1          4.9         1.5 versicolor\n 4          5.5         2.3          4           1.3 versicolor\n 5          6.5         2.8          4.6         1.5 versicolor\n 6          5.7         2.8          4.5         1.3 versicolor\n 7          6.3         3.3          4.7         1.6 versicolor\n 8          4.9         2.4          3.3         1   versicolor\n 9          6.6         2.9          4.6         1.3 versicolor\n10          5.2         2.7          3.9         1.4 versicolor\n# … with 40 more rows\n\n[[3]]\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>    \n 1          6.3         3.3          6           2.5 virginica\n 2          5.8         2.7          5.1         1.9 virginica\n 3          7.1         3            5.9         2.1 virginica\n 4          6.3         2.9          5.6         1.8 virginica\n 5          6.5         3            5.8         2.2 virginica\n 6          7.6         3            6.6         2.1 virginica\n 7          4.9         2.5          4.5         1.7 virginica\n 8          7.3         2.9          6.3         1.8 virginica\n 9          6.7         2.5          5.8         1.8 virginica\n10          7.2         3.6          6.1         2.5 virginica\n# … with 40 more rows\n\ndata_tbl_list %>%\n   map( ~ pull(., Species)) %>%\n   map( ~ as.character(.)) %>%\n   map( ~ unique(.))\n\n[[1]]\n[1] \"setosa\"\n\n[[2]]\n[1] \"versicolor\"\n\n[[3]]\n[1] \"virginica\"\n\n\nNow lets go ahead and apply the names.\n\nnames(data_tbl_list) <- data_tbl_list %>%\n   map( ~ pull(., Species)) %>%\n   map( ~ as.character(.)) %>%\n   map( ~ unique(.))\n\ndata_tbl_list\n\n<list_of<\n  tbl_df<\n    Sepal.Length: double\n    Sepal.Width : double\n    Petal.Length: double\n    Petal.Width : double\n    Species     : factor<fb977>\n  >\n>[3]>\n$setosa\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 40 more rows\n\n$versicolor\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n 1          7           3.2          4.7         1.4 versicolor\n 2          6.4         3.2          4.5         1.5 versicolor\n 3          6.9         3.1          4.9         1.5 versicolor\n 4          5.5         2.3          4           1.3 versicolor\n 5          6.5         2.8          4.6         1.5 versicolor\n 6          5.7         2.8          4.5         1.3 versicolor\n 7          6.3         3.3          4.7         1.6 versicolor\n 8          4.9         2.4          3.3         1   versicolor\n 9          6.6         2.9          4.6         1.3 versicolor\n10          5.2         2.7          3.9         1.4 versicolor\n# … with 40 more rows\n\n$virginica\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>    \n 1          6.3         3.3          6           2.5 virginica\n 2          5.8         2.7          5.1         1.9 virginica\n 3          7.1         3            5.9         2.1 virginica\n 4          6.3         2.9          5.6         1.8 virginica\n 5          6.5         3            5.8         2.2 virginica\n 6          7.6         3            6.6         2.1 virginica\n 7          4.9         2.5          4.5         1.7 virginica\n 8          7.3         2.9          6.3         1.8 virginica\n 9          6.7         2.5          5.8         1.8 virginica\n10          7.2         3.6          6.1         2.5 virginica\n# … with 40 more rows\n\n\nLet’s now see how we do this in {healthyR}\n\nlibrary(healthyR)\n\nnamed_item_list(iris, Species)\n\n<list_of<\n  tbl_df<\n    Sepal.Length: double\n    Sepal.Width : double\n    Petal.Length: double\n    Petal.Width : double\n    Species     : factor<fb977>\n  >\n>[3]>\n$setosa\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 40 more rows\n\n$versicolor\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n 1          7           3.2          4.7         1.4 versicolor\n 2          6.4         3.2          4.5         1.5 versicolor\n 3          6.9         3.1          4.9         1.5 versicolor\n 4          5.5         2.3          4           1.3 versicolor\n 5          6.5         2.8          4.6         1.5 versicolor\n 6          5.7         2.8          4.5         1.3 versicolor\n 7          6.3         3.3          4.7         1.6 versicolor\n 8          4.9         2.4          3.3         1   versicolor\n 9          6.6         2.9          4.6         1.3 versicolor\n10          5.2         2.7          3.9         1.4 versicolor\n# … with 40 more rows\n\n$virginica\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>    \n 1          6.3         3.3          6           2.5 virginica\n 2          5.8         2.7          5.1         1.9 virginica\n 3          7.1         3            5.9         2.1 virginica\n 4          6.3         2.9          5.6         1.8 virginica\n 5          6.5         3            5.8         2.2 virginica\n 6          7.6         3            6.6         2.1 virginica\n 7          4.9         2.5          4.5         1.7 virginica\n 8          7.3         2.9          6.3         1.8 virginica\n 9          6.7         2.5          5.8         1.8 virginica\n10          7.2         3.6          6.1         2.5 virginica\n# … with 40 more rows\n\n\nIf you use this in conjunction with the healthyR function save_to_excel() then it will write an excel file with a tab for each named item in the list.\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-06/index.html",
    "href": "posts/rtip-2022-12-06/index.html",
    "title": "Z-Score Scaling Step Recipe with {healthyR.ai}",
    "section": "",
    "text": "Function\nHere is the full function call:\n\nstep_hai_scale_zscore(\n  recipe,\n  ...,\n  role = \"predictor\",\n  trained = FALSE,\n  columns = NULL,\n  skip = FALSE,\n  id = rand_id(\"hai_scale_zscore\")\n)\n\nHere are the arguments to the function.\n\nrecipe - A recipe object. The step will be added to the sequence of operations for this recipe.\n... - One or more selector functions to choose which variables that will be used to create the new variables. The selected variables should have class numeric\nrole - For model terms created by this step, what analysis role should they be assigned?. By default, the function assumes that the new variable columns created by the original variables will be used as predictors in a model.\ntrained - A logical to indicate if the quantities for preprocessing have been estimated.\ncolumns - A character string of variables that will be used as inputs. This field is a placeholder and will be populated once recipes::prep() is used.\nskip - A logical. Should the step be skipped when the recipe is baked by bake.recipe()? While all operations are baked when prep.recipe() is run, some operations may not be able to be conducted on new data (e.g. processing the outcome variable(s)). Care should be taken when using skip = TRUE as it may affect the computations for subsequent operations.\nid - A character string that is unique to this step to identify it.\n\n\n\nExample\nHere is a simple example.\n\nlibrary(healthyR.ai)\nlibrary(dplyr)\nlibrary(recipes)\n\ndf <- iris |>\n  as_tibble() |>\n  select(Species, Sepal.Length)\n\nrec_obj <- recipe(Sepal.Length ~ ., data = df) %>%\n  step_hai_scale_zscore(Sepal.Length)\n\nrec_obj\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          1\n\nOperations:\n\nZero-One Scale Transformation on Sepal.Length\n\nsummary(rec_obj)\n\n# A tibble: 2 × 4\n  variable     type      role      source  \n  <chr>        <list>    <chr>     <chr>   \n1 Species      <chr [3]> predictor original\n2 Sepal.Length <chr [2]> outcome   original\n\n\nNow let’s take a look at the differences.\n\nlibrary(ggplot2)\nlibrary(plotly)\n\ndf_tbl <- get_juiced_data(rec_obj)\n\ndf_tbl |>\n  purrr::set_names(\"Species\",\"Sepal_Length\",\"Scaled_Sepal_Length\") |>\n  ggplot(aes(x = Sepal_Length)) +\n  geom_histogram(color = \"black\", fill = \"lightgreen\") +\n  geom_histogram(aes(x = Scaled_Sepal_Length), \n                 color = \"black\", fill = \"steelblue\") +\n  theme_minimal() +\n  labs(\n    y = \"Count\",\n    x = \"Sepal Length\",\n    title = \"Speal.Length: Original vs. Z-Score Scaled\",\n    subtitle = \"Original (Light Green) Scaled (Steelblue)\"\n  )\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-07/index.html",
    "href": "posts/rtip-2022-12-07/index.html",
    "title": "Create Multiple {parsnip} Model Specs with {purrr}",
    "section": "",
    "text": "Function\nHere is the function call for the create_model_spec() for once it is release.\n\ncreate_model_spec(\n  .parsnip_eng = list(\"lm\"),\n  .mode = list(\"regression\"),\n  .parsnip_fns = list(\"linear_reg\"),\n  .return_tibble = TRUE\n)\n\nHere are the arguments to the function. * .parsnip_eng - The input must be a list. The default for this is set to all. This means that all of the parsnip linear regression engines will be used, for example lm, or glm. You can also choose to pass a c() vector like c(‘lm’, ‘glm’) * .mode- The input must be a list. The default is ‘regression’ * .parsnip_fns - The input must be a list. The default for this is set to all. This means that all of the parsnip linear regression functions will be used, for example linear_reg(), or cubist_rules. You can also choose to pass a c() vector like c(“linear_reg”,“cubist_rules”) * .return_tibble - The default is TRUE. FALSE will return a list object.\n\n\nExample\nHere is the function at work.\n\nlibrary(tidyaml)\n\ncreate_model_spec(\n .parsnip_eng = list(\"lm\",\"glm\",\"glmnet\",\"cubist\"),\n .parsnip_fns = list(\n      rep(\n        \"linear_reg\", 3),\n        \"cubist_rules\"\n     )\n )\n\n# A tibble: 4 × 4\n  .parsnip_engine .parsnip_mode .parsnip_fns .model_spec\n  <chr>           <chr>         <chr>        <list>     \n1 lm              regression    linear_reg   <spec[+]>  \n2 glm             regression    linear_reg   <spec[+]>  \n3 glmnet          regression    linear_reg   <spec[+]>  \n4 cubist          regression    cubist_rules <spec[+]>  \n\ncreate_model_spec(\n .parsnip_eng = list(\"lm\",\"glm\",\"glmnet\",\"cubist\"),\n .parsnip_fns = list(\n      rep(\n        \"linear_reg\", 3),\n        \"cubist_rules\"\n     ),\n .return_tibble = FALSE\n )\n\n$.parsnip_engine\n$.parsnip_engine[[1]]\n[1] \"lm\"\n\n$.parsnip_engine[[2]]\n[1] \"glm\"\n\n$.parsnip_engine[[3]]\n[1] \"glmnet\"\n\n$.parsnip_engine[[4]]\n[1] \"cubist\"\n\n\n$.parsnip_mode\n$.parsnip_mode[[1]]\n[1] \"regression\"\n\n\n$.parsnip_fns\n$.parsnip_fns[[1]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[2]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[3]]\n[1] \"linear_reg\"\n\n$.parsnip_fns[[4]]\n[1] \"cubist_rules\"\n\n\n$.model_spec\n$.model_spec[[1]]\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n$.model_spec[[2]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\n$.model_spec[[3]]\nLinear Regression Model Specification (regression)\n\nComputational engine: glmnet \n\n\n$.model_spec[[4]]\nCubist Model Specification (regression)\n\nComputational engine: cubist \n\n\nNow that we have seen what is to come in the future, let’s take a look at a pseudo solution that is easy to replicate now.\n\n# Load the purrr package\nlibrary(purrr)\nlibrary(parsnip)\n\n# Create a list of parsnip engines\nengines <- list(\n  engine1 = \"lm\",\n  engine2 = \"glm\",\n  engine3 = \"randomForest\"\n)\n\n# Create a list of parsnip call names\nparsnip_calls <- list(\n  call1 = \"linear_reg\",\n  call2 = \"linear_reg\",\n  call3 = \"rand_forest\"\n)\n\n# Use pmap() to create a list of parsnip model specs from the list of engines\n# and parsnip call names\n# Set the mode argument to \"regression\"\nmodel_specs <- pmap(list(engines, parsnip_calls), function(engine, call) {\n  match.fun(call)(engine = engine, mode = \"regression\")\n})\n\n# Print the list of model specs to the console\nmodel_specs\n\n$engine1\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n$engine2\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\n\n$engine3\nRandom Forest Model Specification (regression)\n\nComputational engine: randomForest \n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-08/index.html",
    "href": "posts/rtip-2022-12-08/index.html",
    "title": "Create a Faceted Historgram Plot with {healthyR.ai}",
    "section": "",
    "text": "Function\nLet’s take a look at the full function call.\n\nhai_histogram_facet_plot(\n  .data,\n  .bins = 10,\n  .scale_data = FALSE,\n  .ncol = 5,\n  .fct_reorder = FALSE,\n  .fct_rev = FALSE,\n  .fill = \"steelblue\",\n  .color = \"white\",\n  .scale = \"free\",\n  .interactive = FALSE\n)\n\nHere are the parameters and the arguments that get passed to them.\n\n.data - The data you want to pass to the function.\n.bins - The number of bins for the histograms.\n.scale_data - This is a boolean set to FALSE. TRUE will use hai_scale_zero_one_vec() to [0, 1] scale the data.\n.ncol - The number of columns for the facet_warp argument.\n.fct_reorder - Should the factor column be reordered? TRUE/FALSE, default of FALSE\n.fct_rev - Should the factor column be reversed? TRUE/FALSE, default of FALSE\n.fill - Default is steelblue\n.color - Default is ‘white’\n.scale - Default is ‘free’\n.interactive - Default is FALSE, TRUE will produce a {plotly} plot.\n\n\n\nExamples\nLet’s take a look at some example.\n\nlibrary(healthyR.ai, quietly = TRUE)\n\nhai_histogram_facet_plot(mtcars)\n\n\n\n\nNow lets scale the data and review.\n\nhai_histogram_facet_plot(mtcars, .scale_data = TRUE)\n\n\n\n\nLet’s take a look the iris data set now.\n\noutput <- hai_histogram_facet_plot(iris, .interactive = TRUE)\noutput$plot\n\n\n\n\n\nIn this blog post, we showcased the hai_histogram_facet_plot() function from the {healthyR.ai} package, which makes it easy to create histogram plots and faceted histograms in R. The hai_histogram_facet_plot() function allows you to quickly and easily visualize the distribution of your data, and can provide valuable insights into the relationships between different variables\nVoila!"
  },
  {
    "objectID": "posts/tidydensity-20221007/index.html",
    "href": "posts/tidydensity-20221007/index.html",
    "title": "TidyDensity Primer",
    "section": "",
    "text": "The goal of {TidyDensity} is to make working with random numbers from different distributions easy. All tidy_ distribution functions provide the following components:\n\n[r_]\n[d_]\n[q_]\n[p_]\n\n\nInstallation\nYou can install the released version of {TidyDensity} from CRAN with:\ninstall.packages(\"TidyDensity\")\nAnd the development version from GitHub with:\n# install.packages(\"devtools\")\ndevtools::install_github(\"spsanderson/TidyDensity\")\n\n\nExample Data\nThis is a basic example which shows you how to solve a common problem, which is, how do we generate randomly generated data from a normal distribution of some mean, and some standard deviation with n points and sims number of simulations?\nWith the function tidy_normal() we can generate such data. All functions that are condsidered tidy_ distribution functions, meaning those that generate randomly generated data from some distribution, have the same API call structure.\nFor example, using tidy_normal() the full function call at it’s default is as follows:\ntidy_normal(.n = 50, .mean = 0, .sd = 1, .num_sims = 1).\nWhat this means is that we want to generate 50 points from a standard normal distribution of mean 0 and with a standard deviation of 1, and we want to generate a single simulation of this data.\nLet’s see an example below:\n\nsuppressPackageStartupMessages(library(TidyDensity))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(ggplot2))\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\nset.seed(123)\ntidy_normal()\n\n# A tibble: 50 × 7\n   sim_number     x       y    dx       dy     p       q\n   <fct>      <int>   <dbl> <dbl>    <dbl> <dbl>   <dbl>\n 1 1              1 -0.560  -3.11 0.000256 0.288 -0.560 \n 2 1              2 -0.230  -2.98 0.000691 0.409 -0.230 \n 3 1              3  1.56   -2.85 0.00167  0.940  1.56  \n 4 1              4  0.0705 -2.72 0.00362  0.528  0.0705\n 5 1              5  0.129  -2.59 0.00707  0.551  0.129 \n 6 1              6  1.72   -2.45 0.0125   0.957  1.72  \n 7 1              7  0.461  -2.32 0.0201   0.678  0.461 \n 8 1              8 -1.27   -2.19 0.0298   0.103 -1.27  \n 9 1              9 -0.687  -2.06 0.0415   0.246 -0.687 \n10 1             10 -0.446  -1.93 0.0552   0.328 -0.446 \n# … with 40 more rows\n\n\nWhat comes back we see is a tibble. This is true for all functions in the {TidyDensity} library. It was a goal to return items that are consistent with the tidyverse.\nNow let’s talk a bit about what was actually returned. There are a few columns that are returned, these are referred to as the r, d, p, and q\n\n[r_] Shows as y and is the randomly generated data from the underlying distribution.\n[d_] Two components come back, dx and dy where these are generated from the [stats::density()] function with n set to .n from the function input.\n[p_] Shows as p and is the results of the p_ function, in this case pnorm() where the x of the input goes from 0-1 with .n points.\n[q_] Shows as q and is the results of the q_ function, in this case qnorm() where the x of the input goes from 0-1 with .n points.\n\nNow you will also see two more columns, namely, sim_number a factor column and x an integer column. The sim_number column represents the current simulation for which data was drawn, and the x represents the nth point in that simulation.\n\n\nVisualization Example\nWith data typically comes the need to see it! Show me the data! TidyDensity has a variety of autoplot functionality that will present only data from a tidy_ distribution function. We will take a look at output from tidy_normal() and set a see otherwise everytime this site is rendered the data would change.\n\nset.seed(123)\ntn <- tidy_normal(.n = 100, .num_sims = 6)\ntidy_autoplot(tn, .plot_type = \"density\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"quantile\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"probability\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"qq\")\n\n\n\n\nWe can see that the plots are faily informative. There are the regular density plot, the quantile plot, probability and qq plots. The title and subtitle of these plots are generated from attributes that are attached to the output of the tidy_ distribution function. Let’s take a look at the attributes of tn\n\nattributes(tn)\n\n$class\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$row.names\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n[163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n[181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n[199] 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n[217] 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234\n[235] 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252\n[253] 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270\n[271] 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288\n[289] 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306\n[307] 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324\n[325] 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342\n[343] 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360\n[361] 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378\n[379] 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396\n[397] 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414\n[415] 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432\n[433] 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450\n[451] 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468\n[469] 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486\n[487] 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504\n[505] 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522\n[523] 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540\n[541] 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558\n[559] 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576\n[577] 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594\n[595] 595 596 597 598 599 600\n\n$names\n[1] \"sim_number\" \"x\"          \"y\"          \"dx\"         \"dy\"        \n[6] \"p\"          \"q\"         \n\n$distribution_family_type\n[1] \"continuous\"\n\n$.mean\n[1] 0\n\n$.sd\n[1] 1\n\n$.n\n[1] 100\n\n$.num_sims\n[1] 6\n\n$tibble_type\n[1] \"tidy_gaussian\"\n\n$ps\n  [1] 0.00000000 0.01010101 0.02020202 0.03030303 0.04040404 0.05050505\n  [7] 0.06060606 0.07070707 0.08080808 0.09090909 0.10101010 0.11111111\n [13] 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717\n [19] 0.18181818 0.19191919 0.20202020 0.21212121 0.22222222 0.23232323\n [25] 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929\n [31] 0.30303030 0.31313131 0.32323232 0.33333333 0.34343434 0.35353535\n [37] 0.36363636 0.37373737 0.38383838 0.39393939 0.40404040 0.41414141\n [43] 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747\n [49] 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354\n [55] 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.59595960\n [61] 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566\n [67] 0.66666667 0.67676768 0.68686869 0.69696970 0.70707071 0.71717172\n [73] 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778\n [79] 0.78787879 0.79797980 0.80808081 0.81818182 0.82828283 0.83838384\n [85] 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.89898990\n [91] 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596\n [97] 0.96969697 0.97979798 0.98989899 1.00000000\n\n$qs\n  [1] 0.00000000 0.01010101 0.02020202 0.03030303 0.04040404 0.05050505\n  [7] 0.06060606 0.07070707 0.08080808 0.09090909 0.10101010 0.11111111\n [13] 0.12121212 0.13131313 0.14141414 0.15151515 0.16161616 0.17171717\n [19] 0.18181818 0.19191919 0.20202020 0.21212121 0.22222222 0.23232323\n [25] 0.24242424 0.25252525 0.26262626 0.27272727 0.28282828 0.29292929\n [31] 0.30303030 0.31313131 0.32323232 0.33333333 0.34343434 0.35353535\n [37] 0.36363636 0.37373737 0.38383838 0.39393939 0.40404040 0.41414141\n [43] 0.42424242 0.43434343 0.44444444 0.45454545 0.46464646 0.47474747\n [49] 0.48484848 0.49494949 0.50505051 0.51515152 0.52525253 0.53535354\n [55] 0.54545455 0.55555556 0.56565657 0.57575758 0.58585859 0.59595960\n [61] 0.60606061 0.61616162 0.62626263 0.63636364 0.64646465 0.65656566\n [67] 0.66666667 0.67676768 0.68686869 0.69696970 0.70707071 0.71717172\n [73] 0.72727273 0.73737374 0.74747475 0.75757576 0.76767677 0.77777778\n [79] 0.78787879 0.79797980 0.80808081 0.81818182 0.82828283 0.83838384\n [85] 0.84848485 0.85858586 0.86868687 0.87878788 0.88888889 0.89898990\n [91] 0.90909091 0.91919192 0.92929293 0.93939394 0.94949495 0.95959596\n [97] 0.96969697 0.97979798 0.98989899 1.00000000\n\n$param_grid\n# A tibble: 1 × 2\n  .mean   .sd\n  <dbl> <dbl>\n1     0     1\n\n$param_grid_txt\n[1] \"c(0, 1)\"\n\n$dist_with_params\n[1] \"Gaussian c(0, 1)\"\n\n\nI won’t go over them but as you can see, the attribute list can get long and has a lot of great information in it.\nNow what if we have simulations over 9? The legend would get fairly large making the visualization difficult to read.\nLet’s take a look at 20 simulations.\n\ntn <- tidy_normal(.n = 100, .num_sims = 20)\ntidy_autoplot(tn, .plot_type = \"density\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"quantile\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"probability\")\n\n\n\ntidy_autoplot(tn, .plot_type = \"qq\")\n\n\n\n\nWe see that the legend disappears! That’s great, but what if we still want to see what simulation is what? Well, make the plot interactive!\n\ntidy_autoplot(tn, .interactive = TRUE)"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html",
    "title": "PCA with healthyR.ai",
    "section": "",
    "text": "In this post we are going to talk about how you can perform principal component analysis in R with {healthyR.ai} in a tidyverse compliant fashion.\nThe specific function we are going to discuss on this post is pca_your_recipe()"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#library-load",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#library-load",
    "title": "PCA with healthyR.ai",
    "section": "Library Load",
    "text": "Library Load\n\npacman::p_load(\n  \"healthyR.ai\",\n  \"healthyR.data\",\n  \"timetk\",\n  \"dplyr\",\n  \"purrr\",\n  \"rsample\",\n  \"recipes\"\n)\n\nNow that we have our libraries loaded lets get the data."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#data",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#data",
    "title": "PCA with healthyR.ai",
    "section": "Data",
    "text": "Data\n\ndata_tbl <- healthyR_data %>%\n  select(visit_end_date_time) %>%\n  summarise_by_time(\n    .date_var = visit_end_date_time,\n    .by       = \"month\",\n    value     = n()\n  ) %>%\n  set_names(\"date_col\", \"value\") %>%\n  filter_by_time(\n    .date_var = date_col,\n    .start_date = \"2013\",\n    .end_date = \"2020\"\n  )\n\nhead(data_tbl, 5)\n\n# A tibble: 5 × 2\n  date_col            value\n  <dttm>              <int>\n1 2013-01-01 00:00:00  2082\n2 2013-02-01 00:00:00  1719\n3 2013-03-01 00:00:00  1796\n4 2013-04-01 00:00:00  1865\n5 2013-05-01 00:00:00  2028\n\n\nNow for the splits object."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#splits",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#splits",
    "title": "PCA with healthyR.ai",
    "section": "Splits",
    "text": "Splits\n\nsplits <- initial_split(data = data_tbl, prop = 0.8)\n\nsplits\n\n<Training/Testing/Total>\n<76/19/95>"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#recipe-and-output",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#recipe-and-output",
    "title": "PCA with healthyR.ai",
    "section": "Recipe and Output",
    "text": "Recipe and Output\nNow it is time for the recipe and the output objects.\n\nrec_obj <- recipe(value ~ ., training(splits)) %>%\n  step_timeseries_signature(date_col) %>%\n  step_rm(matches(\"(iso$)|(xts$)|(hour)|(min)|(sec)|(am.pm)\"))\n\noutput_list <- pca_your_recipe(rec_obj, .data = data_tbl)"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-transform",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-transform",
    "title": "PCA with healthyR.ai",
    "section": "PCA Transform",
    "text": "PCA Transform\n\noutput_list$pca_transform\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          1\n\nOperations:\n\nTimeseries signature features from date_col\nVariables removed matches(\"(iso$)|(xts$)|(hour)|(min)|(sec)|(am.pm)\")\nCentering for recipes::all_numeric()\nScaling for recipes::all_numeric()\nSparse, unbalanced variable filter on recipes::all_numeric()\nPCA extraction with recipes::all_numeric_predictors()"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#variable-loadings",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#variable-loadings",
    "title": "PCA with healthyR.ai",
    "section": "Variable Loadings",
    "text": "Variable Loadings\n\noutput_list$variable_loadings\n\n# A tibble: 169 × 4\n   terms                 value component id       \n   <chr>                 <dbl> <chr>     <chr>    \n 1 date_col_index.num -0.00137 PC1       pca_bVc37\n 2 date_col_year       0.0529  PC1       pca_bVc37\n 3 date_col_half      -0.385   PC1       pca_bVc37\n 4 date_col_quarter   -0.434   PC1       pca_bVc37\n 5 date_col_month     -0.437   PC1       pca_bVc37\n 6 date_col_wday      -0.0159  PC1       pca_bVc37\n 7 date_col_qday      -0.0608  PC1       pca_bVc37\n 8 date_col_yday      -0.437   PC1       pca_bVc37\n 9 date_col_mweek      0.0537  PC1       pca_bVc37\n10 date_col_week      -0.438   PC1       pca_bVc37\n# … with 159 more rows"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#variable-variance",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#variable-variance",
    "title": "PCA with healthyR.ai",
    "section": "Variable Variance",
    "text": "Variable Variance\n\noutput_list$variable_variance\n\n# A tibble: 52 × 4\n   terms       value component id       \n   <chr>       <dbl>     <int> <chr>    \n 1 variance 5.14             1 pca_bVc37\n 2 variance 2.08             2 pca_bVc37\n 3 variance 1.47             3 pca_bVc37\n 4 variance 1.40             4 pca_bVc37\n 5 variance 1.07             5 pca_bVc37\n 6 variance 0.684            6 pca_bVc37\n 7 variance 0.583            7 pca_bVc37\n 8 variance 0.519            8 pca_bVc37\n 9 variance 0.0534           9 pca_bVc37\n10 variance 0.000231        10 pca_bVc37\n# … with 42 more rows"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-estimates",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-estimates",
    "title": "PCA with healthyR.ai",
    "section": "PCA Estimates",
    "text": "PCA Estimates\n\noutput_list$pca_estimates\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          1\n\nTraining data contained 76 data points and no missing data.\n\nOperations:\n\nTimeseries signature features from date_col [trained]\nVariables removed date_col_year.iso, date_col_month.xts, date_col_hour, d... [trained]\nCentering for value, date_col_index.num, date_col_year, date_... [trained]\nScaling for value, date_col_index.num, date_col_year, date_... [trained]\nSparse, unbalanced variable filter removed date_col_day, date_col_mday, date_col_m... [trained]\nPCA extraction with date_col_index.num, date_col_year, date_col_half... [trained]"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-juiced-estimates",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-juiced-estimates",
    "title": "PCA with healthyR.ai",
    "section": "PCA Juiced Estimates",
    "text": "PCA Juiced Estimates\n\noutput_list$pca_juiced_estimates\n\n# A tibble: 76 × 8\n   date_col              value date_col…¹ date_…²     PC1     PC2     PC3    PC4\n   <dttm>                <dbl> <ord>      <ord>     <dbl>   <dbl>   <dbl>  <dbl>\n 1 2018-06-01 00:00:00  0.676  June       Friday   0.733   1.23   -1.30    0.536\n 2 2016-01-01 00:00:00 -0.133  January    Friday   3.51   -0.102  -0.500  -0.556\n 3 2013-05-01 00:00:00  1.67   May        Wednes…  1.15   -2.07   -1.68   -0.319\n 4 2018-10-01 00:00:00  0.337  October    Monday  -2.30    0.499   1.91   -1.92 \n 5 2016-09-01 00:00:00 -0.130  September  Thursd… -1.08    0.0972  0.410   2.64 \n 6 2016-07-01 00:00:00 -0.364  July       Friday  -0.0591  0.0211 -0.333   1.11 \n 7 2020-02-01 00:00:00 -0.646  February   Saturd…  3.08    2.48   -0.0249  0.214\n 8 2020-08-01 00:00:00 -1.42   August     Saturd… -0.491   2.60    0.142   1.88 \n 9 2018-03-01 00:00:00  0.243  March      Thursd…  2.74    0.848  -1.53    0.260\n10 2015-05-01 00:00:00 -0.0148 May        Friday   1.18   -0.636  -1.88   -0.147\n# … with 66 more rows, and abbreviated variable names ¹​date_col_month.lbl,\n#   ²​date_col_wday.lbl"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-baked-data",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-baked-data",
    "title": "PCA with healthyR.ai",
    "section": "PCA Baked Data",
    "text": "PCA Baked Data\n\noutput_list$pca_baked_data\n\n# A tibble: 95 × 8\n   date_col            value date_col_month…¹ date_…²    PC1   PC2    PC3    PC4\n   <dttm>              <dbl> <ord>            <ord>    <dbl> <dbl>  <dbl>  <dbl>\n 1 2013-01-01 00:00:00 1.86  January          Tuesday  3.60  -2.64  1.13  -0.871\n 2 2013-02-01 00:00:00 0.596 February         Friday   2.93  -1.76 -0.532  0.424\n 3 2013-03-01 00:00:00 0.864 March            Friday   2.62  -1.95 -2.24   0.643\n 4 2013-04-01 00:00:00 1.10  April            Monday   2.09  -2.68  1.39  -0.867\n 5 2013-05-01 00:00:00 1.67  May              Wednes…  1.15  -2.07 -1.68  -0.319\n 6 2013-06-01 00:00:00 0.923 June             Saturd…  0.612 -1.57 -2.01   0.919\n 7 2013-07-01 00:00:00 1.29  July             Monday  -0.669 -2.41  2.29  -0.420\n 8 2013-08-01 00:00:00 1.18  August           Thursd… -0.628 -1.76 -0.165  1.96 \n 9 2013-09-01 00:00:00 0.714 September        Sunday  -1.11  -2.19  0.910  2.25 \n10 2013-10-01 00:00:00 1.40  October          Tuesday -2.55  -1.91 -0.128 -1.48 \n# … with 85 more rows, and abbreviated variable names ¹​date_col_month.lbl,\n#   ²​date_col_wday.lbl"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-variance-data-frame",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-variance-data-frame",
    "title": "PCA with healthyR.ai",
    "section": "PCA Variance Data Frame",
    "text": "PCA Variance Data Frame\n\noutput_list$pca_variance_df\n\n# A tibble: 13 × 6\n   PC    var_explained var_pct_txt cum_var_pct cum_var_pct_txt ou_threshold\n   <chr>         <dbl> <chr>             <dbl> <chr>           <fct>       \n 1 PC1   0.395         39.55%            0.395 39.55%          Under       \n 2 PC2   0.160         16.02%            0.556 55.57%          Under       \n 3 PC3   0.113         11.29%            0.669 66.86%          Under       \n 4 PC4   0.107         10.75%            0.776 77.61%          Over        \n 5 PC5   0.0824        8.24%             0.858 85.85%          Over        \n 6 PC6   0.0526        5.26%             0.911 91.11%          Over        \n 7 PC7   0.0449        4.49%             0.956 95.59%          Over        \n 8 PC8   0.0400        4.00%             0.996 99.59%          Over        \n 9 PC9   0.00411       0.41%             1.00  100.00%         Over        \n10 PC10  0.0000178     0.00%             1.00  100.00%         Over        \n11 PC11  0.000000712   0.00%             1.00  100.00%         Over        \n12 PC12  0.000000273   0.00%             1.00  100.00%         Over        \n13 PC13  0.00000000196 0.00%             1     100.00%         Over"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-rotation-data-frame",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-rotation-data-frame",
    "title": "PCA with healthyR.ai",
    "section": "PCA Rotation Data Frame",
    "text": "PCA Rotation Data Frame\n\noutput_list$pca_rotation_df\n\n# A tibble: 13 × 13\n        PC1      PC2      PC3     PC4     PC5     PC6      PC7     PC8      PC9\n      <dbl>    <dbl>    <dbl>   <dbl>   <dbl>   <dbl>    <dbl>   <dbl>    <dbl>\n 1 -0.00137  0.671    0.116   -0.0521 -0.183   0.0165  0.0471   0.0277 -0.0177 \n 2  0.0529   0.667    0.116   -0.0610 -0.173   0.0146  0.0466   0.0313  0.00904\n 3 -0.385    0.0115   0.222    0.173   0.140   0.217   0.250   -0.0112  0.802  \n 4 -0.434    0.00824  0.0752  -0.0427  0.0615  0.135   0.00843  0.0332 -0.272  \n 5 -0.437    0.00278 -0.00879  0.0737 -0.0734  0.0167  0.00135 -0.0264 -0.213  \n 6 -0.0159   0.265   -0.406    0.274   0.468   0.254  -0.520   -0.366   0.0484 \n 7 -0.0608  -0.0200  -0.325    0.480  -0.542  -0.476  -0.0318  -0.244   0.187  \n 8 -0.437    0.00435 -0.00655  0.0740 -0.0733  0.0149  0.00231 -0.0303 -0.216  \n 9  0.0537  -0.164    0.562   -0.0242 -0.414   0.267  -0.572   -0.285   0.0519 \n10 -0.438    0.00638 -0.00736  0.0676 -0.0704  0.0236  0.00470 -0.0266 -0.219  \n11  0.250   -0.0238   0.208    0.420   0.0436  0.301   0.544   -0.492  -0.293  \n12 -0.0474   0.0762   0.516    0.142   0.460  -0.676  -0.108   -0.144  -0.0636 \n13  0.123    0.00776  0.150    0.666   0.0183  0.152  -0.161    0.676  -0.111  \n# … with 4 more variables: PC10 <dbl>, PC11 <dbl>, PC12 <dbl>, PC13 <dbl>"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-variance-scree-plot",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-variance-scree-plot",
    "title": "PCA with healthyR.ai",
    "section": "PCA Variance Scree Plot",
    "text": "PCA Variance Scree Plot\n\noutput_list$pca_variance_scree_plt"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-loadings-plot",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#pca-loadings-plot",
    "title": "PCA with healthyR.ai",
    "section": "PCA Loadings Plot",
    "text": "PCA Loadings Plot\n\noutput_list$pca_loadings_plt\n\n\n\noutput_list$pca_loadings_plotly"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#top-n-loadings-plots",
    "href": "posts/weekly-rtip-healthyrai-2022-10-28/index.html#top-n-loadings-plots",
    "title": "PCA with healthyR.ai",
    "section": "Top N Loadings Plots",
    "text": "Top N Loadings Plots\n\noutput_list$pca_top_n_loadings_plt\n\n\n\noutput_list$pca_top_n_plotly"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-12-02/index.html",
    "href": "posts/weekly-rtip-healthyrai-2022-12-02/index.html",
    "title": "Auto KNN with {healthyR.ai}",
    "section": "",
    "text": "Minimal coding ML is not something that is unheard of and is rather prolific, think h2o and pycaret just to name two. There is also no shortage available for R with the h2o interface, and tidyfit. There are also similar low-code workflows in my r package {healthyR.ai}. Today I will specifically go through the workflow for Automatic KNN classification for the Iris data set where we will classify the Species."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#recipe-output",
    "href": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#recipe-output",
    "title": "Auto KNN with {healthyR.ai}",
    "section": "Recipe Output",
    "text": "Recipe Output\n\nauto_knn$recipe_info\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nOperations:\n\nNovel factor level assignment for recipes::all_nominal_predictors()\nDummy variables from recipes::all_nominal_predictors()\nZero variance filter on recipes::all_predictors()\nCentering and scaling for recipes::all_numeric()"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#model-info",
    "href": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#model-info",
    "title": "Auto KNN with {healthyR.ai}",
    "section": "Model Info",
    "text": "Model Info\n\nauto_knn$model_info$was_tuned\n\n[1] \"tuned\"\n\n\n\nauto_knn$model_info$model_spec\n\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = tune::tune()\n  weight_func = tune::tune()\n  dist_power = tune::tune()\n\nComputational engine: kknn \n\n\n\nauto_knn$model_info$wflw\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_novel()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = tune::tune()\n  weight_func = tune::tune()\n  dist_power = tune::tune()\n\nComputational engine: kknn \n\n\n\nauto_knn$model_info$fitted_wflw\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_novel()\n• step_dummy()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nkknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(2L,     data, 5), distance = ~1.24146585899405, kernel = ~\"inv\")\n\nType of response variable: nominal\nMinimal misclassification: 0.03571429\nBest kernel: inv\nBest k: 2"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#tuning-info",
    "href": "posts/weekly-rtip-healthyrai-2022-12-02/index.html#tuning-info",
    "title": "Auto KNN with {healthyR.ai}",
    "section": "Tuning Info",
    "text": "Tuning Info\n\nauto_knn$tuned_info$tuning_grid\n\n# A tibble: 10 × 3\n   neighbors weight_func  dist_power\n       <int> <chr>             <dbl>\n 1         6 rank              1.81 \n 2         4 biweight          0.266\n 3         9 rectangular       0.647\n 4         7 triweight         0.783\n 5         4 epanechnikov      1.05 \n 6        12 gaussian          1.16 \n 7        11 cos               1.83 \n 8        14 triangular        0.457\n 9         2 inv               1.24 \n10        10 optimal           1.45 \n\n\n\nauto_knn$tuned_info$cv_obj\n\n# Monte Carlo cross-validation (0.75/0.25) with 25 resamples  \n# A tibble: 25 × 2\n   splits          id        \n   <list>          <chr>     \n 1 <split [84/28]> Resample01\n 2 <split [84/28]> Resample02\n 3 <split [84/28]> Resample03\n 4 <split [84/28]> Resample04\n 5 <split [84/28]> Resample05\n 6 <split [84/28]> Resample06\n 7 <split [84/28]> Resample07\n 8 <split [84/28]> Resample08\n 9 <split [84/28]> Resample09\n10 <split [84/28]> Resample10\n# … with 15 more rows\n\n\n\nauto_knn$tuned_info$tuned_results\n\n# Tuning results\n# Monte Carlo cross-validation (0.75/0.25) with 25 resamples  \n# A tibble: 25 × 4\n   splits          id         .metrics           .notes          \n   <list>          <chr>      <list>             <list>          \n 1 <split [84/28]> Resample01 <tibble [110 × 7]> <tibble [0 × 3]>\n 2 <split [84/28]> Resample02 <tibble [110 × 7]> <tibble [0 × 3]>\n 3 <split [84/28]> Resample03 <tibble [110 × 7]> <tibble [0 × 3]>\n 4 <split [84/28]> Resample04 <tibble [110 × 7]> <tibble [0 × 3]>\n 5 <split [84/28]> Resample05 <tibble [110 × 7]> <tibble [0 × 3]>\n 6 <split [84/28]> Resample06 <tibble [110 × 7]> <tibble [0 × 3]>\n 7 <split [84/28]> Resample07 <tibble [110 × 7]> <tibble [0 × 3]>\n 8 <split [84/28]> Resample08 <tibble [110 × 7]> <tibble [0 × 3]>\n 9 <split [84/28]> Resample09 <tibble [110 × 7]> <tibble [0 × 3]>\n10 <split [84/28]> Resample10 <tibble [110 × 7]> <tibble [0 × 3]>\n# … with 15 more rows\n\n\n\nauto_knn$tuned_info$grid_size\n\n[1] 10\n\n\n\nauto_knn$tuned_info$best_metric\n\n[1] \"f_meas\"\n\n\n\nauto_knn$tuned_info$best_result_set\n\n# A tibble: 1 × 9\n  neighbors weight_func dist_power .metric .estima…¹  mean     n std_err .config\n      <int> <chr>            <dbl> <chr>   <chr>     <dbl> <int>   <dbl> <chr>  \n1         2 inv               1.24 f_meas  macro     0.965    25 0.00788 Prepro…\n# … with abbreviated variable name ¹​.estimator\n\n\n\nauto_knn$tuned_info$tuning_grid_plot\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\nauto_knn$tuned_info$plotly_grid_plot\n\n\n\n\n\nVoila!\nThank you for reading."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html",
    "title": "Time Series Lag Correlation Plots",
    "section": "",
    "text": "In time series analysis there is something called a lag. This simply means we take a look at some past event from some point in time t. This is a non-statistical method for looking at a relationship between a timeseries and its lags.\n{healthyR.ts} has a function called ts_lag_correlation(). This function, as described by it’s name, provides more than just a simple lag plot.\nThis function provides a lot of extra information for the end user. First let’s go over the function call."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#function-call",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#function-call",
    "title": "Time Series Lag Correlation Plots",
    "section": "Function Call",
    "text": "Function Call\nHere is the full call:\n\nts_lag_correlation(\n  .data,\n  .date_col,\n  .value_col,\n  .lags = 1,\n  .heatmap_color_low = \"white\",\n  .heatmap_color_hi = \"steelblue\"\n)\n\nHere are the arguments that get supplied to the different parameters.\n\n.data - A tibble of time series data\n.date_col - A date column\n.value_col - The value column being analyzed\n.lags - This is a vector of integer lags, ie 1 or c(1,6,12)\n.heatmap_color_low - What color should the low values of the heatmap of the correlation matrix be, the default is ‘white’\n.heatmap_color_hi - What color should the low values of the heatmap of the correlation matrix be, the default is ‘steelblue’"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#function-return",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#function-return",
    "title": "Time Series Lag Correlation Plots",
    "section": "Function Return",
    "text": "Function Return\nThe function itself returns a list object. The list has the following elements in it:\nData Elements\n\nlag_list\nlag_tbl\ncorrelation_lag_matrix\ncorrelation_lag_tbl\n\nPlot Elements\n\nlag_plot\nplotly_lag_plot\ncorrelation_heatmap\nplotly_heatmap"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#data-elements",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#data-elements",
    "title": "Time Series Lag Correlation Plots",
    "section": "Data Elements",
    "text": "Data Elements\nHere are the data elements.\n\noutput$data$lag_list\n\n[[1]]\n# A tibble: 143 × 3\n   lag   value lagged_value\n   <fct> <dbl>        <dbl>\n 1 1       118          112\n 2 1       132          118\n 3 1       129          132\n 4 1       121          129\n 5 1       135          121\n 6 1       148          135\n 7 1       148          148\n 8 1       136          148\n 9 1       119          136\n10 1       104          119\n# … with 133 more rows\n\n[[2]]\n# A tibble: 141 × 3\n   lag   value lagged_value\n   <fct> <dbl>        <dbl>\n 1 3       129          112\n 2 3       121          118\n 3 3       135          132\n 4 3       148          129\n 5 3       148          121\n 6 3       136          135\n 7 3       119          148\n 8 3       104          148\n 9 3       118          136\n10 3       115          119\n# … with 131 more rows\n\n[[3]]\n# A tibble: 138 × 3\n   lag   value lagged_value\n   <fct> <dbl>        <dbl>\n 1 6       148          112\n 2 6       148          118\n 3 6       136          132\n 4 6       119          129\n 5 6       104          121\n 6 6       118          135\n 7 6       115          148\n 8 6       126          148\n 9 6       141          136\n10 6       135          119\n# … with 128 more rows\n\n[[4]]\n# A tibble: 132 × 3\n   lag   value lagged_value\n   <fct> <dbl>        <dbl>\n 1 12      115          112\n 2 12      126          118\n 3 12      141          132\n 4 12      135          129\n 5 12      125          121\n 6 12      149          135\n 7 12      170          148\n 8 12      170          148\n 9 12      158          136\n10 12      133          119\n# … with 122 more rows\n\n\nThis is a list of all the tibbles of the different lags that were chosen.\n\noutput$data$lag_tbl\n\n# A tibble: 554 × 4\n   lag   value lagged_value lag_title\n   <fct> <dbl>        <dbl> <fct>    \n 1 1       118          112 Lag: 1   \n 2 1       132          118 Lag: 1   \n 3 1       129          132 Lag: 1   \n 4 1       121          129 Lag: 1   \n 5 1       135          121 Lag: 1   \n 6 1       148          135 Lag: 1   \n 7 1       148          148 Lag: 1   \n 8 1       136          148 Lag: 1   \n 9 1       119          136 Lag: 1   \n10 1       104          119 Lag: 1   \n# … with 544 more rows\n\n\nThis is the long lag tibble with all of the lags in it.\n\noutput$data$correlation_lag_matrix\n\n                value value_lag1 value_lag3 value_lag6 value_lag12\nvalue       1.0000000  0.9542938  0.8186636  0.7657001   0.9905274\nvalue_lag1  0.9542938  1.0000000  0.8828054  0.7726530   0.9492382\nvalue_lag3  0.8186636  0.8828054  1.0000000  0.8349550   0.8218493\nvalue_lag6  0.7657001  0.7726530  0.8349550  1.0000000   0.7780911\nvalue_lag12 0.9905274  0.9492382  0.8218493  0.7780911   1.0000000\n\n\nThis is the correlation matrix.\n\noutput$data$correlation_lag_tbl\n\n# A tibble: 25 × 3\n   name        data_names value\n   <fct>       <fct>      <dbl>\n 1 value       value      1    \n 2 value_lag1  value      0.954\n 3 value_lag3  value      0.819\n 4 value_lag6  value      0.766\n 5 value_lag12 value      0.991\n 6 value       value_lag1 0.954\n 7 value_lag1  value_lag1 1    \n 8 value_lag3  value_lag1 0.883\n 9 value_lag6  value_lag1 0.773\n10 value_lag12 value_lag1 0.949\n# … with 15 more rows\n\n\nThis is the correlation lag tibble"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#plot-elements",
    "href": "posts/weekly-rtip-healthyrts-2022-11-11/index.html#plot-elements",
    "title": "Time Series Lag Correlation Plots",
    "section": "Plot Elements",
    "text": "Plot Elements\n\noutput$plots$lag_plot\n\n\n\n\nThe Lag Plot itself.\n\noutput$plots$plotly_lag_plot\n\n\n\n\n\nA plotly version of the lag plot.\n\noutput$plots$correlation_heatmap\n\n\n\n\nA heatmap of the correlations.\n\noutput$plots$plotly_heatmap\n\n\n\n\n\nA plotly version of the correlation heatmap.\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-tidydensity-2022-11-23/index.html",
    "href": "posts/weekly-rtip-tidydensity-2022-11-23/index.html",
    "title": "Summary Statistics with {TidyDensity}",
    "section": "",
    "text": "Many times someone may want to see a summary or cumulative statistic for a given set of data or even from several simulations of data. I went over bootstrap plotting earlier this month, and this is a form of what we will go over today although slightly more restrictive.\nI have decided to make today my weekly r-tip because tomorrow is Thanksgiving here in the US and I am taking an extended holiday so I won’t be back until Monday.\nToday’s function and weekly tip is on tidy_stat_tbl(). It is meant to be used with a tidy_ distribution function. Let’s take a look."
  },
  {
    "objectID": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#single-simulation",
    "href": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#single-simulation",
    "title": "Summary Statistics with {TidyDensity}",
    "section": "Single Simulation",
    "text": "Single Simulation\nLet’s go over some examples. Firstly, we will go over all the different .return_type’s of a single simulation of tidy_normal() using the quantile function.\nVector Output BE CAREFUL IT USES SAPPLY\n\nlibrary(TidyDensity)\n\nset.seed(123)\ntn <- tidy_normal()\n\ntidy_stat_tbl(\n  .data = tn,\n  .x = y,\n  .return_type = \"vector\",\n  .fns = quantile,\n  na.rm = TRUE,\n  probs = c(0.025, 0.5, 0.975)\n  )\n\n      sim_number_1\n2.5%   -1.59190149\n50%    -0.07264039\n97.5%   1.77074730\n\n\nList Output with lapply\n\ntidy_stat_tbl(\n  tn, y, quantile, \"list\", na.rm = TRUE\n)\n\n$sim_number_1\n         0%         25%         50%         75%        100% \n-1.96661716 -0.55931702 -0.07264039  0.69817699  2.16895597 \n\ntidy_stat_tbl(\n  tn, y, quantile, \"list\", na.rm = TRUE, \n  probs = c(0.025, 0.5, 0.975)\n)\n\n$sim_number_1\n       2.5%         50%       97.5% \n-1.59190149 -0.07264039  1.77074730 \n\n\nTibble output with tibble\n\ntidy_stat_tbl(\n  tn, y, quantile, \"tibble\", na.rm = TRUE\n)\n\n# A tibble: 5 × 3\n  sim_number name  quantile\n  <fct>      <chr>    <dbl>\n1 1          0%     -1.97  \n2 1          25%    -0.559 \n3 1          50%    -0.0726\n4 1          75%     0.698 \n5 1          100%    2.17  \n\ntidy_stat_tbl(\n  tn, y, quantile, \"tibble\", na.rm = TRUE, \n  probs = c(0.025, 0.5, 0.975)\n)\n\n# A tibble: 3 × 3\n  sim_number name  quantile\n  <fct>      <chr>    <dbl>\n1 1          2.5%   -1.59  \n2 1          50%    -0.0726\n3 1          97.5%   1.77  \n\n\nTibble output with data.table The output object is a tibble but data.table is used to perform the calculations which can be magnitudes faster when simulations are large. I will showcase down the post.\n\ntidy_stat_tbl(\n  tn, y, quantile, .use_data_table = TRUE, na.rm = TRUE\n)\n\n# A tibble: 5 × 3\n  sim_number name  quantile\n  <fct>      <fct>    <dbl>\n1 1          0%     -1.97  \n2 1          25%    -0.559 \n3 1          50%    -0.0726\n4 1          75%     0.698 \n5 1          100%    2.17  \n\ntidy_stat_tbl(\n  tn, y, quantile, .use_data_table = TRUE, na.rm = TRUE, \n  probs = c(0.025, 0.5, 0.975)\n)\n\n# A tibble: 3 × 3\n  sim_number name  quantile\n  <fct>      <fct>    <dbl>\n1 1          2.5%   -1.59  \n2 1          50%    -0.0726\n3 1          97.5%   1.77  \n\n\nNow let’s take a look with multiple simulations."
  },
  {
    "objectID": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#multiple-simulations",
    "href": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#multiple-simulations",
    "title": "Summary Statistics with {TidyDensity}",
    "section": "Multiple Simulations",
    "text": "Multiple Simulations\nLet’s set our simulation count to 5. While this is not a large amount it will serve as a good illustration on the outputs.\n\nns <- 5\nf  <- quantile\nnr <- TRUE\np  <- c(0.025, 0.975)\n\nOk let’s run the same simulations but with the updated params.\nVector Output BE CAREFUL IT USES SAPPLY\n\nset.seed(123)\ntn <- tidy_normal(.num_sims = ns)\n\ntidy_stat_tbl(\n  .data = tn,\n  .x = y,\n  .return_type = \"vector\",\n  .fns = f,\n  na.rm = nr,\n  probs = p\n  )\n\n      sim_number_1 sim_number_2 sim_number_3 sim_number_4 sim_number_5\n2.5%     -1.591901    -1.474945    -1.656679    -1.258156    -1.309749\n97.5%     1.770747     1.933653     1.894424     2.098923     1.943384\n\ntidy_stat_tbl(\n  tn, y, .return_type = \"vector\",\n  .fns = f, na.rm = nr\n)\n\n     sim_number_1 sim_number_2 sim_number_3 sim_number_4 sim_number_5\n0%    -1.96661716   -2.3091689   -2.0532472  -1.31080153   -1.3598407\n25%   -0.55931702   -0.3612969   -0.9505826  -0.49541417   -0.7140627\n50%   -0.07264039    0.1525789   -0.3048700  -0.07675993   -0.2240352\n75%    0.69817699    0.6294358    0.2900859   0.55145766    0.5287605\n100%   2.16895597    2.1873330    2.1001089   3.24103993    2.1988103\n\n\nList Output with lapply\n\ntidy_stat_tbl(\n  tn, y, f, \"list\", na.rm = nr\n)\n\n$sim_number_1\n         0%         25%         50%         75%        100% \n-1.96661716 -0.55931702 -0.07264039  0.69817699  2.16895597 \n\n$sim_number_2\n        0%        25%        50%        75%       100% \n-2.3091689 -0.3612969  0.1525789  0.6294358  2.1873330 \n\n$sim_number_3\n        0%        25%        50%        75%       100% \n-2.0532472 -0.9505826 -0.3048700  0.2900859  2.1001089 \n\n$sim_number_4\n         0%         25%         50%         75%        100% \n-1.31080153 -0.49541417 -0.07675993  0.55145766  3.24103993 \n\n$sim_number_5\n        0%        25%        50%        75%       100% \n-1.3598407 -0.7140627 -0.2240352  0.5287605  2.1988103 \n\ntidy_stat_tbl(\n  tn, y, f, \"list\", na.rm = nr, \n  probs = p\n)\n\n$sim_number_1\n     2.5%     97.5% \n-1.591901  1.770747 \n\n$sim_number_2\n     2.5%     97.5% \n-1.474945  1.933653 \n\n$sim_number_3\n     2.5%     97.5% \n-1.656679  1.894424 \n\n$sim_number_4\n     2.5%     97.5% \n-1.258156  2.098923 \n\n$sim_number_5\n     2.5%     97.5% \n-1.309749  1.943384 \n\n\nTibble output with tibble\n\ntidy_stat_tbl(\n  tn, y, f, \"tibble\", na.rm = nr\n)\n\n# A tibble: 25 × 3\n   sim_number name        f\n   <fct>      <chr>   <dbl>\n 1 1          0%    -1.97  \n 2 1          25%   -0.559 \n 3 1          50%   -0.0726\n 4 1          75%    0.698 \n 5 1          100%   2.17  \n 6 2          0%    -2.31  \n 7 2          25%   -0.361 \n 8 2          50%    0.153 \n 9 2          75%    0.629 \n10 2          100%   2.19  \n# … with 15 more rows\n\ntidy_stat_tbl(\n  tn, y, f, \"tibble\", na.rm = nr, \n  probs = p\n)\n\n# A tibble: 10 × 3\n   sim_number name      f\n   <fct>      <chr> <dbl>\n 1 1          2.5%  -1.59\n 2 1          97.5%  1.77\n 3 2          2.5%  -1.47\n 4 2          97.5%  1.93\n 5 3          2.5%  -1.66\n 6 3          97.5%  1.89\n 7 4          2.5%  -1.26\n 8 4          97.5%  2.10\n 9 5          2.5%  -1.31\n10 5          97.5%  1.94\n\n\nTibble output with data.table The output object is a tibble but data.table is used to perform the calculations which can be magnitudes faster when simulations are large. I will showcase down the post.\n\ntidy_stat_tbl(\n  tn, y, f, .use_data_table = TRUE, na.rm = nr\n)\n\n# A tibble: 25 × 3\n   sim_number name        f\n   <fct>      <fct>   <dbl>\n 1 1          0%    -1.97  \n 2 1          25%   -0.559 \n 3 1          50%   -0.0726\n 4 1          75%    0.698 \n 5 1          100%   2.17  \n 6 2          0%    -2.31  \n 7 2          25%   -0.361 \n 8 2          50%    0.153 \n 9 2          75%    0.629 \n10 2          100%   2.19  \n# … with 15 more rows\n\ntidy_stat_tbl(\n  tn, y, f, .use_data_table = TRUE, na.rm = nr, \n  probs = p\n)\n\n# A tibble: 10 × 3\n   sim_number name      f\n   <fct>      <fct> <dbl>\n 1 1          2.5%  -1.59\n 2 1          97.5%  1.77\n 3 2          2.5%  -1.47\n 4 2          97.5%  1.93\n 5 3          2.5%  -1.66\n 6 3          97.5%  1.89\n 7 4          2.5%  -1.26\n 8 4          97.5%  2.10\n 9 5          2.5%  -1.31\n10 5          97.5%  1.94\n\n\nOk, now that we have shown that, let’s ratchet up the simulations so we can see the true difference in using the .use_data_tbl parameter when simulations are large. We are going to use {rbenchmark} for"
  },
  {
    "objectID": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#benchmarking",
    "href": "posts/weekly-rtip-tidydensity-2022-11-23/index.html#benchmarking",
    "title": "Summary Statistics with {TidyDensity}",
    "section": "Benchmarking",
    "text": "Benchmarking\nHere we go. We are going to make a tidy_bootstrap() of the mtcars$mpg data which will produce 2000 simulations, we will replicate this 25 times.\n\nlibrary(rbenchmark)\nlibrary(TidyDensity)\nlibrary(dplyr)\n\n# Get the interesting vector, well for this anyways\nx <- mtcars$mpg\n\n# Bootstrap the vector (2k simulations is default)\ntb <- tidy_bootstrap(x) %>%\n  bootstrap_unnest_tbl()\n\nbenchmark(\n  \"tibble\" = {\n    tidy_stat_tbl(tb, y, IQR, \"tibble\")\n  },\n  \"data.table\" = {\n    tidy_stat_tbl(tb, y, IQR, .use_data_table = TRUE, type = 7)\n  },\n  \"sapply\" = {\n    tidy_stat_tbl(tb, y, IQR, \"vector\")\n  },\n  \"lapply\" = {\n    tidy_stat_tbl(tb, y, IQR, \"list\")\n  },\n  replications = 25,\n  columns = c(\"test\",\"replications\",\"elapsed\",\"relative\",\"user.self\",\"sys.self\"  )\n) %>%\n  arrange(relative)\n\n        test replications elapsed relative user.self sys.self\n1 data.table           25    4.11    1.000      3.33     0.11\n2     lapply           25   24.14    5.873     20.02     0.38\n3     sapply           25   25.11    6.109     21.01     0.28\n4     tibble           25   33.18    8.073     27.45     0.51\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-update-2022-11-18/index.html",
    "href": "posts/weekly-rtip-update-2022-11-18/index.html",
    "title": "Updates to {healthyverse} packages",
    "section": "",
    "text": "I have made several updates to {healthyverse}, this has resulted in new releases to CRAN for {healthyR.ai}, {healthyR.ts}, and {TidyDesnsity}."
  },
  {
    "objectID": "posts/weekly-rtip-update-2022-11-18/index.html#tidydensity-1",
    "href": "posts/weekly-rtip-update-2022-11-18/index.html#tidydensity-1",
    "title": "Updates to {healthyverse} packages",
    "section": "TidyDensity",
    "text": "TidyDensity\nFor TidyDensity a new distribution was added, welcome tidy_bernoulli(). This distribution also comes with the standard util_distname_param_estimate() and the util_distname_stats_tbl() functions. Let’s take a look at the function calls.\n\ntidy_bernoulli(.n = 50, .prob = 0.1, .num_sims = 1)\n\nutil_bernoulli_param_estimate(.x, .auto_gen_empirical = TRUE)\n\nutil_bernoulli_stats_tbl(.data)\n\nLet’s see them in use.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\ntb <- tidy_bernoulli()\n\ntb\n\n# A tibble: 50 × 7\n   sim_number     x     y      dx     dy     p     q\n   <fct>      <int> <int>   <dbl>  <dbl> <dbl> <dbl>\n 1 1              1     0 -0.338  0.0366   0.9     0\n 2 1              2     0 -0.304  0.0866   0.9     0\n 3 1              3     0 -0.270  0.187    0.9     0\n 4 1              4     0 -0.236  0.369    0.9     0\n 5 1              5     0 -0.201  0.663    0.9     0\n 6 1              6     0 -0.167  1.09     0.9     0\n 7 1              7     0 -0.133  1.63     0.9     0\n 8 1              8     1 -0.0988 2.22     1       1\n 9 1              9     0 -0.0646 2.76     0.9     0\n10 1             10     0 -0.0304 3.14     0.9     0\n# … with 40 more rows\n\nutil_bernoulli_param_estimate(tb$y)\n\n$combined_data_tbl\n# A tibble: 100 × 8\n   sim_number     x     y      dx     dy     p     q dist_type\n   <fct>      <int> <dbl>   <dbl>  <dbl> <dbl> <dbl> <fct>    \n 1 1              1     0 -0.338  0.0366  0.92     0 Empirical\n 2 1              2     0 -0.304  0.0866  0.92     0 Empirical\n 3 1              3     0 -0.270  0.187   0.92     0 Empirical\n 4 1              4     0 -0.236  0.369   0.92     0 Empirical\n 5 1              5     0 -0.201  0.663   0.92     0 Empirical\n 6 1              6     0 -0.167  1.09    0.92     0 Empirical\n 7 1              7     0 -0.133  1.63    0.92     0 Empirical\n 8 1              8     1 -0.0988 2.22    1        0 Empirical\n 9 1              9     0 -0.0646 2.76    0.92     0 Empirical\n10 1             10     0 -0.0304 3.14    0.92     0 Empirical\n# … with 90 more rows\n\n$parameter_tbl\n# A tibble: 1 × 8\n  dist_type samp_size   min   max  mean variance sum_x  prob\n  <chr>         <int> <dbl> <dbl> <dbl>    <dbl> <dbl> <dbl>\n1 Bernoulli        50     0     1  0.08   0.0736     4  0.08\n\nutil_bernoulli_stats_tbl(tb) %>%\n  glimpse()\n\nRows: 1\nColumns: 18\n$ tidy_function      <chr> \"tidy_bernoulli\"\n$ function_call      <chr> \"Bernoulli c(0.1)\"\n$ distribution       <chr> \"Bernoulli\"\n$ distribution_type  <chr> \"discrete\"\n$ points             <dbl> 50\n$ simulations        <dbl> 1\n$ mean               <dbl> 0.1\n$ mode               <chr> \"0\"\n$ coeff_var          <dbl> 0.09\n$ skewness           <dbl> 2.666667\n$ kurtosis           <dbl> 5.111111\n$ mad                <dbl> 0.5\n$ entropy            <dbl> 0.325083\n$ fisher_information <dbl> 11.11111\n$ computed_std_skew  <dbl> 3.096281\n$ computed_std_kurt  <dbl> 10.58696\n$ ci_lo              <dbl> 0\n$ ci_hi              <dbl> 1"
  },
  {
    "objectID": "posts/weekly-rtip-update-2022-11-18/index.html#healthyr.ai-1",
    "href": "posts/weekly-rtip-update-2022-11-18/index.html#healthyr.ai-1",
    "title": "Updates to {healthyverse} packages",
    "section": "healthyR.ai",
    "text": "healthyR.ai\nThis was a minor patch release that exported some previously internal only functions and fixed an error with the custom recipe steps. One of the functions that has been exported is hai_data_impute()\nLet’s take a look.\n\nhai_data_impute(\n  .recipe_object = NULL,\n  ...,\n  .seed_value = 123,\n  .type_of_imputation = \"mean\",\n  .number_of_trees = 25,\n  .neighbors = 5,\n  .mean_trim = 0,\n  .roll_statistic,\n  .roll_window = 5\n)\n\nLet’s take a look at an example of it’s use.\n\nlibrary(dplyr)\nlibrary(recipes)\nlibrary(healthyR.ai)\n\ndate_seq <- seq.Date(from = as.Date(\"2013-01-01\"), length.out = 100, by = \"month\")\nval_seq <- rep(c(rnorm(9), NA), times = 10)\ndf_tbl <- tibble(\n  date_col = date_seq,\n  value    = val_seq\n)\n\ndf_tbl\n\n# A tibble: 100 × 2\n   date_col     value\n   <date>       <dbl>\n 1 2013-01-01  0.325 \n 2 2013-02-01 -0.308 \n 3 2013-03-01  0.562 \n 4 2013-04-01  0.0805\n 5 2013-05-01 -0.528 \n 6 2013-06-01 -2.49  \n 7 2013-07-01 -0.964 \n 8 2013-08-01 -0.337 \n 9 2013-09-01 -1.08  \n10 2013-10-01 NA     \n# … with 90 more rows\n\nrec_obj <- recipe(value ~ ., df_tbl)\n\nhai_data_impute(\n  .recipe_object = rec_obj,\n  value,\n  .type_of_imputation = \"roll\",\n  .roll_statistic = median\n)$impute_rec_obj %>%\n  get_juiced_data()\n\n# A tibble: 100 × 2\n   date_col     value\n   <date>       <dbl>\n 1 2013-01-01  0.325 \n 2 2013-02-01 -0.308 \n 3 2013-03-01  0.562 \n 4 2013-04-01  0.0805\n 5 2013-05-01 -0.528 \n 6 2013-06-01 -2.49  \n 7 2013-07-01 -0.964 \n 8 2013-08-01 -0.337 \n 9 2013-09-01 -1.08  \n10 2013-10-01 -0.322 \n# … with 90 more rows"
  },
  {
    "objectID": "posts/weekly-rtip-update-2022-11-18/index.html#healthyr.ts-1",
    "href": "posts/weekly-rtip-update-2022-11-18/index.html#healthyr.ts-1",
    "title": "Updates to {healthyverse} packages",
    "section": "healthyR.ts",
    "text": "healthyR.ts\nThis was a minor patch release fixing the function ts_lag_correlation() when the column that was the value was not explicitly called…value.\nThank you!"
  },
  {
    "objectID": "posts/weekly-tip-tidydensity-2022-11-04/index.html",
    "href": "posts/weekly-tip-tidydensity-2022-11-04/index.html",
    "title": "Bootstrapping and Plots with TidyDensity",
    "section": "",
    "text": "Many times in the real world we have a data set which is actually a sample as we typically do not know what the actual population is. This is where bootstrapping tends to come into play. It allows us to get a hold on what the possible parameter values are by taking repeated samples of the data that is available to us.\nAt it’s core it is a resampling method with replacement where it assigns measures of accuracy to the sample estimates. Here is the Wikipedia Article for bootstrapping.\nIn this post I am going to go over how to use the bootstrap function set with {TidyDensity}. You can find the pkgdown site with all function references here: TidyDensity"
  },
  {
    "objectID": "posts/weekly-tip-tidydensity-2022-11-04/index.html#cumulative-mean",
    "href": "posts/weekly-tip-tidydensity-2022-11-04/index.html#cumulative-mean",
    "title": "Bootstrapping and Plots with TidyDensity",
    "section": "Cumulative Mean",
    "text": "Cumulative Mean\n\ntb %>%\n  bootstrap_stat_plot(.value = y)\n\n\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .show_groups = TRUE)\n\n\n\ntb %>%\n  bootstrap_stat_plot(\n    .value = y,\n    .show_groups = TRUE,\n    .show_ci_labels = FALSE\n  )\n\n\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .interactive = TRUE)\n\n\n\n\n\nYou can see from this output that the statistic you choose is printed in the chart title and on the y axis, the caption will also tell you how many simulations are present. Lets look at skewness as another example."
  },
  {
    "objectID": "posts/weekly-tip-tidydensity-2022-11-04/index.html#cumulative-skewness",
    "href": "posts/weekly-tip-tidydensity-2022-11-04/index.html#cumulative-skewness",
    "title": "Bootstrapping and Plots with TidyDensity",
    "section": "Cumulative Skewness",
    "text": "Cumulative Skewness\n\nsc <- \"cskewness\"\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .stat = sc)\n\n\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .show_groups = TRUE,\n                      .stat = sc)\n\n\n\ntb %>%\n  bootstrap_stat_plot(\n    .value = y,\n    .stat = sc,\n    .show_groups = TRUE,\n    .show_ci_labels = FALSE\n  )\n\n\n\ntb %>%\n  bootstrap_stat_plot(.value = y, .interactive = TRUE,\n                      .show_groups = TRUE,\n                      .stat = sc)\n\n\n\n\n\nVolia!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To Steve On Data",
    "section": "",
    "text": "I am using this as a site to host all of the tips and tricks for R/SQL and data that I will post to my LinkedIn, Twitter and Telegram channels."
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-12-09/index.html",
    "href": "posts/weekly-rtip-healthyrts-2022-12-09/index.html",
    "title": "Create QQ Plots for Time Series Models with {healthyR.ts}",
    "section": "",
    "text": "Function\nLet’s take a look at the full function call and the arguments that get provided to the parameters.\n\nts_qq_plot(\n  .calibration_tbl, \n  .model_id = NULL, \n  .interactive = FALSE\n  )\n\n\n.calibration_tbl - A calibrated modeltime table.\n.model_id - The id of a particular model from a calibration tibble. If there are multiple models in the tibble and this remains NULL then the plot will be returned using ggplot2::facet_grid(~ .model_id)\n.interactive - A boolean with a default value of FALSE. TRUE will produce an interactive plotly plot.\n\n\n\nExamples\nLet’s work through an example, and since we already spoke about ARMA let’s try out an ARMA model.\n\nlibrary(healthyR.ts)\nlibrary(modeltime)\nlibrary(timetk)\nlibrary(rsample)\nlibrary(workflows)\nlibrary(recipes)\nlibrary(parsnip)\n\ndata_tbl <- ts_to_tbl(AirPassengers) %>%\n  select(-index)\n\nsplits <- time_series_split(\n  data_tbl,\n  date_var = date_col,\n  assess = \"12 months\",\n  cumulative = TRUE\n)\n\nrec_obj <- recipe(value ~ ., training(splits))\n\nmodel_spec_arima <- arima_reg() %>%\n  set_engine(engine = \"auto_arima\")\n\nwflw_fit_arima <- workflow() %>%\n  add_recipe(rec_obj) %>%\n  add_model(model_spec_arima) %>%\n  fit(training(splits))\n\nmodel_tbl <- modeltime_table(wflw_fit_arima)\n\ncalibration_tbl <- model_tbl %>%\n  modeltime_calibrate(new_data = testing(splits))\n\nts_qq_plot(calibration_tbl, .interactive = TRUE)"
  },
  {
    "objectID": "posts/rtip-2022-12-13/index.html",
    "href": "posts/rtip-2022-12-13/index.html",
    "title": "Mixture Distributions with {TidyDensity}",
    "section": "",
    "text": "Function\nLet’s take a look a function in {TidyDensity} that allows us to do this. At this moment, weights are not a parameter to the function.\n\ntidy_mixture_density(...)\n\nNow let’s take a look at the arguments that get supplied to the ... parameter.\n\n... - The random data you want to pass. Example rnorm(50,0,1) or something like tidy_normal(.mean = 5, .sd = 1)\n\n\n\nExample\nLet’s take a look at an example.\n\nlibrary(TidyDensity)\n\noutput <- tidy_mixture_density(\n  rnorm(100, 0, 1), \n  tidy_normal(.mean = 5, .sd = 1)\n)\n\nAs you can see, you can enter a function that outputs a numeric vector or you can use a {TidyDensity} distribution function.\nLet’s take a look at the outputs.\n\noutput$data\n\n$dist_tbl\n# A tibble: 150 × 2\n       x       y\n   <int>   <dbl>\n 1     1  0.341 \n 2     2  1.02  \n 3     3  0.0727\n 4     4  0.166 \n 5     5  1.22  \n 6     6 -1.02  \n 7     7  0.510 \n 8     8 -0.440 \n 9     9  0.154 \n10    10 -1.71  \n# … with 140 more rows\n\n$dens_tbl\n# A tibble: 150 × 2\n       x         y\n   <dbl>     <dbl>\n 1 -5.10 0.0000452\n 2 -5.00 0.0000649\n 3 -4.91 0.0000926\n 4 -4.81 0.000130 \n 5 -4.71 0.000182 \n 6 -4.61 0.000251 \n 7 -4.52 0.000342 \n 8 -4.42 0.000463 \n 9 -4.32 0.000620 \n10 -4.23 0.000822 \n# … with 140 more rows\n\n$input_data\n$input_data$`rnorm(100, 0, 1)`\n  [1]  0.340537094  1.017947724  0.072677575  0.165989029  1.219143938\n  [6] -1.023473289  0.509929681 -0.440443183  0.153820008 -1.705894963\n [11] -1.597345196 -0.186384136 -2.594099308  0.233653364  0.811994363\n [16] -0.594942094  0.003107744 -0.889182158 -0.440905806  2.613419135\n [21]  0.853828238 -0.350536770  0.377864298  0.840972405 -0.132862546\n [26] -0.722874709  0.662916105  0.241055167  0.417758316 -0.638472585\n [31] -1.641039972 -0.920414204 -0.152141451 -0.437875952  0.085670806\n [36]  0.463827108 -0.522106822  0.500837004 -1.681585000 -0.154955758\n [41] -1.233628434 -0.725378152 -0.768707055 -2.070839826  2.306342549\n [46]  0.687025505 -1.081833687 -0.310171612 -1.709459599  0.995053299\n [51]  1.739132624 -0.108686350  0.916712737 -0.187255212  0.743396519\n [56] -1.141785566  1.671065850  1.953865678 -0.890899203 -1.537271940\n [61]  0.503394168  0.224776316  1.346257014  0.126784177  1.200079518\n [66] -1.453852152 -1.127011369  0.261459814 -0.002019175  1.640353320\n [71]  1.139002423  1.508466841  0.645805779 -0.322963918  1.340095220\n [76] -1.300832878  0.546650568 -0.558880636  1.951096245 -1.015339234\n [81]  1.069275018  0.133173860 -1.357403704  0.310231722 -0.784578245\n [86]  0.390688526 -0.719058316  0.593524764 -0.397886322 -0.126173974\n [91]  0.017003588 -0.266109359 -1.271686264 -0.288485067  0.932965997\n [96] -0.500822905 -0.879803183 -0.016200167  0.684898020 -0.536289497\n\n$input_data$`tidy_normal(.mean = 5, .sd = 1)`\n# A tibble: 50 × 7\n   sim_number     x     y    dx       dy       p     q\n   <fct>      <int> <dbl> <dbl>    <dbl>   <dbl> <dbl>\n 1 1              1  5.11  1.42 0.000273 0.545    5.11\n 2 1              2  4.38  1.55 0.000835 0.267    4.38\n 3 1              3  4.81  1.68 0.00217  0.425    4.81\n 4 1              4  2.40  1.81 0.00482  0.00470  2.40\n 5 1              5  5.34  1.94 0.00913  0.632    5.34\n 6 1              6  5.39  2.07 0.0147   0.651    5.39\n 7 1              7  5.41  2.20 0.0203   0.659    5.41\n 8 1              8  4.20  2.34 0.0240   0.212    4.20\n 9 1              9  6.13  2.47 0.0246   0.871    6.13\n10 1             10  5.58  2.60 0.0225   0.718    5.58\n# … with 40 more rows\n\n\nAnd now the visuals that come with it.\n\noutput$plots\n\n$line_plot\n\n\n\n\n\n\n$dens_plot\n\n\n\n\n\nThe function also lists the input functions as well.\n\noutput$input_fns\n\n[[1]]\nrnorm(100, 0, 1)\n\n[[2]]\ntidy_normal(.mean = 5, .sd = 1)\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-14/index.html",
    "href": "posts/rtip-2022-12-14/index.html",
    "title": "Distribution Summaries with {TidyDensity}",
    "section": "",
    "text": "Function\nLet’s take a look at the full function call.\n\ntidy_distribution_summary_tbl(.data, ...)\n\nHere are the arguments that go to the parameters.\n\n.data - The data that is going to be passed from a a tidy_ distribution function.\n... - This is the grouping variable that gets passed to dplyr::group_by() and dplyr::select().\n\n\n\nExample\nNow let’s go over a simple example.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\ntn <- tidy_normal(.num_sims = 5)\ntb <- tidy_beta(.num_sims = 5)\n\ntidy_distribution_summary_tbl(tn) |>\n  glimpse()\n\nRows: 1\nColumns: 12\n$ mean_val   <dbl> -0.01072756\n$ median_val <dbl> 0.01006799\n$ std_val    <dbl> 1.06121\n$ min_val    <dbl> -2.545341\n$ max_val    <dbl> 2.530692\n$ skewness   <dbl> 0.05476884\n$ kurtosis   <dbl> 2.606813\n$ range      <dbl> 5.076033\n$ iqr        <dbl> 1.504759\n$ variance   <dbl> 1.126167\n$ ci_low     <dbl> -1.973479\n$ ci_high    <dbl> 2.194386\n\ntidy_distribution_summary_tbl(tn, sim_number) |>\n  glimpse()\n\nRows: 5\nColumns: 13\n$ sim_number <fct> 1, 2, 3, 4, 5\n$ mean_val   <dbl> -0.32875968, -0.19670017, 0.05070292, 0.25380550, 0.16731364\n$ median_val <dbl> -0.36716805, -0.05945659, -0.01785947, 0.25904889, 0.063569…\n$ std_val    <dbl> 0.9452359, 1.0505691, 1.0977859, 1.0649735, 1.0678004\n$ min_val    <dbl> -2.083314, -1.994462, -2.340762, -2.145067, -2.545341\n$ max_val    <dbl> 1.513944, 2.205105, 2.505945, 2.301243, 2.530692\n$ skewness   <dbl> -0.08503104, 0.15111654, 0.07025537, -0.06130672, -0.031174…\n$ kurtosis   <dbl> 1.995540, 2.344348, 2.545780, 2.665472, 3.140276\n$ range      <dbl> 3.597258, 4.199567, 4.846706, 4.446311, 5.076033\n$ iqr        <dbl> 1.416398, 1.641004, 1.524895, 1.212874, 1.258469\n$ variance   <dbl> 0.8934709, 1.1036955, 1.2051339, 1.1341685, 1.1401977\n$ ci_low     <dbl> -1.904586, -1.968764, -1.923947, -1.784431, -1.936098\n$ ci_high    <dbl> 1.306136, 1.951288, 2.173104, 2.194386, 2.180618\n\ndata_tbl <- tidy_combine_distributions(tn, tb)\n\ntidy_distribution_summary_tbl(data_tbl) |>\n  glimpse()\n\nRows: 1\nColumns: 12\n$ mean_val   <dbl> 0.2524826\n$ median_val <dbl> 0.3910187\n$ std_val    <dbl> 0.8185376\n$ min_val    <dbl> -2.545341\n$ max_val    <dbl> 2.530692\n$ skewness   <dbl> -0.6970009\n$ kurtosis   <dbl> 4.168271\n$ range      <dbl> 5.076033\n$ iqr        <dbl> 0.7479198\n$ variance   <dbl> 0.6700038\n$ ci_low     <dbl> -1.82445\n$ ci_high    <dbl> 1.963619\n\ntidy_distribution_summary_tbl(data_tbl, dist_type) |>\n  glimpse()\n\nRows: 2\nColumns: 13\n$ dist_type  <fct> \"Gaussian c(0, 1)\", \"Beta c(1, 1, 0)\"\n$ mean_val   <dbl> -0.01072756, 0.51569281\n$ median_val <dbl> 0.01006799, 0.53941220\n$ std_val    <dbl> 1.0612103, 0.2782362\n$ min_val    <dbl> -2.545340827, 0.003264565\n$ max_val    <dbl> 2.5306917, 0.9861081\n$ skewness   <dbl> 0.05476884, -0.11482885\n$ kurtosis   <dbl> 2.606813, 1.824557\n$ range      <dbl> 5.0760325, 0.9828436\n$ iqr        <dbl> 1.5047591, 0.4771629\n$ variance   <dbl> 1.1261674, 0.0774154\n$ ci_low     <dbl> -1.97347940, 0.03494393\n$ ci_high    <dbl> 2.1943864, 0.9579465"
  },
  {
    "objectID": "posts/rtip-2022-12-15/index.html",
    "href": "posts/rtip-2022-12-15/index.html",
    "title": "Simple Moving Average Plots with {healthyR.ts}",
    "section": "",
    "text": "The R package {healthyR.ts}, is an R package that allows users to easily plot and analyze their time series data. The package includes a variety of functions, but one of the standout features is the ts_sma_plot() function, which allows users to quickly visualize their time series data and any number of simple moving averages (SMAs) of their choosing.\nSMAs are a common tool used by analysts and investors to smooth out short-term fluctuations in data and identify longer-term trends. By overlaying SMAs of different time periods on top of the original time series data, the ts_sma_plot() function makes it easy to compare and contrast different time periods and identify potential trends and patterns in the data.\nWith {healthyR.ts} and the ts_sma_plot() function, users can quickly and easily gain valuable insights into their time series data and make more informed decisions based on the trends and patterns they uncover.\nOk enough of that, let’s see the function."
  },
  {
    "objectID": "posts/rtip-2022-12-15/index.html#data",
    "href": "posts/rtip-2022-12-15/index.html#data",
    "title": "Simple Moving Average Plots with {healthyR.ts}",
    "section": "Data",
    "text": "Data\n\nout$data\n\n# A tibble: 288 × 5\n   index     date_col   value sma_order sma_value\n   <yearmon> <date>     <dbl> <fct>         <dbl>\n 1 Jan 1949  1949-01-01   112 3               NA \n 2 Feb 1949  1949-02-01   118 3              121.\n 3 Mar 1949  1949-03-01   132 3              126.\n 4 Apr 1949  1949-04-01   129 3              127.\n 5 May 1949  1949-05-01   121 3              128.\n 6 Jun 1949  1949-06-01   135 3              135.\n 7 Jul 1949  1949-07-01   148 3              144.\n 8 Aug 1949  1949-08-01   148 3              144 \n 9 Sep 1949  1949-09-01   136 3              134.\n10 Oct 1949  1949-10-01   119 3              120.\n# … with 278 more rows"
  },
  {
    "objectID": "posts/rtip-2022-12-15/index.html#plots",
    "href": "posts/rtip-2022-12-15/index.html#plots",
    "title": "Simple Moving Average Plots with {healthyR.ts}",
    "section": "Plots",
    "text": "Plots\n\nout$plots$static_plot\n\nWarning: Removed 7 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\nout$plots$interactive_plot\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-12-16/index.html",
    "href": "posts/weekly-rtip-healthyrts-2022-12-16/index.html",
    "title": "Model Scedacity Plots with {healthyR.ts}",
    "section": "",
    "text": "Function\nLet’s take a look at the full function call.\n\nts_scedacity_scatter_plot(\n  .calibration_tbl,\n  .model_id = NULL,\n  .interactive = FALSE\n)\n\nLet’s take a look at the arguments that get provided to the parameters.\n\n.calibration_tbl - A calibrated modeltime table.\n.model_id - The id of a particular model from a calibration tibble. If there are multiple models in the tibble and this remains NULL then the plot will be returned using ggplot2::facet_grid(~ .model_id)\n.interactive - A boolean with a default value of FALSE. TRUE will produce an interactive plotly plot.\n\n\n\nExample\nNow for an example.\n\nlibrary(healthyR.ts)\nlibrary(dplyr)\nlibrary(timetk)\nlibrary(modeltime)\nlibrary(rsample)\nlibrary(workflows)\nlibrary(parsnip)\nlibrary(recipes)\n\ndata_tbl <- ts_to_tbl(AirPassengers) %>%\n  select(-index)\n\nsplits <- time_series_split(\n  data_tbl,\n  date_var = date_col,\n  assess = \"12 months\",\n  cumulative = TRUE\n)\n\nrec_obj <- recipe(value ~ ., training(splits))\n\nmodel_spec_arima <- arima_reg() %>%\n  set_engine(engine = \"auto_arima\")\n\nmodel_spec_mars <- mars(mode = \"regression\") %>%\n  set_engine(\"earth\")\n\nwflw_fit_arima <- workflow() %>%\n  add_recipe(rec_obj) %>%\n  add_model(model_spec_arima) %>%\n  fit(training(splits))\n\nwflw_fit_mars <- workflow() %>%\n  add_recipe(rec_obj) %>%\n  add_model(model_spec_mars) %>%\n  fit(training(splits))\n\nmodel_tbl <- modeltime_table(wflw_fit_arima, wflw_fit_mars)\n\ncalibration_tbl <- model_tbl %>%\n  modeltime_calibrate(new_data = testing(splits))\n\nts_scedacity_scatter_plot(calibration_tbl)\n\n\n\n\nNow the interactive plot.\n\nts_scedacity_scatter_plot(calibration_tbl, .interactive = TRUE)\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-19/index.html",
    "href": "posts/rtip-2022-12-19/index.html",
    "title": "Viewing Different Versions of the Same Statistical Distribution with {TidyDensity}",
    "section": "",
    "text": "Introduction\nIn statistics, it is often useful to view different versions of the same statistical distribution. For example, when working with the normal distribution, it may be helpful to see how the distribution changes as the mean and standard deviation are varied.\nOne way to do this is by using the R library {TidyDensity}, which has a function called tidy_multi_single_dist(). This function allows a user to easily generate multiple versions of the same statistical distribution which can be plotted on the same graph, with each version representing a different combination of mean and standard deviation.\nTo use this function, the user simply needs to specify the distribution they want to plot (e.g. “normal”), the range of values for the mean and standard deviation, and the number of versions they want to plot. The function will then generate a plot showing the different versions of the distribution, with each version represented by a different color.\nThere are several reasons why it might be a good idea to view different versions of the same statistical distribution. For one, it can help the user understand how the shape of the distribution changes as the mean and standard deviation are varied. This can be particularly useful for distributions that have a wide range of possible values for the mean and standard deviation, such as the normal distribution.\nIn addition, viewing different versions of the same distribution can also help the user identify patterns and trends in the data. For example, the user may notice that the distribution becomes more spread out as the standard deviation increases, or that the distribution shifts to the left or right as the mean changes.\nOverall, the TidyDensity function tidy_multi_single_dist() is a useful tool for anyone interested in visualizing different versions of the same statistical distribution. Whether you are a student learning about statistics for the first time, or an experienced data scientist looking to better understand your data, this function can help you gain a deeper understanding of the underlying distribution and identify patterns and trends in your data.\n\n\nFunction\nLet’s take a look at the full function call.\n\ntidy_multi_single_dist(\n  .tidy_dist = NULL, \n  .param_list = list()\n  )\n\nNow let’s look at the arguments that go to the parameters.\n\n.tidy_dist - The type of tidy_ distribution that you want to run. You can only choose one.\n.param_list - This must be a list() object of the parameters that you want to pass through to the {TidyDensity} ‘tidy_’ distribution function.\n\n\n\nExample\nLet’s run through an example.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\ntn <-tidy_multi_single_dist(\n  .tidy_dist = \"tidy_normal\",\n  .param_list = list(\n    .n = 200,\n    .mean = c(-1, 0, 1),\n    .sd = 1,\n    .num_sims = 3\n  )\n)\n\nNow that we have generated the data, let’s take a look and see if these different distributions have indeed been created.\n\ntidy_distribution_summary_tbl(tn, dist_name) |>\n  select(dist_name, mean_val, std_val)\n\n# A tibble: 3 × 3\n  dist_name         mean_val std_val\n  <fct>                <dbl>   <dbl>\n1 Gaussian c(-1, 1)  -1.03     0.988\n2 Gaussian c(0, 1)    0.0136   1.01 \n3 Gaussian c(1, 1)    0.990    1.02 \n\n\nLook’s good there, now let’s visualize.\n\ntn %>%\n  tidy_multi_dist_autoplot()\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-20/index.html",
    "href": "posts/rtip-2022-12-20/index.html",
    "title": "Random Walks with {TidyDensity}",
    "section": "",
    "text": "Introduction\nRandom walks are a type of stochastic process that can be used to model the movement of a particle or system over time. At each time step, the position of the particle is updated based on a random step drawn from a given probability distribution. This process can be represented as a sequence of independent and identically distributed (i.i.d.) random variables, and the resulting path traced by the particle is known as a random walk.\nRandom walks have a wide range of applications, including modeling the movement of stock prices, animal migration, and the spread of infectious diseases. They are also a fundamental concept in probability and statistics, and have been studied extensively in the literature.\nThe {TidyDensity} package provides a convenient way to generate and visualize random walks using the tidy_random_walk() function. This function takes a probability distribution as an argument, and generates a random walk by sampling from this distribution at each time step. For example, to generate a random walk with normally distributed steps, we can use the tidy_normal() function as follows:\n\nlibrary(TidyDensity)\n\n# Generate a random walk with normally distributed steps\nrw <- tidy_random_walk(tidy_normal())\n\nThe resulting object rw is a tibble with the typical tidy_ distribution columns and one augmented column called random_walk_value. The columns that are output are:\n\nsim_number The current simulation number from the tidy_ distribution\nx (You can think of this as time t)\ny The randomly generated value.\ndx & dy The density estimates of y at x\np & q The probability and quantile values of y\nrandom_walk_value The random walk value generated from tidy_random_walk() (You can think of this as the position of the particle at time t or the x)\n\nTo visualize the random walk, we can use the tidy_random_walk_autoplot() function, which creates a ggplot object showing the position of the particle at each time step. For example:\n\n# Visualize the random walk\ntidy_random_walk_autoplot(rw)\n\nThis will produce a plot showing the trajectory of the particle over time. You can customize the appearance of the plot by passing additional arguments to the tidy_random_walk_autoplot() function, such as the geom argument to specify the type of plot to use (e.g. geom = “line” for a line plot, or geom = “point” for a scatter plot).\nIn summary, the {TidyDensity} package provides a convenient and user-friendly interface for generating and visualizing random walks. With the tidy_random_walk() and tidy_random_walk_autoplot() functions, you can easily explore the behavior of random walks and their applications in a wide range of contexts.\nLet’s take a look at these functions.\n\n\nFunction\nFirstly we will look at the tidy_random_walk() function.\n\ntidy_random_walk(\n  .data,\n  .initial_value = 0,\n  .sample = FALSE,\n  .replace = FALSE,\n  .value_type = \"cum_prod\"\n)\n\nHere are the arguments that get provided to the parameters of this function.\n\n.data - The data that is being passed from a tidy_ distribution function.\n.initial_value - The default is 0, this can be set to whatever you want.\n.sample - This is a boolean value TRUE/FALSE. The default is FALSE. If set to TRUE then the y value from the tidy_ distribution function is sampled.\n.replace - This is a boolean value TRUE/FALSE. The default is FALSE. If set to TRUE AND .sample is set to TRUE then the replace parameter of the sample function will be set to TRUE.\n.value_type - This can take one of three different values for now. These are the following:\n\n“cum_prod” - This will take the cumprod of y\n“cum_sum” - This will take the cumsum of y\n\n\nNow let’s do the same with the tidy_random_walk_autoplot() function.\n\ntidy_random_walk_autoplot(\n  .data,\n  .line_size = 1,\n  .geom_rug = FALSE,\n  .geom_smooth = FALSE,\n  .interactive = FALSE\n)\n\nHere are the arguments that get provided to the parameters.\n\n.data - The data passed in from a tidy_distribution function like tidy_normal()\n.line_size - The size param ggplot\n.geom_rug - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return the use of ggplot2::geom_rug()\n.geom_smooth - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return the use of ggplot2::geom_smooth() The aes parameter of group is set to FALSE. This ensures a single smoothing band returned with SE also set to FALSE. Color is set to ‘black’ and linetype is ‘dashed’.\n.interactive - A Boolean value of TRUE/FALSE, FALSE is the default. TRUE will return an interactive plotly plot.\n\n\n\nExamples\nLet’s go over some examples.\n\nlibrary(TidyDensity)\n\ndist_data <- tidy_normal(.sd = .1, .num_sims = 5)\n\ntidy_random_walk(.data = dist_data, .value_type = \"cum_sum\") %>%\n  tidy_random_walk_autoplot()\n\n\n\n\nAnd another.\n\ntidy_normal(.sd = .1, .num_sims = 20) %>%\n  tidy_random_walk(\n    .value_type = \"cum_sum\", \n    .sample = TRUE, \n    .replace = TRUE\n    ) %>%\n  tidy_random_walk_autoplot()\n\n\n\n\nNow let’s get an interactive one.\n\ntidy_normal(.sd = .1, .num_sims = 20) %>%\n  tidy_random_walk(\n    .value_type = \"cum_sum\", \n    .sample = TRUE, \n    .replace = TRUE\n    ) %>%\n  tidy_random_walk_autoplot(.interactive = TRUE)\n\n\n\n\n\nOne last example, let’s use a different distribution. Let’s use a cauchy distribution.\n\ntidy_cauchy(.num_sims = 9, .location = .5) %>%\n  tidy_random_walk(.value_type = \"cum_sum\", .sample = TRUE) %>%\n  tidy_random_walk_autoplot()\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-21/index.html",
    "href": "posts/rtip-2022-12-21/index.html",
    "title": "Distribution Statistics with {TidyDensity}",
    "section": "",
    "text": "Introduction\nIf you’re working with statistical distributions in R, you may be interested in the {TidyDensity} package. This package provides a set of functions for creating, manipulating, and visualizing probability distributions in a tidy format. One of these functions is tidy_chisquare(), which allows you to create a chi-square distribution with a specified number of degrees of freedom and a non-centrality parameter.\nOnce you’ve created a chi-square distribution using tidy_chisquare(), you may want to get some summary statistics about the distribution. This is where the util_chisquare_stats_tbl() function comes in handy. This function takes a chi-square distribution (created with tidy_chisquare()) as input and returns a tibble with several statistics about the distribution.\nSome of the statistics included in the table are:\n\nMean: The mean of the chi-square distribution, also known as the expected value.\nVariance: The variance of the chi-square distribution, which is a measure of how spread out the data is.\nSkewness: The skewness of the chi-square distribution, which is a measure of the symmetry of the data.\nKurtosis: The kurtosis of the chi-square distribution, which is a measure of the peakedness of the data.\n\nTo use the util_chisquare_stats_tbl() function, you’ll need to install and load the {TidyDensity} package first. Then, you can create a chi-square distribution using tidy_chisquare() and pass it to util_chisquare_stats_tbl() like this:\n\n# install and load TidyDensity\ninstall.packages(\"TidyDensity\")\nlibrary(TidyDensity)\nlibrary(dplyr)\n\n# create a chi-square distribution with 5 degrees of freedom\ndistribution <- tidy_chisquare(.df = 5)\n\n# get statistics about the distribution\nutil_chisquare_stats_tbl(distribution) |>\n  glimpse()\n\nThe output will be a table with the mean, variance, skewness, and kurtosis of the chi-square distribution. These statistics can be useful for understanding the characteristics of the distribution and making statistical inferences.\nOverall, the {TidyDensity} package is a useful tool for working with statistical distributions in R. The util_chisquare_stats_tbl() function is just one of many functions available in the package that can help you analyze and understand your data. Give it a try and see how it can help with your statistical analysis!\n\n\nFunction\nLet’s take a look at the full function call.\n\nutil_chisquare_stats_tbl(.data)\n\nLet’s take a look at the arguments that get supplied to the function parameters.\n\n.data - The data being passed from a tidy_ distribution function.\n\n\n\nExample\nNow for a full example with output.\n\nlibrary(TidyDensity)\nlibrary(dplyr)\n\ntidy_chisquare() %>%\n  util_chisquare_stats_tbl() %>%\n  glimpse()\n\nRows: 1\nColumns: 17\n$ tidy_function     <chr> \"tidy_chisquare\"\n$ function_call     <chr> \"Chisquare c(1, 1)\"\n$ distribution      <chr> \"Chisquare\"\n$ distribution_type <chr> \"continuous\"\n$ points            <dbl> 50\n$ simulations       <dbl> 1\n$ mean              <dbl> 1\n$ median            <dbl> 0.3333333\n$ mode              <chr> \"undefined\"\n$ std_dv            <dbl> 1.414214\n$ coeff_var         <dbl> 1.414214\n$ skewness          <dbl> 2.828427\n$ kurtosis          <dbl> 15\n$ computed_std_skew <dbl> 1.132669\n$ computed_std_kurt <dbl> 3.894553\n$ ci_lo             <dbl> 0.002189912\n$ ci_hi             <dbl> 6.521727\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-22/index.html",
    "href": "posts/rtip-2022-12-22/index.html",
    "title": "Listing Functions and Parameters",
    "section": "",
    "text": "Introduction\nI got a little bored one day and decided I wanted to list out all of the functions inside of a package along with their parameters in a tibble. Not sure if this serves any particular purpose or not, I was just bored.\nThis does not work for packages that have data as an export like {healthyR} or {healthyR.data} but it will work for packages like {TidyDensity}.\nLet’s run through it\n\n\nExamples\nHere we go.\n\nlibrary(TidyDensity)\n\ndplyr::tibble(fns = ls(paste0(\"package:TidyDensity\"))) |>\n  dplyr::group_by(fns) |>\n  dplyr::mutate(params = purrr::map(fns, formalArgs) |>\n           toString()) |>\n  dplyr::mutate(func_with_params = stringr::str_remove(params, \"c\")) |>\n  dplyr::mutate(func_with_params = ifelse(\n    stringr::str_detect(\n      func_with_params, \"\\\\(\"), \n      paste0(fns, func_with_params), \n      paste0(fns, \"(\", func_with_params, \")\")\n    )) |>\n  dplyr::ungroup() |>\n  knitr::kable()\n\n\n\n\n\n\n\n\n\nfns\nparams\nfunc_with_params\n\n\n\n\n%>%\nc(“lhs”, “rhs”)\n%>%(“lhs”, “rhs”)\n\n\n:=\nc(“x”, “y”)\n:=(“x”, “y”)\n\n\nas_label\nx\nas_label(x)\n\n\nas_name\nx\nas_name(x)\n\n\nbootstrap_density_augment\n.data\nbootstrap_density_augment(.data)\n\n\nbootstrap_p_augment\nc(“.data”, “.value”, “.names”)\nbootstrap_p_augment(“.data”, “.value”, “.names”)\n\n\nbootstrap_p_vec\n.x\nbootstrap_p_vec(.x)\n\n\nbootstrap_q_augment\nc(“.data”, “.value”, “.names”)\nbootstrap_q_augment(“.data”, “.value”, “.names”)\n\n\nbootstrap_q_vec\n.x\nbootstrap_q_vec(.x)\n\n\nbootstrap_stat_plot\nc(“.data”, “.value”, “.stat”, “.show_groups”, “.show_ci_labels”, “.interactive”)\nbootstrap_stat_plot(“.data”, “.value”, “.stat”, “.show_groups”, “.show_ci_labels”, “.interactive”)\n\n\nbootstrap_unnest_tbl\n.data\nbootstrap_unnest_tbl(.data)\n\n\ncgmean\n.x\ncgmean(.x)\n\n\nchmean\n.x\nchmean(.x)\n\n\nci_hi\nc(“.x”, “.na_rm”)\nci_hi(“.x”, “.na_rm”)\n\n\nci_lo\nc(“.x”, “.na_rm”)\nci_lo(“.x”, “.na_rm”)\n\n\nckurtosis\n.x\nckurtosis(.x)\n\n\ncmean\n.x\ncmean(.x)\n\n\ncmedian\n.x\ncmedian(.x)\n\n\ncolor_blind\nNULL\ncolor_blind(NULL)\n\n\ncsd\n.x\ncsd(.x)\n\n\ncskewness\n.x\ncskewness(.x)\n\n\ncvar\n.x\ncvar(.x)\n\n\ndist_type_extractor\n.x\ndist_type_extractor(.x)\n\n\nenquo\narg\nenquo(arg)\n\n\nenquos\nc(“…”, “.named”, “.ignore_empty”, “.unquote_names”, “.homonyms”, “.check_assign”)\nenquos(“…”, “.named”, “.ignore_empty”, “.unquote_names”, “.homonyms”, “.check_assign”)\n\n\ntd_scale_color_colorblind\nc(“…”, “theme”)\ntd_scale_color_colorblind(“…”, “theme”)\n\n\ntd_scale_fill_colorblind\nc(“…”, “theme”)\ntd_scale_fill_colorblind(“…”, “theme”)\n\n\ntidy_autoplot\nc(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\ntidy_autoplot(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\n\n\ntidy_bernoulli\nc(“.n”, “.prob”, “.num_sims”)\ntidy_bernoulli(“.n”, “.prob”, “.num_sims”)\n\n\ntidy_beta\nc(“.n”, “.shape1”, “.shape2”, “.ncp”, “.num_sims”)\ntidy_beta(“.n”, “.shape1”, “.shape2”, “.ncp”, “.num_sims”)\n\n\ntidy_binomial\nc(“.n”, “.size”, “.prob”, “.num_sims”)\ntidy_binomial(“.n”, “.size”, “.prob”, “.num_sims”)\n\n\ntidy_bootstrap\nc(“.x”, “.num_sims”, “.proportion”, “.distribution_type”)\ntidy_bootstrap(“.x”, “.num_sims”, “.proportion”, “.distribution_type”)\n\n\ntidy_burr\nc(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\ntidy_burr(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_cauchy\nc(“.n”, “.location”, “.scale”, “.num_sims”)\ntidy_cauchy(“.n”, “.location”, “.scale”, “.num_sims”)\n\n\ntidy_chisquare\nc(“.n”, “.df”, “.ncp”, “.num_sims”)\ntidy_chisquare(“.n”, “.df”, “.ncp”, “.num_sims”)\n\n\ntidy_combine_distributions\n…\ntidy_combine_distributions(…)\n\n\ntidy_combined_autoplot\nc(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\ntidy_combined_autoplot(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\n\n\ntidy_distribution_comparison\nc(“.x”, “.distribution_type”)\ntidy_distribution_comparison(“.x”, “.distribution_type”)\n\n\ntidy_distribution_summary_tbl\nc(“.data”, “…”)\ntidy_distribution_summary_tbl(“.data”, “…”)\n\n\ntidy_empirical\nc(“.x”, “.num_sims”, “.distribution_type”)\ntidy_empirical(“.x”, “.num_sims”, “.distribution_type”)\n\n\ntidy_exponential\nc(“.n”, “.rate”, “.num_sims”)\ntidy_exponential(“.n”, “.rate”, “.num_sims”)\n\n\ntidy_f\nc(“.n”, “.df1”, “.df2”, “.ncp”, “.num_sims”)\ntidy_f(“.n”, “.df1”, “.df2”, “.ncp”, “.num_sims”)\n\n\ntidy_four_autoplot\nc(“.data”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\ntidy_four_autoplot(“.data”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\n\n\ntidy_gamma\nc(“.n”, “.shape”, “.scale”, “.num_sims”)\ntidy_gamma(“.n”, “.shape”, “.scale”, “.num_sims”)\n\n\ntidy_generalized_beta\nc(“.n”, “.shape1”, “.shape2”, “.shape3”, “.rate”, “.scale”, “.num_sims”)\ntidy_generalized_beta(“.n”, “.shape1”, “.shape2”, “.shape3”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_generalized_pareto\nc(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\ntidy_generalized_pareto(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_geometric\nc(“.n”, “.prob”, “.num_sims”)\ntidy_geometric(“.n”, “.prob”, “.num_sims”)\n\n\ntidy_hypergeometric\nc(“.n”, “.m”, “.nn”, “.k”, “.num_sims”)\ntidy_hypergeometric(“.n”, “.m”, “.nn”, “.k”, “.num_sims”)\n\n\ntidy_inverse_burr\nc(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\ntidy_inverse_burr(“.n”, “.shape1”, “.shape2”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_inverse_exponential\nc(“.n”, “.rate”, “.scale”, “.num_sims”)\ntidy_inverse_exponential(“.n”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_inverse_gamma\nc(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\ntidy_inverse_gamma(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_inverse_normal\nc(“.n”, “.mean”, “.shape”, “.dispersion”, “.num_sims”)\ntidy_inverse_normal(“.n”, “.mean”, “.shape”, “.dispersion”, “.num_sims”)\n\n\ntidy_inverse_pareto\nc(“.n”, “.shape”, “.scale”, “.num_sims”)\ntidy_inverse_pareto(“.n”, “.shape”, “.scale”, “.num_sims”)\n\n\ntidy_inverse_weibull\nc(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\ntidy_inverse_weibull(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_kurtosis_vec\n.x\ntidy_kurtosis_vec(.x)\n\n\ntidy_logistic\nc(“.n”, “.location”, “.scale”, “.num_sims”)\ntidy_logistic(“.n”, “.location”, “.scale”, “.num_sims”)\n\n\ntidy_lognormal\nc(“.n”, “.meanlog”, “.sdlog”, “.num_sims”)\ntidy_lognormal(“.n”, “.meanlog”, “.sdlog”, “.num_sims”)\n\n\ntidy_mixture_density\n…\ntidy_mixture_density(…)\n\n\ntidy_multi_dist_autoplot\nc(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\ntidy_multi_dist_autoplot(“.data”, “.plot_type”, “.line_size”, “.geom_point”, “.point_size”, “.geom_rug”, “.geom_smooth”, “.geom_jitter”, “.interactive”)\n\n\ntidy_multi_single_dist\nc(“.tidy_dist”, “.param_list”)\ntidy_multi_single_dist(“.tidy_dist”, “.param_list”)\n\n\ntidy_negative_binomial\nc(“.n”, “.size”, “.prob”, “.num_sims”)\ntidy_negative_binomial(“.n”, “.size”, “.prob”, “.num_sims”)\n\n\ntidy_normal\nc(“.n”, “.mean”, “.sd”, “.num_sims”)\ntidy_normal(“.n”, “.mean”, “.sd”, “.num_sims”)\n\n\ntidy_paralogistic\nc(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\ntidy_paralogistic(“.n”, “.shape”, “.rate”, “.scale”, “.num_sims”)\n\n\ntidy_pareto\nc(“.n”, “.shape”, “.scale”, “.num_sims”)\ntidy_pareto(“.n”, “.shape”, “.scale”, “.num_sims”)\n\n\ntidy_pareto1\nc(“.n”, “.shape”, “.min”, “.num_sims”)\ntidy_pareto1(“.n”, “.shape”, “.min”, “.num_sims”)\n\n\ntidy_poisson\nc(“.n”, “.lambda”, “.num_sims”)\ntidy_poisson(“.n”, “.lambda”, “.num_sims”)\n\n\ntidy_random_walk\nc(“.data”, “.initial_value”, “.sample”, “.replace”, “.value_type”)\ntidy_random_walk(“.data”, “.initial_value”, “.sample”, “.replace”, “.value_type”)\n\n\ntidy_random_walk_autoplot\nc(“.data”, “.line_size”, “.geom_rug”, “.geom_smooth”, “.interactive”)\ntidy_random_walk_autoplot(“.data”, “.line_size”, “.geom_rug”, “.geom_smooth”, “.interactive”)\n\n\ntidy_range_statistic\n.x\ntidy_range_statistic(.x)\n\n\ntidy_scale_zero_one_vec\n.x\ntidy_scale_zero_one_vec(.x)\n\n\ntidy_skewness_vec\n.x\ntidy_skewness_vec(.x)\n\n\ntidy_stat_tbl\nc(“.data”, “.x”, “.fns”, “.return_type”, “.use_data_table”, “…”)\ntidy_stat_tbl(“.data”, “.x”, “.fns”, “.return_type”, “.use_data_table”, “…”)\n\n\ntidy_t\nc(“.n”, “.df”, “.ncp”, “.num_sims”)\ntidy_t(“.n”, “.df”, “.ncp”, “.num_sims”)\n\n\ntidy_uniform\nc(“.n”, “.min”, “.max”, “.num_sims”)\ntidy_uniform(“.n”, “.min”, “.max”, “.num_sims”)\n\n\ntidy_weibull\nc(“.n”, “.shape”, “.scale”, “.num_sims”)\ntidy_weibull(“.n”, “.shape”, “.scale”, “.num_sims”)\n\n\ntidy_zero_truncated_binomial\nc(“.n”, “.size”, “.prob”, “.num_sims”)\ntidy_zero_truncated_binomial(“.n”, “.size”, “.prob”, “.num_sims”)\n\n\ntidy_zero_truncated_geometric\nc(“.n”, “.prob”, “.num_sims”)\ntidy_zero_truncated_geometric(“.n”, “.prob”, “.num_sims”)\n\n\ntidy_zero_truncated_negative_binomial\nc(“.n”, “.size”, “.prob”, “.num_sims”)\ntidy_zero_truncated_negative_binomial(“.n”, “.size”, “.prob”, “.num_sims”)\n\n\ntidy_zero_truncated_poisson\nc(“.n”, “.lambda”, “.num_sims”)\ntidy_zero_truncated_poisson(“.n”, “.lambda”, “.num_sims”)\n\n\nutil_bernoulli_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_bernoulli_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_bernoulli_stats_tbl\n.data\nutil_bernoulli_stats_tbl(.data)\n\n\nutil_beta_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_beta_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_beta_stats_tbl\n.data\nutil_beta_stats_tbl(.data)\n\n\nutil_binomial_param_estimate\nc(“.x”, “.size”, “.auto_gen_empirical”)\nutil_binomial_param_estimate(“.x”, “.size”, “.auto_gen_empirical”)\n\n\nutil_binomial_stats_tbl\n.data\nutil_binomial_stats_tbl(.data)\n\n\nutil_cauchy_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_cauchy_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_cauchy_stats_tbl\n.data\nutil_cauchy_stats_tbl(.data)\n\n\nutil_chisquare_stats_tbl\n.data\nutil_chisquare_stats_tbl(.data)\n\n\nutil_exponential_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_exponential_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_exponential_stats_tbl\n.data\nutil_exponential_stats_tbl(.data)\n\n\nutil_f_stats_tbl\n.data\nutil_f_stats_tbl(.data)\n\n\nutil_gamma_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_gamma_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_gamma_stats_tbl\n.data\nutil_gamma_stats_tbl(.data)\n\n\nutil_geometric_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_geometric_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_geometric_stats_tbl\n.data\nutil_geometric_stats_tbl(.data)\n\n\nutil_hypergeometric_param_estimate\nc(“.x”, “.m”, “.total”, “.k”, “.auto_gen_empirical”)\nutil_hypergeometric_param_estimate(“.x”, “.m”, “.total”, “.k”, “.auto_gen_empirical”)\n\n\nutil_hypergeometric_stats_tbl\n.data\nutil_hypergeometric_stats_tbl(.data)\n\n\nutil_logistic_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_logistic_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_logistic_stats_tbl\n.data\nutil_logistic_stats_tbl(.data)\n\n\nutil_lognormal_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_lognormal_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_lognormal_stats_tbl\n.data\nutil_lognormal_stats_tbl(.data)\n\n\nutil_negative_binomial_param_estimate\nc(“.x”, “.size”, “.auto_gen_empirical”)\nutil_negative_binomial_param_estimate(“.x”, “.size”, “.auto_gen_empirical”)\n\n\nutil_negative_binomial_stats_tbl\n.data\nutil_negative_binomial_stats_tbl(.data)\n\n\nutil_normal_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_normal_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_normal_stats_tbl\n.data\nutil_normal_stats_tbl(.data)\n\n\nutil_pareto_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_pareto_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_pareto_stats_tbl\n.data\nutil_pareto_stats_tbl(.data)\n\n\nutil_poisson_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_poisson_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_poisson_stats_tbl\n.data\nutil_poisson_stats_tbl(.data)\n\n\nutil_t_stats_tbl\n.data\nutil_t_stats_tbl(.data)\n\n\nutil_uniform_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_uniform_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_uniform_stats_tbl\n.data\nutil_uniform_stats_tbl(.data)\n\n\nutil_weibull_param_estimate\nc(“.x”, “.auto_gen_empirical”)\nutil_weibull_param_estimate(“.x”, “.auto_gen_empirical”)\n\n\nutil_weibull_stats_tbl\n.data\nutil_weibull_stats_tbl(.data)\n\n\n\n\n\nAnother example.\n\nlibrary(healthyverse)\n\nlibrary(TidyDensity)\n\ndplyr::tibble(fns = ls(paste0(\"package:healthyverse\"))) |>\n  dplyr::group_by(fns) |>\n  dplyr::mutate(params = purrr::map(fns, formalArgs) |>\n           toString()) |>\n  dplyr::mutate(func_with_params = stringr::str_remove(params, \"c\")) |>\n  dplyr::mutate(func_with_params = ifelse(\n    stringr::str_detect(\n      func_with_params, \"\\\\(\"), \n      paste0(fns, func_with_params), \n      paste0(fns, \"(\", func_with_params, \")\")\n    )) |>\n  dplyr::ungroup() |>\n  knitr::kable()\n\n\n\n\n\n\n\n\n\nfns\nparams\nfunc_with_params\n\n\n\n\n%>%\nc(“lhs”, “rhs”)\n%>%(“lhs”, “rhs”)\n\n\n:=\nc(“x”, “y”)\n:=(“x”, “y”)\n\n\nas_label\nx\nas_label(x)\n\n\nas_name\nx\nas_name(x)\n\n\nenquo\narg\nenquo(arg)\n\n\nenquos\nc(“…”, “.named”, “.ignore_empty”, “.unquote_names”, “.homonyms”, “.check_assign”)\nenquos(“…”, “.named”, “.ignore_empty”, “.unquote_names”, “.homonyms”, “.check_assign”)\n\n\nexpr\nexpr\nexpr(expr)\n\n\nhealthyverse_conflicts\nNULL\nhealthyverse_conflicts(NULL)\n\n\nhealthyverse_deps\nc(“recursive”, “repos”)\nhealthyverse_deps(“recursive”, “repos”)\n\n\nhealthyverse_packages\ninclude_self\nhealthyverse_packages(inlude_self)\n\n\nhealthyverse_sitrep\nNULL\nhealthyverse_sitrep(NULL)\n\n\nhealthyverse_update\nc(“recursive”, “repos”)\nhealthyverse_update(“recursive”, “repos”)\n\n\nsym\nx\nsym(x)\n\n\nsyms\nx\nsyms(x)\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/weekly-rtip-2022-12-23/index.html",
    "href": "posts/weekly-rtip-2022-12-23/index.html",
    "title": "Simulating Time Series Model Forecasts with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nTime series models are a powerful tool for forecasting future values of a time-dependent variable. These models are commonly used in a variety of fields, including finance, economics, and engineering, to predict future outcomes based on past data.\nOne important aspect of time series modeling is the ability to simulate model forecasts. This allows us to evaluate the performance of different forecasting methods and to compare the results of different models. Simulating forecasts also allows us to assess the uncertainty associated with our predictions, which can be especially useful when making important decisions based on the forecast.\nThere are several benefits to simulating time series model forecasts:\n\nImproved accuracy: By simulating forecasts, we can identify the best forecasting method for a given time series and optimize its parameters. This can lead to more accurate forecasts, especially for long-term predictions.\nEnhanced understanding: Simulating forecasts helps us to understand how different factors, such as seasonality and trend, affect the prediction. This understanding can inform our decision-making and allow us to make more informed predictions.\nImproved communication: Simulating forecasts allows us to present the uncertainty associated with our predictions, which can be useful for communicating the potential risks and benefits of different courses of action.\n\nThe R package {healthyR.ts} includes a function called ts_forecast_simulator() that can be used to simulate time series model forecasts. This function allows users to specify the forecasting method, the number of simulations to run, and the length of the forecast horizon. It also provides options for visualizing the results, including plots of the forecast distribution and summary statistics such as the mean and standard deviation of the forecasts.\nIn summary, simulating time series model forecasts is a valuable tool for improving the accuracy and understanding of our predictions, as well as for communicating the uncertainty associated with these forecasts. The ts_forecast_simulator() function in the {healthyR.ts} package is a useful tool for performing these simulations in R.\n\n\nFunction\nLet’s take a look at the full function call.\n\nts_forecast_simulator(\n  .model,\n  .data,\n  .ext_reg = NULL,\n  .frequency = NULL,\n  .bootstrap = TRUE,\n  .horizon = 4,\n  .iterations = 25,\n  .sim_color = \"steelblue\",\n  .alpha = 0.05\n)\n\nNow let’s take a look at the arguments that get provided to the parameters.\n\n.model - A forecasting model of one of the following from the forecast package:\n\nArima\nauto.arima\nets\nnnetar\nArima() with xreg\n\n.data - The data that is used for the .model parameter. This is used with timetk::tk_index()\n.ext_reg - A tibble or matrix of future xregs that should be the same length as the horizon you want to forecast.\n.frequency - This is for the conversion of an internal table and should match the time frequency of the data.\n.bootstrap - A boolean value of TRUE/FALSE. From forecast::simulate.Arima() Do simulation using resampled errors rather than normally distributed errors.\n.horizon - An integer defining the forecast horizon.\n.iterations - An integer, set the number of iterations of the simulation.\n.sim_color - Set the color of the simulation paths lines.\n.alpha - Set the opacity level of the simulation path lines.\n\nGreat, now let’s take a look at examples.\n\n\nExamples\n\nlibrary(healthyR.ts)\nlibrary(forecast)\n\nfit <- auto.arima(AirPassengers)\ndata_tbl <- ts_to_tbl(AirPassengers)\n\n# Simulate 50 possible forecast paths, with .horizon of 12 months\noutput <- ts_forecast_simulator(\n  .model        = fit\n  , .horizon    = 12\n  , .iterations = 50\n  , .data       = data_tbl\n)\n\nOk, so now we have our output object, which is a list object. Let’s see what it contains.\n\n\n\nForecast Simulation Output\n\n\nNow, let’s explore each element.\nFirst the forecast simulation data.\n\noutput$forecast_sim\n\n            sim_1    sim_2    sim_3    sim_4    sim_5    sim_6    sim_7\nJan 1961 444.0783 464.6459 435.1001 443.3926 460.1770 455.5960 443.3926\nFeb 1961 410.1508 428.1348 410.7124 417.7103 448.2808 438.3287 414.0853\nMar 1961 433.4585 466.1261 438.2495 436.5187 462.0361 455.9610 438.5567\nApr 1961 477.3621 496.4721 475.0898 500.9698 491.2944 514.5328 494.7475\nMay 1961 498.8577 511.1907 492.8757 525.1364 509.3054 516.3687 498.7613\nJun 1961 559.1105 591.6428 549.4045 570.5291 560.1250 590.8551 563.2961\nJul 1961 659.4558 669.9017 627.6346 658.7581 670.8004 685.0800 652.6926\nAug 1961 637.3978 637.2279 640.9244 640.6556 648.1910 666.1584 648.9248\nSep 1961 539.7950 542.1781 536.1719 562.2053 551.0824 561.6655 565.8203\nOct 1961 510.0380 496.5867 493.1495 503.8047 501.3881 528.2415 522.0221\nNov 1961 431.4060 403.1573 418.7376 441.9347 433.3918 435.8772 443.0993\nDec 1961 471.0064 454.4918 470.1169 462.5856 477.5090 470.3985 481.6106\n            sim_8    sim_9   sim_10   sim_11   sim_12   sim_13   sim_14\nJan 1961 444.2746 426.0994 441.0622 434.7175 439.1804 464.6459 442.9633\nFeb 1961 432.3743 403.4654 415.4007 412.4137 397.1687 451.0807 426.6916\nMar 1961 445.9289 426.9240 436.0152 436.9728 422.5894 491.2045 459.9356\nApr 1961 483.3744 465.2526 475.1208 495.3483 462.4882 515.1628 498.4882\nMay 1961 480.8858 482.5294 502.0741 506.8488 493.2425 510.5408 534.5532\nJun 1961 561.5578 561.8430 565.9861 579.4113 552.6567 580.3826 586.1801\nJul 1961 654.8122 659.1271 654.4413 654.0360 640.4989 675.0941 658.4366\nAug 1961 649.5836 640.1375 637.0663 639.7284 639.7749 650.8844 647.8402\nSep 1961 548.1653 551.4731 538.5364 539.7313 550.0647 550.5820 557.4349\nOct 1961 498.1929 499.2556 490.0446 491.4540 499.1013 496.7938 511.1965\nNov 1961 444.3749 425.9994 418.8421 410.5625 430.4175 419.9848 441.4195\nDec 1961 475.2648 479.8478 472.3391 460.1686 469.3343 475.1452 478.3377\n           sim_15   sim_16   sim_17   sim_18   sim_19   sim_20   sim_21\nJan 1961 444.2156 444.3580 454.4293 409.2796 444.2885 437.0193 446.4539\nFeb 1961 430.1387 410.9953 424.5369 415.5862 416.8146 413.2215 419.0774\nMar 1961 455.6870 452.8860 457.6583 441.3237 457.7849 440.5469 439.2507\nApr 1961 484.5390 475.4026 462.6041 486.5865 495.6696 468.2959 474.9072\nMay 1961 494.1008 487.8137 488.4780 491.8505 510.9093 484.1188 482.9407\nJun 1961 557.3412 550.9653 561.9112 557.1223 555.7177 544.8112 542.9244\nJul 1961 632.6491 642.1085 656.7757 636.9144 635.3725 600.9389 633.5334\nAug 1961 645.9216 617.8916 648.8207 616.9876 642.0941 613.5926 632.3239\nSep 1961 539.2447 508.7016 537.3101 537.4114 553.7588 529.0607 549.2177\nOct 1961 487.1370 468.3302 496.5455 478.2974 506.4618 490.5820 488.3003\nNov 1961 421.7249 380.5834 415.3303 412.4914 420.2843 418.5592 423.7430\nDec 1961 459.3009 432.4970 458.2413 451.1740 465.7803 480.2968 448.5086\n           sim_22   sim_23   sim_24   sim_25   sim_26   sim_27   sim_28\nJan 1961 447.2302 456.2503 436.7128 431.5729 445.7399 448.9127 449.6957\nFeb 1961 420.0557 437.8165 401.4785 410.4397 409.6106 429.4429 425.3145\nMar 1961 440.4207 453.1414 443.5447 438.7145 438.0261 460.7891 477.8071\nApr 1961 494.3475 484.4190 487.4335 485.5828 490.5304 510.9307 513.7889\nMay 1961 492.5874 489.2580 497.9143 508.2445 480.8237 513.7845 531.1984\nJun 1961 563.8807 556.0438 564.1188 561.7625 550.4049 580.6604 577.2518\nJul 1961 642.5184 651.5929 661.2144 650.6643 678.5399 663.2760 660.3744\nAug 1961 632.2756 636.9294 645.8384 624.7512 648.4739 654.3770 634.0568\nSep 1961 526.7933 541.5644 545.4205 523.9379 561.8836 551.4157 550.9190\nOct 1961 482.6396 505.3507 501.6859 468.8800 496.0365 502.9716 487.9256\nNov 1961 424.9468 422.8521 438.5865 402.6399 426.8928 450.1945 436.9170\nDec 1961 449.7488 465.7964 495.3195 445.6892 467.5324 488.8905 490.3376\n           sim_29   sim_30   sim_31   sim_32   sim_33   sim_34   sim_35\nJan 1961 456.2503 432.1975 447.9726 446.1336 438.0253 437.0193 444.2156\nFeb 1961 416.0651 405.8724 386.4568 434.5503 414.4022 407.4945 429.2142\nMar 1961 447.5735 451.8487 417.2209 454.7883 434.2226 422.7515 452.9812\nApr 1961 495.3393 488.7947 456.0453 508.1489 474.7016 463.4303 460.7364\nMay 1961 518.2500 512.6435 491.8027 513.4317 504.4126 466.5950 493.9472\nJun 1961 576.0651 549.5581 545.0426 577.5135 580.5937 524.3987 549.3030\nJul 1961 648.5789 644.2386 645.4658 681.9690 661.4651 616.6036 644.4488\nAug 1961 656.4244 628.6673 626.7399 647.7804 658.5214 624.7581 614.8706\nSep 1961 555.3110 542.1264 538.3109 551.4988 519.2544 522.2879 509.3608\nOct 1961 495.3525 500.4989 490.3424 525.6306 483.8441 477.4092 469.8093\nNov 1961 418.4821 442.9516 397.7748 433.5565 404.7262 427.2061 393.6362\nDec 1961 474.7837 474.9655 444.5088 468.2018 448.3421 473.2992 440.2477\n           sim_36   sim_37   sim_38   sim_39   sim_40   sim_41   sim_42\nJan 1961 444.3041 436.3152 429.9248 445.6030 425.2956 430.9920 448.0607\nFeb 1961 402.9596 437.3810 409.4394 459.8234 406.2236 426.2994 402.3491\nMar 1961 437.5495 452.9946 430.0721 468.8317 437.6899 458.3786 433.7203\nApr 1961 484.7950 509.0917 491.4725 505.0216 486.6741 509.1101 477.0294\nMay 1961 494.9942 495.5896 510.0478 495.6117 537.0217 517.1334 498.7959\nJun 1961 553.7846 561.9860 561.6282 528.8183 582.3007 577.8466 559.3089\nJul 1961 641.9214 639.2986 645.9305 632.6647 664.5879 661.5590 638.0610\nAug 1961 636.8767 631.5135 639.6749 609.5456 628.7758 634.6867 622.2595\nSep 1961 520.3098 532.5600 539.0030 519.9259 538.8346 539.6474 525.7926\nOct 1961 467.1799 474.0092 501.0003 470.8937 489.3974 496.7073 477.6349\nNov 1961 400.7174 402.8441 430.2743 400.4070 418.7196 415.1624 400.9941\nDec 1961 450.4931 447.7201 458.0801 430.4308 458.8718 471.1464 440.3392\n           sim_43   sim_44   sim_45   sim_46   sim_47   sim_48   sim_49\nJan 1961 433.8962 431.9061 438.1303 441.5963 436.0231 446.3898 450.9442\nFeb 1961 409.3267 404.4595 425.5290 421.1926 412.2501 432.6751 415.0008\nMar 1961 438.4435 435.0668 463.5053 471.5038 455.1177 450.4521 451.1212\nApr 1961 473.8291 460.0592 494.5388 503.3624 490.9725 492.1162 487.2704\nMay 1961 483.4406 481.1193 504.9772 525.1390 496.5639 506.3061 490.4784\nJun 1961 561.3707 538.4233 570.0629 595.4120 551.5930 576.2151 554.0761\nJul 1961 647.0771 644.2353 657.3742 640.3391 640.6643 646.9610 638.9689\nAug 1961 636.0418 624.6064 637.4168 627.9038 625.3946 633.3995 636.1746\nSep 1961 576.9532 528.4517 539.4845 518.0352 548.8798 534.8344 516.3925\nOct 1961 518.5970 481.6991 492.4232 476.5997 498.2267 488.0005 477.0912\nNov 1961 463.6603 413.0277 399.7997 406.3397 424.6032 425.7309 393.2567\nDec 1961 514.5698 451.0460 435.0231 449.0356 469.3483 478.4579 442.3569\n           sim_50\nJan 1961 443.7523\nFeb 1961 427.1762\nMar 1961 450.7599\nApr 1961 506.4329\nMay 1961 506.4456\nJun 1961 536.6103\nJul 1961 645.3020\nAug 1961 611.8298\nSep 1961 532.1847\nOct 1961 495.4034\nNov 1961 444.7745\nDec 1961 476.1355\n\noutput$forecast_sim_tbl\n\n# A tibble: 600 × 4\n       x     y n        id\n   <dbl> <dbl> <chr> <int>\n 1 1961.  444. sim_1     1\n 2 1961.  410. sim_1     2\n 3 1961.  433. sim_1     3\n 4 1961.  477. sim_1     4\n 5 1961.  499. sim_1     5\n 6 1961.  559. sim_1     6\n 7 1962.  659. sim_1     7\n 8 1962.  637. sim_1     8\n 9 1962.  540. sim_1     9\n10 1962.  510. sim_1    10\n# … with 590 more rows\n\n\nThe time series that was used.\n\noutput$time_series\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\n\nThe fitted values in two different formats\n\noutput$fitted_values\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1949 111.9353 117.9664 131.9662 128.9774 120.9892 134.9782 147.9692 147.9731\n1950 115.4270 121.3807 138.5312 137.2522 127.5180 139.7865 158.9159 166.3207\n1951 132.8130 151.5147 164.7662 165.5934 153.1413 186.9835 201.0564 197.1598\n1952 171.2150 174.9573 205.3873 181.9983 189.5111 191.9692 229.3659 230.2887\n1953 197.4932 205.0669 211.9492 214.7030 229.4790 261.9978 259.3789 272.3394\n1954 205.8921 206.1940 236.4993 236.1932 227.3492 247.7618 281.4459 303.3972\n1955 229.7559 221.0431 274.5806 260.1968 270.5411 299.1164 345.0987 346.2332\n1956 283.9172 273.9637 307.7212 313.9008 312.5977 358.8094 415.5070 394.8641\n1957 313.6903 307.2137 343.5753 347.2471 353.0923 409.4687 455.6903 452.6105\n1958 346.2681 328.3116 377.2767 360.7205 361.5534 432.0632 479.8147 490.8438\n1959 346.5194 335.2349 385.8185 384.9676 406.8419 485.3013 531.0698 553.0149\n1960 418.4120 397.2579 454.0138 420.2105 468.2327 523.6974 603.6761 623.9211\n          Sep      Oct      Nov      Dec\n1949 135.9874 119.0049 104.0187 118.0778\n1950 156.2023 139.1631 119.1047 128.9974\n1951 185.1990 158.4085 140.6879 169.2312\n1952 220.7255 190.3056 172.8446 191.9035\n1953 238.8716 218.7628 194.3686 207.1910\n1954 261.6971 232.5912 199.3564 222.5606\n1955 309.8394 277.5670 246.5073 264.0691\n1956 363.2702 318.0959 271.0990 310.9291\n1957 409.6920 354.9782 312.2741 341.2514\n1958 438.0323 360.3301 317.3131 346.5759\n1959 454.6235 412.1130 357.5358 384.6475\n1960 513.8591 450.7760 410.8955 439.9468\n\noutput$fitted_values_tbl\n\n# A tibble: 144 × 2\n   index     value\n   <yearmon> <dbl>\n 1 Jan 1949   112.\n 2 Feb 1949   118.\n 3 Mar 1949   132.\n 4 Apr 1949   129.\n 5 May 1949   121.\n 6 Jun 1949   135.\n 7 Jul 1949   148.\n 8 Aug 1949   148.\n 9 Sep 1949   136.\n10 Oct 1949   119.\n# … with 134 more rows\n\n\nThe residual values in two different formats\n\noutput$residual_values\n\n               Jan           Feb           Mar           Apr           May\n1949   0.064663218   0.033565844   0.033806149   0.022551853   0.010753877\n1950  -0.426993657   4.619296276   2.468817592  -2.252222539  -2.517970381\n1951  12.187004737  -1.514734464  13.233788623  -2.593406722  18.858703025\n1952  -0.215049450   5.042657391 -12.387298452  -0.998279160  -6.511066526\n1953  -1.493243603  -9.066863638  24.050789583  20.296981218  -0.479038408\n1954  -1.892145409 -18.194023466  -1.499295070  -9.193240870   6.650775967\n1955  12.244087324  11.956865808  -7.580632008   8.803192574  -0.541058565\n1956   0.082775370   3.036286487   9.278782426  -0.900756132   5.402300489\n1957   1.309660582  -6.213658164  12.424731881   0.752860797   1.907693986\n1958  -6.268081829 -10.311568418 -15.276674427 -12.720532082   1.446551672\n1959  13.480639348   6.765147100  20.181485627  11.032373098  13.158068772\n1960  -1.412019863  -6.257914445 -35.013829003  40.789527421   3.767286916\n               Jun           Jul           Aug           Sep           Oct\n1949   0.021825883   0.030792890   0.026927580   0.012574909  -0.004856125\n1950   9.213518228  11.084130208   3.679258769   1.797714396  -6.163078961\n1951  -8.983477860  -2.056432959   1.840247841  -1.199018264   3.591516610\n1952  26.030752750   0.634055619  11.711292178 -11.725472510   0.694363698\n1953 -18.997830355   4.621111100  -0.339432546  -1.871647726  -7.762821892\n1954  16.238163730  20.554095186 -10.397216564  -2.697121208  -3.591173672\n1955  15.883576851  18.901348128   0.766758698   2.160561460  -3.567021842\n1956  15.190550278  -2.507027255  10.135908860  -8.270245332 -12.095862166\n1957  12.531339373   9.309735715  14.389487605  -5.692026179  -7.978198549\n1958   2.936791273  11.185298228  14.156225803 -34.032306461  -1.330101426\n1959 -13.301336524  16.930226311   5.985059029   8.376509562  -5.112953069\n1960  11.302583143  18.323916561 -17.921058274  -5.859106651  10.223989361\n               Nov           Dec\n1949  -0.018746667  -0.077775679\n1950  -5.104668785  11.002554045\n1951   5.312079856  -3.231170377\n1952  -0.844622054   2.096450492\n1953 -14.368618246  -6.190983141\n1954   3.643587684   6.439397882\n1955  -9.507327199  13.930943896\n1956  -0.099013624  -4.929146809\n1957  -7.274091927  -5.251369244\n1958  -7.313133943  -9.575869505\n1959   4.464243872  20.352533059\n1960 -20.895479201  -7.946822359\n\noutput$residual_values_tbl\n\n# A tibble: 144 × 2\n   index        value\n   <yearmon>    <dbl>\n 1 Jan 1949   0.0647 \n 2 Feb 1949   0.0336 \n 3 Mar 1949   0.0338 \n 4 Apr 1949   0.0226 \n 5 May 1949   0.0108 \n 6 Jun 1949   0.0218 \n 7 Jul 1949   0.0308 \n 8 Aug 1949   0.0269 \n 9 Sep 1949   0.0126 \n10 Oct 1949  -0.00486\n# … with 134 more rows\n\n\nThe input data itself\n\noutput$input_data\n\n# A tibble: 144 × 2\n   index     value\n   <yearmon> <dbl>\n 1 Jan 1949    112\n 2 Feb 1949    118\n 3 Mar 1949    132\n 4 Apr 1949    129\n 5 May 1949    121\n 6 Jun 1949    135\n 7 Jul 1949    148\n 8 Aug 1949    148\n 9 Sep 1949    136\n10 Oct 1949    119\n# … with 134 more rows\n\n\nThe time series simulations\n\noutput$sim_ts_tbl\n\n# A tibble: 600 × 5\n   index         x     y n        id\n   <yearmon> <dbl> <dbl> <chr> <int>\n 1 Jan 1961  1961.  444. sim_1     1\n 2 Feb 1961  1961.  410. sim_1     2\n 3 Mar 1961  1961.  433. sim_1     3\n 4 Apr 1961  1961.  477. sim_1     4\n 5 May 1961  1961.  499. sim_1     5\n 6 Jun 1961  1961.  559. sim_1     6\n 7 Jul 1961  1962.  659. sim_1     7\n 8 Aug 1961  1962.  637. sim_1     8\n 9 Sep 1961  1962.  540. sim_1     9\n10 Oct 1961  1962.  510. sim_1    10\n# … with 590 more rows\n\n\nNow, the visuals, first the static ggplot\n\noutput$ggplot\n\n\n\n\nThe interactive plotly plot.\n\noutput$plotly_plot\n\n\n\n\n\nVoila!"
  },
  {
    "objectID": "posts/rtip-2022-12-29/index.html",
    "href": "posts/rtip-2022-12-29/index.html",
    "title": "Gartner Magic Chart and its usefulness in healthcare analytics with {healthyR}",
    "section": "",
    "text": "Introduction\nThe Gartner Magic Chart is a powerful tool for analyzing healthcare data and identifying trends and patterns that can inform decision making. It was developed by Dr. James Gartner in the early 2000s as a way to visualize the relationship between two key metrics, for example: Excess Length of Stay (ELOS) and Excess Readmit Rate.\nIn healthcare, length of stay (LOS) refers to the amount of time a patient spends in the hospital. Excess LOS is the difference between the actual LOS of a patient and the expected LOS for that patient, based on their diagnosis and other factors. Excess readmit rate is the percentage of patients who are readmitted to the hospital within a certain time period after being discharged, above and beyond what is expected based on their diagnosis and other factors.\nThe Gartner Magic Chart can plot excess LOS on the x-axis and excess readmit rate on the y-axis. The resulting chart is divided into four quadrants, with the top right quadrant representing high excess LOS and high excess readmit rate, the bottom left quadrant representing low excess LOS and low excess readmit rate, and the other two quadrants representing intermediate values of these metrics.\nOne of the key benefits of the Gartner Magic Chart is that it allows healthcare professionals to quickly and easily identify areas of concern and opportunities for improvement. For example, if a hospital has a high excess LOS and a high excess readmit rate, it may be an indication that the hospital is not effectively managing patient care and is instead relying on costly and unnecessary readmissions to address problems that could have been avoided in the first place.\nThe Gartner Magic Chart can also be used to identify trends over time, allowing healthcare professionals to track progress and see the impact of changes they have made to patient care processes.\nIf you are interested in creating a Gartner Magic Chart for your own healthcare data, the R package {healthyR} has a convenient function called gartner_magic_chart_plt() that allows you to easily create this chart from data supplied by the end user. Simply input your excess LOS and excess readmit rate data, and the function will generate the chart for you.\nIn summary, the Gartner Magic Chart is a valuable tool for analyzing healthcare data and identifying trends and patterns that can inform decision making. By using the gartner_magic_chart_plt() function from the {healthyR} package, you can easily create this chart for your own data and start using it to improve patient care and outcomes.\n\n\nFunction\nLet’s take a look at the full function call for gartner_magic_chart_plt().\n\ngartner_magic_chart_plt(\n  .data,\n  .x_col,\n  .y_col,\n  .point_size_col = NULL,\n  .y_lab,\n  .x_lab,\n  .plt_title,\n  .tl_lbl,\n  .tr_lbl,\n  .br_lbl,\n  .bl_lbl\n)\n\nNow let’s take a look at the arguments to the parameters.\n\n.data - The data set you want to plot\n.x_col - The x-axis for the plot\n.y_col - The y-axis for the plot\n.point_size_col - The default is NULL, if you want to size the dots by a column in the data.frame/tibble then enter the column name here.\n.y_lab - The y-axis label\n.x_lab - The x-axis label\n.plt_title - The title of the plot\n.tl_lbl - The top left label\n.tr_lbl - The top right label\n.br_lbl - The bottom right label\n.bl_lbl - The bottom left label\n\n\n\nExample\nLet’s see the function in action.\n\nlibrary(dplyr)\nlibrary(healthyR)\n\ndata_tbl <- tibble(\n    x = rnorm(100, 0, 1),\n    y = rnorm(100, 0, 1),\n    z = abs(x) + abs(y)\n )\n\ngartner_magic_chart_plt(\n  .data = data_tbl,\n  .x_col = x,\n  .y_col = y,\n  .point_size = z,\n  .x_lab = \"los\",\n  .y_lab = \"ra\",\n  .plt_title = \"Test Title\",\n  .tr_lbl = \"High RA-LOS\",\n  .tl_lbl = \"High RA\",\n  .bl_lbl = \"Leader\",\n  .br_lbl = \"High LOS\"\n)\n\n\n\n\nExample two.\n\ngartner_magic_chart_plt(\n  .data = data_tbl,\n  .x_col = x,\n  .y_col = y,\n  .point_size = NULL,\n  .x_lab = \"los\",\n  .y_lab = \"ra\",\n  .plt_title = \"Test Title\",\n  .tr_lbl = \"High RA-LOS\",\n  .tl_lbl = \"High RA\",\n  .bl_lbl = \"Leader\",\n  .br_lbl = \"High LOS\"\n)"
  },
  {
    "objectID": "posts/weekly-rtip-healthyrts-2022-12-30/index.html",
    "href": "posts/weekly-rtip-healthyrts-2022-12-30/index.html",
    "title": "Event Analysis with {healthyR.ts}",
    "section": "",
    "text": "Introduction\nTime-to-event analysis, also known as survival analysis, is a statistical technique used to analyze the length of time until an event occurs. This type of analysis is often used in fields such as healthcare, engineering, and finance to understand the factors that influence the likelihood of an event occurring and to make predictions about future events.\nIn economics, an event study is a statistical technique used to analyze the effect of a specific event on a particular market or financial instrument. Event studies are commonly used in finance to understand how events, such as the announcement of a new product, the release of financial earnings, or a change in government policy, may impact the price or performance of a company’s stock or other financial instruments.\nTo conduct an event study, analysts typically collect data on the performance of a market or financial instrument before and after the event in question. This data is then used to estimate the effect of the event on the market or instrument.\nThere are several different methods that can be used to conduct an event study, including the market model, the abnormal return method, and the buy-and-hold abnormal return method. These methods allow analysts to quantify the effect of the event on the market or instrument and to identify any changes in market behavior that may have occurred as a result of the event.\nOverall, event studies are a valuable tool for understanding how specific events may impact financial markets and instruments, and are widely used in finance and economics to inform investment decisions and to better understand market behavior.\nIn this post we are going to examine a function from the R package {healthyR.ts} has a function called ts_time_event_analysis_tbl() that will help us understand what happens after a specified event, in this instance it will always be some percentage decrease or increase in a value.\nThere is a great article from Investopedia on this economic topic here\n\n\nFunction\nThe function is ts_time_event_analysis_tbl() and it’s complimentary plotting function ts_event_analysis_plot().\nHere is the tibble data return function.\n\nts_time_event_analysis_tbl(\n  .data,\n  .date_col,\n  .value_col,\n  .percent_change = 0.05,\n  .horizon = 12,\n  .precision = 2,\n  .direction = \"forward\",\n  .filter_non_event_groups = TRUE\n)\n\nLet’s take a look at the arguments to the parameters for this one.\n\n.data - The date.frame/tibble that holds the data.\n.date_col - The column with the date value.\n.value_col - The column with the value you are measuring.\n.percent_change - This defaults to 0.05 which is a 5% increase in the value_col.\n.horizon - How far do you want to look back or ahead.\n.precision - The default is 2 which means it rounds the lagged 1 value percent change to 2 decimal points. You may want more for more finely tuned results, this will result in fewer groupings.\n.direction - The default is forward. You can supply either forward, backwards or both.\nfilter_non_event_groups - The default is TRUE, this drops groupings with no events on the rare occasion it does occur.\n\nNow the plotting function.\n\nts_event_analysis_plot(\n  .data,\n  .plot_type = \"mean\",\n  .plot_ci = TRUE,\n  .interactive = FALSE\n)\n\n\n.data - The data that comes from the ts_time_event_analysis_tbl()\n.plot_type - The default is “mean” which will show the mean event change of the output from the analysis tibble. The possible values for this are: mean, median, and individual.\n.plot_ci - The default is TRUE. This will only work if you choose one of the aggregate plots of either “mean” or “median”\n.interactive - The default is FALSE. TRUE will return a plotly plot.\n\n\n\nExamples\nLet’s go through a couple examples using the AirPassengers data. We will first transform it into a tibble and then we will use a look period of 6. Let’s see the data output and then we will visualize.\n\nlibrary(healthyR.ts)\nlibrary(dplyr)\n\ndf <- ts_to_tbl(AirPassengers) %>% select(-index)\n\nevent_tbl <- ts_time_event_analysis_tbl(\n  .data = df,\n  .horizon = 6,\n  .date_col = date_col,\n  .value_col = value,\n  .direction = \"both\"\n)\n\nglimpse(event_tbl)\n\nRows: 33\nColumns: 18\n$ rowid                <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15…\n$ date_col             <date> 1949-02-01, 1949-03-01, 1949-04-01, 1949-05-01, …\n$ value                <dbl> 118, 132, 129, 121, 135, 148, 148, 199, 184, 162,…\n$ lag_val              <dbl> 112, 118, 132, 129, 121, 135, 148, 199, 199, 184,…\n$ adj_diff             <dbl> 6, 14, -3, -8, 14, 13, 0, 0, -15, -22, -16, 20, 5…\n$ relative_change_raw  <dbl> 0.05357143, 0.11864407, -0.02272727, -0.06201550,…\n$ relative_change      <dbl> 0.05, 0.12, -0.02, -0.06, 0.12, 0.10, 0.00, 0.00,…\n$ pct_chg_mark         <lgl> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n$ event_base_change    <dbl> 0.00000000, 0.11864407, -0.02272727, -0.06201550,…\n$ group_number         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2…\n$ numeric_group_number <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2…\n$ group_event_number   <int> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ x                    <int> 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1…\n$ mean_event_change    <dbl> 0.00000000, 0.03849647, -0.06815622, -0.04991040,…\n$ median_event_change  <dbl> 0.00000000, 0.07222222, -0.06217617, -0.06201550,…\n$ event_change_ci_low  <dbl> 0.00000000, -0.06799693, -0.11669576, -0.09692794…\n$ event_change_ci_high <dbl> 0.000000000, 0.116322976, -0.024699717, 0.0073964…\n$ event_type           <fct> Before, Before, Before, Before, Before, Before, A…\n\n\nLet’s visualize!\n\nts_event_analysis_plot(\n  .data = event_tbl\n)\n\n\n\n\nLet’s see the median now.\n\nts_event_analysis_plot(\n  .data = event_tbl,\n  .plot_type = \"median\"\n)\n\n\n\n\nNow let’s see it as an interactive plot.\n\nts_event_analysis_plot(\n  .data = event_tbl,\n  .plot_type = \"median\",\n  .interactive = TRUE\n)\n\n\n\n\n\nNow let’s see all the individual groups.\n\nts_event_analysis_plot(\n  .data = event_tbl,\n  .plot_type = \"individual\",\n  .interactive = TRUE\n)\n\n\n\n\n\nSingle direction plotting.\n\nts_time_event_analysis_tbl(\n  .data = df,\n  .horizon = 6,\n  .date_col = date_col,\n  .value_col = value,\n  .direction = \"backward\"\n) %>%\n  ts_event_analysis_plot()\n\n\n\n\nAnd…\n\nts_time_event_analysis_tbl(\n  .data = df,\n  .horizon = 6,\n  .date_col = date_col,\n  .value_col = value,\n  .direction = \"forward\"\n) %>%\n  ts_event_analysis_plot()\n\n\n\n\nVoila!"
  }
]